{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hafizur45/Job-Success-prediction-bangladesh/blob/main/Research2_jobsuccess_prediction_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjPx3TihDQw6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1zoRB1I-N3h",
        "outputId": "4681beaa-5317-4513-aeb6-41db71e5d5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcB9O2C0DQw9"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel('/content/drive/MyDrive/Datasets/Copy of data.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "NrG0lHZO_BQ-",
        "outputId": "b802d79c-c3c3-4e15-d46e-eb8500b33eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ID                  Name  Gender  Age  Bachelor  \\\n",
              "0   26000     Md. Taslim Mahmud       1   44         1   \n",
              "1  135207    Ashfaq Newaz Ahmed       1   41         1   \n",
              "2  208486     S.M. Shafiul Azam       1   52         1   \n",
              "3  209871  Kazi Mohammadun Nabi       1   47         1   \n",
              "4  237524           Aslam Uddin       1   45         1   \n",
              "\n",
              "   Bachelor Education Organization (Public / Private)  B_SUBJECT  Masters  \\\n",
              "0                                                  1           0        1   \n",
              "1                                                  1           0        1   \n",
              "2                                                  1           0        1   \n",
              "3                                                  1           1        0   \n",
              "4                                                  1           0        1   \n",
              "\n",
              "   Masters Education Organization (Public / Private)  m_SUBJECT  video cv  \\\n",
              "0                                                  1          0         1   \n",
              "1                                                  1          0         1   \n",
              "2                                                  1          0         1   \n",
              "3                                                  0          0         1   \n",
              "4                                                  1          0         1   \n",
              "\n",
              "   Technical Skill  SOFT Skill  EDU_Result  EXPerienced  output  \n",
              "0                1           0           1            1       1  \n",
              "1                1           1           1            1       1  \n",
              "2                0           0           1            0       0  \n",
              "3                0           1           1            1       1  \n",
              "4                0           0           1            0       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93886096-4737-4082-a625-d3fbaf13a3d1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Bachelor</th>\n",
              "      <th>Bachelor Education Organization (Public / Private)</th>\n",
              "      <th>B_SUBJECT</th>\n",
              "      <th>Masters</th>\n",
              "      <th>Masters Education Organization (Public / Private)</th>\n",
              "      <th>m_SUBJECT</th>\n",
              "      <th>video cv</th>\n",
              "      <th>Technical Skill</th>\n",
              "      <th>SOFT Skill</th>\n",
              "      <th>EDU_Result</th>\n",
              "      <th>EXPerienced</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26000</td>\n",
              "      <td>Md. Taslim Mahmud</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>135207</td>\n",
              "      <td>Ashfaq Newaz Ahmed</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>208486</td>\n",
              "      <td>S.M. Shafiul Azam</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>209871</td>\n",
              "      <td>Kazi Mohammadun Nabi</td>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>237524</td>\n",
              "      <td>Aslam Uddin</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93886096-4737-4082-a625-d3fbaf13a3d1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-93886096-4737-4082-a625-d3fbaf13a3d1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-93886096-4737-4082-a625-d3fbaf13a3d1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-95eca71f-dfd4-4855-8b4e-aafa3253e86c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-95eca71f-dfd4-4855-8b4e-aafa3253e86c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-95eca71f-dfd4-4855-8b4e-aafa3253e86c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 86874,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1484084,\n        \"min\": 20737,\n        \"max\": 6878510,\n        \"num_unique_values\": 85685,\n        \"samples\": [\n          6400879,\n          6435420,\n          5868628\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 69499,\n        \"samples\": [\n          \"Md Shakhawat Hossain Sifat\",\n          \"NURJAHAN \",\n          \"MD Kibria Khan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 124,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          44,\n          29,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bachelor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bachelor Education Organization (Public / Private)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_SUBJECT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Masters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Masters Education Organization (Public / Private)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"m_SUBJECT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video cv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Technical Skill\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOFT Skill\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EDU_Result\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EXPerienced\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euO5PmtQDQw-"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['ID', 'Name'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd7Jkh3cDQw_"
      },
      "outputs": [],
      "source": [
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpam9niGDQxA",
        "outputId": "e4063ef5-0060-4b07-a889-ec4b26369b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.07551300177552273\n",
            "Root Mean Squared Error: 0.2747962914151549\n",
            "Coefficients: [ 0.00121251 -0.00169859 -0.02129704 -0.02129704  0.03920786  0.01263861\n",
            "  0.01263861 -0.01130895  0.25229652  0.41045685  0.34677724  0.04932276\n",
            "  0.28311694]\n",
            "Intercept: -0.12834132705215406\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' from the previous code\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "# Optionally, you can also print the coefficients and intercept\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luzeXqsuDQxB",
        "outputId": "825c9c64-49ba-4456-a3af-4f4584bfc615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9493525179856115\n",
            "Confusion Matrix:\n",
            "[[10895   766]\n",
            " [  114  5600]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "X = df.drop('output', axis=1)\n",
        "y = df['output']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwRjSwqKDQxC",
        "outputId": "e8473ac9-aeae-4da1-e727-c49743b15968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model:\n",
            "Accuracy: 0.9388776978417266\n",
            "Confusion Matrix:\n",
            "[[10599  1062]\n",
            " [    0  5714]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     11661\n",
            "           1       0.84      1.00      0.91      5714\n",
            "\n",
            "    accuracy                           0.94     17375\n",
            "   macro avg       0.92      0.95      0.93     17375\n",
            "weighted avg       0.95      0.94      0.94     17375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# Train the SVM model on the training set\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "svm_y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the SVM model\n",
        "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "svm_conf_matrix = confusion_matrix(y_test, svm_y_pred)\n",
        "svm_classification_rep = classification_report(y_test, svm_y_pred)\n",
        "\n",
        "print(\"SVM Model:\")\n",
        "print(f\"Accuracy: {svm_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{svm_conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{svm_classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfQW1c7VDQxC",
        "outputId": "ef0c4143-228f-4317-de71-193c8fff5e6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Model:\n",
            "Accuracy: 0.9490071942446043\n",
            "Confusion Matrix:\n",
            "[[10882   779]\n",
            " [  107  5607]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the Decision Tree model on the training set\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "dt_y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Decision Tree model\n",
        "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
        "dt_conf_matrix = confusion_matrix(y_test, dt_y_pred)\n",
        "dt_classification_rep = classification_report(y_test, dt_y_pred)\n",
        "\n",
        "print(\"\\nDecision Tree Model:\")\n",
        "print(f\"Accuracy: {dt_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{dt_conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{dt_classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC_tQ_pRDQxD",
        "outputId": "20eb65b5-ef4f-4a21-ccfc-82e28f0d2256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Model:\n",
            "Accuracy: 0.9502733812949641\n",
            "Confusion Matrix:\n",
            "[[10878   783]\n",
            " [   81  5633]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.99      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Train the Random Forest model on the training set\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "rf_conf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
        "rf_classification_rep = classification_report(y_test, rf_y_pred)\n",
        "\n",
        "print(\"\\nRandom Forest Model:\")\n",
        "print(f\"Accuracy: {rf_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{rf_conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{rf_classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FIhqVa9DQxD",
        "outputId": "240ae17f-df14-4443-cca1-c586c17e4767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "\n",
            "XGBoost Model:\n",
            "Accuracy: 0.9518273381294964\n",
            "Confusion Matrix:\n",
            "[[10860   801]\n",
            " [   36  5678]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     11661\n",
            "           1       0.88      0.99      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.96      0.95      0.95     17375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip3 install xgboost\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = XGBClassifier()\n",
        "\n",
        "# Train the XGBoost model on the training set\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "xgb_y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "xgb_conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
        "xgb_classification_rep = classification_report(y_test, xgb_y_pred)\n",
        "\n",
        "print(\"\\nXGBoost Model:\")\n",
        "print(f\"Accuracy: {xgb_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{xgb_conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{xgb_classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybd-yUkJDQxE",
        "outputId": "ff96229c-040e-4876-d9a5-0bbcac8ca3d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "30 fits failed out of a total of 60.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.89958128        nan 0.936848          nan 0.9496971\n",
            "        nan 0.94979782        nan 0.95048847        nan 0.95014314]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.949294964028777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example for Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Best Model Accuracy: {accuracy_best}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZFMlwsKDQxE",
        "outputId": "dd1aaa95-94f1-4b9d-a95a-510ad1a19371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Cross-Validation Accuracy: 0.9509633124064099\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Example for Random Forest\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Use cross_val_score to evaluate the model with different hyperparameter settings\n",
        "scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
        "average_accuracy = np.mean(scores)\n",
        "\n",
        "print(f\"Average Cross-Validation Accuracy: {average_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aMP8SPQDQxE",
        "outputId": "76fd7b41-7de2-419e-fa02-555a3c46c4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.9499280575539568\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "# Fit the model with different hyperparameter combinations\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Best Model Accuracy: {accuracy_best}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huhmJOmaDQxE",
        "outputId": "6f6507e3-64e1-47db-e5d4-2778804db9d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.9492374100719424\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the hyperparameter distribution\n",
        "param_dist = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring='accuracy', cv=5, n_iter=10)\n",
        "\n",
        "# Fit the model with different hyperparameter combinations\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Best Model Accuracy: {accuracy_best}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eH2FXNNDQxF",
        "outputId": "a129ee01-ab7e-456f-ba4f-e7b7cde32daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9493525179856115\n",
            "Confusion Matrix:\n",
            "[[10895   766]\n",
            " [  114  5600]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Model Accuracy after Hyperparameter Tuning: 0.9492374100719424\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\")\n",
        "\n",
        "# Hyperparameter Tuning using Grid Search\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"\\nBest Model Accuracy after Hyperparameter Tuning: {accuracy_best}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAgU0SzWDQxF",
        "outputId": "3356ba9d-4eb3-4ed5-f91f-f231b26dac3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Random Forest Model Accuracy after Hyperparameter Tuning: 0.9518273381294964\n",
            "\n",
            "Best XGBoost Model Accuracy after Hyperparameter Tuning: 0.9520575539568346\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Hyperparameter Tuning\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "best_model_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Evaluate the best Random Forest model\n",
        "y_pred_rf = best_model_rf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"\\nBest Random Forest Model Accuracy after Hyperparameter Tuning: {accuracy_rf}\")\n",
        "\n",
        "# XGBoost Hyperparameter Tuning\n",
        "param_dist_xgb = {\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 1, 2],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1]\n",
        "}\n",
        "\n",
        "random_search_xgb = RandomizedSearchCV(XGBClassifier(), param_distributions=param_dist_xgb, n_iter=10, cv=5)\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "best_params_xgb = random_search_xgb.best_params_\n",
        "best_model_xgb = random_search_xgb.best_estimator_\n",
        "\n",
        "# Evaluate the best XGBoost model\n",
        "y_pred_xgb = best_model_xgb.predict(X_test)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"\\nBest XGBoost Model Accuracy after Hyperparameter Tuning: {accuracy_xgb}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-KnJ3EkDQxF",
        "outputId": "a79f4ff9-d5eb-4052-b1f0-ef402b487dca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "[[10861   800]\n",
            " [   37  5677]]\n",
            "\n",
            "Confusion Matrix for XGBoost:\n",
            "[[10853   808]\n",
            " [   25  5689]]\n"
          ]
        }
      ],
      "source": [
        "# Confusion Matrix for Random Forest\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "print(\"\\nConfusion Matrix for Random Forest:\")\n",
        "print(conf_matrix_rf)\n",
        "\n",
        "# Confusion Matrix for XGBoost\n",
        "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"\\nConfusion Matrix for XGBoost:\")\n",
        "print(conf_matrix_xgb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUjumnL6DQxF",
        "outputId": "18508850-1fac-4173-9fe4-c5664757897c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     11661\n",
            "           1       0.88      0.99      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.96      0.95      0.95     17375\n",
            "\n",
            "\n",
            "Classification Report for XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     11661\n",
            "           1       0.88      1.00      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.96      0.95      0.95     17375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print classification report for Random Forest\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "print(\"\\nClassification Report for Random Forest:\")\n",
        "print(report_rf)\n",
        "\n",
        "# Print classification report for XGBoost\n",
        "report_xgb = classification_report(y_test, y_pred_xgb)\n",
        "print(\"\\nClassification Report for XGBoost:\")\n",
        "print(report_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl-HzUfKDQxG",
        "outputId": "3aad2281-be6c-40b8-e5c1-853009a368ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9502158273381295\n",
            "Confusion Matrix:\n",
            "[[10876   785]\n",
            " [   80  5634]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.99      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBQVJgw3DQxG",
        "outputId": "91fdf6a8-8638-4266-8e2d-b35b35b0cca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9493525179856115\n",
            "Decision Tree Accuracy: 0.9490071942446043\n",
            "SVM Accuracy: 0.9388776978417266\n",
            "XGBoost Accuracy: 0.9518273381294964\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "logistic_regression_model = LogisticRegression(random_state=42)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "svm_model = SVC(random_state=42)\n",
        "xgboost_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "svm_model.fit(X_train, y_train)\n",
        "xgboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = logistic_regression_model.predict(X_test)\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_pred_xgb = xgboost_model.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
        "print(\"SVM Accuracy:\", accuracy_svm)\n",
        "print(\"XGBoost Accuracy:\", accuracy_xgb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCTIly5lDQxG",
        "outputId": "19866c41-709e-48b2-fcd9-f517c47d877d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9425611510791367\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier()\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(\"KNN Accuracy:\", accuracy_knn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwKFsLErDQxG",
        "outputId": "c5913ec7-1de0-43d7-ab71-703c5e1107d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.6504172661870503\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNbr4CocDQxG",
        "outputId": "13d2ca3e-c4fa-498d-dee0-523d6cef1ad1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.951021582733813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "linear_svm_model = LinearSVC()\n",
        "linear_svm_model.fit(X_train, y_train)\n",
        "y_pred_linear_svm = linear_svm_model.predict(X_test)\n",
        "accuracy_linear_svm = accuracy_score(y_test, y_pred_linear_svm)\n",
        "print(\"Linear SVM Accuracy:\", accuracy_linear_svm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw6vmP9gDQxH",
        "outputId": "ce6ae39c-3dfb-4080-cbf3-9cc4d4eb4c51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.9225323741007194\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "y_pred_adaboost = adaboost_model.predict(X_test)\n",
        "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_adaboost)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2VK_eMtDQxH",
        "outputId": "5f541815-f2fd-4a59-d098-bd7cd09d7f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multilayer Perceptron Accuracy: 0.952\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "mlp_model.fit(X_train, y_train)\n",
        "y_pred_mlp = mlp_model.predict(X_test)\n",
        "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
        "print(\"Multilayer Perceptron Accuracy:\", accuracy_mlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npZNOm39DQxH",
        "outputId": "3ba68155-11db-4169-d97f-fa28f6779de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(86874, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyk_-tTgDQxH",
        "outputId": "6937060e-2965-46bd-a9ac-d20a57fd4595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Gender  Age  Bachelor  \\\n",
              "0           1   44         1   \n",
              "1           1   41         1   \n",
              "2           1   52         1   \n",
              "3           1   47         1   \n",
              "4           1   45         1   \n",
              "...       ...  ...       ...   \n",
              "86869       1   20         0   \n",
              "86870       1   18         0   \n",
              "86871       1   28         1   \n",
              "86872       1   22         1   \n",
              "86873       1   22         1   \n",
              "\n",
              "       Bachelor Education Organization (Public / Private)  B_SUBJECT  Masters  \\\n",
              "0                                                      1           0        1   \n",
              "1                                                      1           0        1   \n",
              "2                                                      1           0        1   \n",
              "3                                                      1           1        0   \n",
              "4                                                      1           0        1   \n",
              "...                                                  ...         ...      ...   \n",
              "86869                                                  0           0        0   \n",
              "86870                                                  0           0        0   \n",
              "86871                                                  1           0        1   \n",
              "86872                                                  1           0        0   \n",
              "86873                                                  1           0        0   \n",
              "\n",
              "       Masters Education Organization (Public / Private)  m_SUBJECT  video cv  \\\n",
              "0                                                      1          0         1   \n",
              "1                                                      1          0         1   \n",
              "2                                                      1          0         1   \n",
              "3                                                      0          0         1   \n",
              "4                                                      1          0         1   \n",
              "...                                                  ...        ...       ...   \n",
              "86869                                                  0          0         0   \n",
              "86870                                                  0          0         1   \n",
              "86871                                                  1          0         1   \n",
              "86872                                                  0          0         1   \n",
              "86873                                                  0          0         0   \n",
              "\n",
              "       Technical Skill  SOFT Skill  EDU_Result  EXPerienced  output  \n",
              "0                    1           0           1            1       1  \n",
              "1                    1           1           1            1       1  \n",
              "2                    0           0           1            0       0  \n",
              "3                    0           1           1            1       1  \n",
              "4                    0           0           1            0       0  \n",
              "...                ...         ...         ...          ...     ...  \n",
              "86869                0           0           0            0       0  \n",
              "86870                0           0           0            0       0  \n",
              "86871                1           0           1            1       0  \n",
              "86872                0           1           0            1       0  \n",
              "86873                0           0           0            0       0  \n",
              "\n",
              "[86874 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a54544be-319b-4b8d-8bd0-7af23ef3ac2b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Bachelor</th>\n",
              "      <th>Bachelor Education Organization (Public / Private)</th>\n",
              "      <th>B_SUBJECT</th>\n",
              "      <th>Masters</th>\n",
              "      <th>Masters Education Organization (Public / Private)</th>\n",
              "      <th>m_SUBJECT</th>\n",
              "      <th>video cv</th>\n",
              "      <th>Technical Skill</th>\n",
              "      <th>SOFT Skill</th>\n",
              "      <th>EDU_Result</th>\n",
              "      <th>EXPerienced</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86869</th>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86870</th>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86871</th>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86872</th>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86873</th>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>86874 rows  14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a54544be-319b-4b8d-8bd0-7af23ef3ac2b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a54544be-319b-4b8d-8bd0-7af23ef3ac2b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a54544be-319b-4b8d-8bd0-7af23ef3ac2b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c0fb8ef-029f-4cf6-9da5-534f65d53c96\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c0fb8ef-029f-4cf6-9da5-534f65d53c96')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c0fb8ef-029f-4cf6-9da5-534f65d53c96 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b70b0eaf-b699-4cb7-bf37-11b997af9cad\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b70b0eaf-b699-4cb7-bf37-11b997af9cad button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 86874,\n  \"fields\": [\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 124,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          44,\n          29,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bachelor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bachelor Education Organization (Public / Private)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_SUBJECT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Masters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Masters Education Organization (Public / Private)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"m_SUBJECT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video cv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Technical Skill\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOFT Skill\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EDU_Result\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EXPerienced\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8F-1R4EDQxH",
        "outputId": "ada01afd-b19e-4421-8271-dea3c19a4d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Gradient Boosting: 0.95\n",
            "Classification Report for Gradient Boosting:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     11661\n",
            "           1       0.88      1.00      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.96      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "[[10852   809]\n",
            " [   23  5691]]\n",
            "\n",
            "Accuracy for Random Forest: 0.95\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.99      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.96      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "[[10877   784]\n",
            " [   76  5638]]\n",
            "\n",
            "Accuracy for Decision Tree: 0.95\n",
            "Classification Report for Decision Tree:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            "[[10882   779]\n",
            " [  107  5607]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Logistic Regression: 0.95\n",
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "[[10895   766]\n",
            " [  114  5600]]\n",
            "\n",
            "Accuracy for SVM: 0.94\n",
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     11661\n",
            "           1       0.84      1.00      0.91      5714\n",
            "\n",
            "    accuracy                           0.94     17375\n",
            "   macro avg       0.92      0.95      0.93     17375\n",
            "weighted avg       0.95      0.94      0.94     17375\n",
            "\n",
            "Confusion Matrix for SVM:\n",
            "[[10599  1062]\n",
            " [    0  5714]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"SVM\": SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.2f}\")\n",
        "\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "    print(f\"Classification Report for {name}:\\n{classification_rep}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix for {name}:\\n{conf_matrix}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEPf0AgQDQxH",
        "outputId": "046ddc35-8a63-4239-d8ce-c08ca74bde1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Gradient Boosting: 0.95\n",
            "Classification Report for Gradient Boosting:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     11661\n",
            "           1       0.88      1.00      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.96      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "[[10852   809]\n",
            " [   23  5691]]\n",
            "\n",
            "Accuracy for Random Forest: 0.95\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.99      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.94      0.96      0.95     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "[[10877   784]\n",
            " [   81  5633]]\n",
            "\n",
            "Accuracy for Decision Tree: 0.95\n",
            "Classification Report for Decision Tree:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            "[[10882   779]\n",
            " [  107  5607]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Logistic Regression: 0.95\n",
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     11661\n",
            "           1       0.88      0.98      0.93      5714\n",
            "\n",
            "    accuracy                           0.95     17375\n",
            "   macro avg       0.93      0.96      0.94     17375\n",
            "weighted avg       0.95      0.95      0.95     17375\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "[[10895   766]\n",
            " [  114  5600]]\n",
            "\n",
            "Accuracy for SVM: 0.94\n",
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     11661\n",
            "           1       0.84      1.00      0.91      5714\n",
            "\n",
            "    accuracy                           0.94     17375\n",
            "   macro avg       0.92      0.95      0.93     17375\n",
            "weighted avg       0.95      0.94      0.94     17375\n",
            "\n",
            "Confusion Matrix for SVM:\n",
            "[[10599  1062]\n",
            " [    0  5714]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"SVM\": SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.2f}\")\n",
        "\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "    print(f\"Classification Report for {name}:\\n{classification_rep}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix for {name}:\\n{conf_matrix}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_FfS3qBDQxI",
        "outputId": "dd3eb4b8-6280-45af-c4f9-5cfebe040b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Gradient Boosting: 0.95\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "[[10852   809]\n",
            " [   23  5691]]\n",
            "\n",
            "Accuracy for Random Forest: 0.95\n",
            "Confusion Matrix for Random Forest:\n",
            "[[10878   783]\n",
            " [   81  5633]]\n",
            "\n",
            "Accuracy for Decision Tree: 0.95\n",
            "Confusion Matrix for Decision Tree:\n",
            "[[10882   779]\n",
            " [  107  5607]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Logistic Regression: 0.95\n",
            "Confusion Matrix for Logistic Regression:\n",
            "[[10895   766]\n",
            " [  114  5600]]\n",
            "\n",
            "Accuracy for SVM: 0.94\n",
            "Confusion Matrix for SVM:\n",
            "[[10599  1062]\n",
            " [    0  5714]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"SVM\": SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.2f}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix for {name}:\\n{conf_matrix}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpsiVivZDQxI",
        "outputId": "7faea2b4-24b1-451b-b097-b6af94d6c18b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for SVM: 0.94\n",
            "Confusion Matrix for SVM:\n",
            "[[10599  1062]\n",
            " [    0  5714]]\n",
            "\n",
            "Accuracy for Random Forest: 0.95\n",
            "Confusion Matrix for Random Forest:\n",
            "[[10875   786]\n",
            " [   76  5638]]\n",
            "\n",
            "Accuracy for AdaBoost: 0.92\n",
            "Confusion Matrix for AdaBoost:\n",
            "[[11094   567]\n",
            " [  779  4935]]\n",
            "\n",
            "Accuracy for Bagging: 0.95\n",
            "Confusion Matrix for Bagging:\n",
            "[[10884   777]\n",
            " [   93  5621]]\n",
            "\n",
            "Accuracy for Decision Tree: 0.95\n",
            "Confusion Matrix for Decision Tree:\n",
            "[[10882   779]\n",
            " [  107  5607]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"Bagging\": BaggingClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.2f}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix for {name}:\\n{conf_matrix}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEVzr-VWDQxI",
        "outputId": "521c5e77-2b1c-4b5e-a8aa-9e95ed510c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for KNN: 0.94\n",
            "Confusion Matrix for KNN:\n",
            "[[10851   810]\n",
            " [  188  5526]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    # \"SVM\": SVC(),\n",
        "    # \"Random Forest\": RandomForestClassifier(),\n",
        "    # \"Bagging\": BaggingClassifier(),\n",
        "    # \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.2f}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix for {name}:\\n{conf_matrix}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U263MlKWDQxI",
        "outputId": "cb470c97-82e5-4eec-cfcf-d5fa8bc2c664",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for KNN: 0.94\n",
            "Classification Report for KNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.96     11661\n",
            "           1       0.87      0.97      0.92      5714\n",
            "\n",
            "    accuracy                           0.94     17375\n",
            "   macro avg       0.93      0.95      0.94     17375\n",
            "weighted avg       0.95      0.94      0.94     17375\n",
            "\n",
            "Confusion Matrix for KNN:\n",
            "[[10851   810]\n",
            " [  188  5526]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    # \"SVM\": SVC(),\n",
        "    # \"Random Forest\": RandomForestClassifier(),\n",
        "    # \"Bagging\": BaggingClassifier(),\n",
        "    # \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.2f}\")\n",
        "\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "    print(f\"Classification Report for {name}:\\n{classification_rep}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix for {name}:\\n{conf_matrix}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq4-DYWvDQxI",
        "outputId": "b7258695-a989-444d-9edd-074ec636beeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"5c0521af-d303-46df-9f09-766f3f72dc65\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c0521af-d303-46df-9f09-766f3f72dc65\")) {                    Plotly.newPlot(                        \"5c0521af-d303-46df-9f09-766f3f72dc65\",                        [{\"mode\":\"lines\",\"name\":\"Random Forest (AUC = 0.98)\",\"x\":[0.0,0.0029157019123574306,0.0029157019123574306,0.0030014578509561787,0.0030014578509561787,0.0030872137895549268,0.0030872137895549268,0.003172969728153675,0.003172969728153675,0.003172969728153675,0.003172969728153675,0.003172969728153675,0.003172969728153675,0.0032587256667524224,0.0032587256667524224,0.0033444816053511705,0.0035159934825486666,0.0036017494211474143,0.0036017494211474143,0.0036017494211474143,0.0036017494211474143,0.0036017494211474143,0.0036017494211474143,0.0036875053597461623,0.0038590172369436584,0.0038590172369436584,0.0038590172369436584,0.0038590172369436584,0.0039447731755424065,0.0039447731755424065,0.0039447731755424065,0.0039447731755424065,0.004030529114141155,0.004030529114141155,0.004116285052739903,0.00420204099133865,0.004287796929937398,0.004373552868536146,0.004373552868536146,0.004373552868536146,0.004373552868536146,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.004545064745733642,0.004545064745733642,0.00463082068433239,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004888088500128634,0.004888088500128634,0.0049738444387273815,0.0049738444387273815,0.00505960037732613,0.00505960037732613,0.00505960037732613,0.00505960037732613,0.005145356315924878,0.005145356315924878,0.005145356315924878,0.005231112254523626,0.005402624131721122,0.005402624131721122,0.00548838007031987,0.005659891947517366,0.006002915701912357,0.0060886716405111055,0.0060886716405111055,0.006517451333504845,0.006774719149301089,0.006946231026498585,0.006946231026498585,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007031986965097333,0.007117742903696081,0.0072034988422948285,0.007289254780893577,0.007460766658091073,0.007460766658091073,0.007460766658091073,0.007546522596689821,0.007632278535288569,0.007632278535288569,0.007632278535288569,0.007718034473887317,0.007803790412486065,0.007803790412486065,0.00797530228968356,0.00806105822828231,0.008489837921276049,0.008489837921276049,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008918617614269788,0.009004373552868537,0.009090129491467284,0.009175885430066031,0.009347397307263527,0.009347397307263527,0.009776177000257269,0.009776177000257269,0.01011920075465226,0.01011920075465226,0.01011920075465226,0.01011920075465226,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010290712631849755,0.010290712631849755,0.010376468570448504,0.010462224509047251,0.010462224509047251,0.010462224509047251,0.010462224509047251,0.010633736386244748,0.010633736386244748,0.010633736386244748,0.010633736386244748,0.010719492324843495,0.010719492324843495,0.010805248263442244,0.01097676014063974,0.01097676014063974,0.011062516079238487,0.011062516079238487,0.011577051710830975,0.011577051710830975,0.011662807649429723,0.011662807649429723,0.011662807649429723,0.011662807649429723,0.011662807649429723,0.011834319526627219,0.011834319526627219,0.012091587342423462,0.012177343281022211,0.012177343281022211,0.012263099219620958,0.012263099219620958,0.012434611096818454,0.012520367035417203,0.012520367035417203,0.012520367035417203,0.012520367035417203,0.012520367035417203,0.012520367035417203,0.012520367035417203,0.01260612297401595,0.01260612297401595,0.01260612297401595,0.012777634851213446,0.012777634851213446,0.012863390789812194,0.012863390789812194,0.013377926421404682,0.013377926421404682,0.013463682360003431,0.013463682360003431,0.013463682360003431,0.013463682360003431,0.013463682360003431,0.013635194237200925,0.013720950175799674,0.013806706114398421,0.013806706114398421,0.013978217991595918,0.013978217991595918,0.014235485807392163,0.014235485807392163,0.014235485807392163,0.014835777377583398,0.014921533316182145,0.014921533316182145,0.015264557070577138,0.015350313009175885,0.015350313009175885,0.015350313009175885,0.015350313009175885,0.015521824886373381,0.015521824886373381,0.015521824886373381,0.01560758082497213,0.01560758082497213,0.01569333676357088,0.015779092702169626,0.01612211645656462,0.01646514021095961,0.016550896149558358,0.016550896149558358,0.016636652088157105,0.017065431781150844,0.017236943658348342,0.017408455535545837,0.01757996741274333,0.01757996741274333,0.018008747105737074,0.01809450304433582,0.01852328273732956,0.01852328273732956,0.018694794614527055,0.018866306491724553,0.018866306491724553,0.019123574307520794,0.019123574307520794,0.019123574307520794,0.019209330246119545,0.01938084212331704,0.019638109939113284,0.019638109939113284,0.019981133693508277,0.019981133693508277,0.02015264557070577,0.02015264557070577,0.02058142526369951,0.020838693079495756,0.021095960895292,0.021095960895292,0.021696252465483234,0.021696252465483234,0.02178200840408198,0.02178200840408198,0.022039276219878227,0.022039276219878227,0.022125032158476974,0.02221078809707572,0.022468055912871966,0.022468055912871966,0.022553811851470713,0.022639567790069464,0.02315410342166195,0.02315410342166195,0.02315410342166195,0.02315410342166195,0.023239859360260698,0.023239859360260698,0.023411371237458192,0.02358288311465569,0.02358288311465569,0.023668639053254437,0.023668639053254437,0.023668639053254437,0.023668639053254437,0.023668639053254437,0.023754394991853185,0.023925906869050682,0.023925906869050682,0.023925906869050682,0.024097418746248177,0.024526198439241916,0.025040734070834406,0.025040734070834406,0.025040734070834406,0.025126490009433153,0.025126490009433153,0.0252122459480319,0.025298001886630648,0.0253837578252294,0.0253837578252294,0.0253837578252294,0.0253837578252294,0.0253837578252294,0.025469513763828146,0.025469513763828146,0.025469513763828146,0.025555269702426893,0.02564102564102564,0.025898293456821885,0.02606980533401938,0.026241317211216877,0.026241317211216877,0.026327073149815625,0.026327073149815625,0.026327073149815625,0.026412829088414372,0.02649858502701312,0.02658434096561187,0.026927364720006862,0.026927364720006862,0.0273561444130006,0.027870680044593088,0.027956435983191835,0.028127947860389333,0.028299459737586827,0.028385215676185575,0.028385215676185575,0.028385215676185575,0.028385215676185575,0.028813995369179314,0.028813995369179314,0.02907126318497556,0.029757310693765544,0.029757310693765544,0.029757310693765544,0.029757310693765544,0.02984306663236429,0.02984306663236429,0.02984306663236429,0.02984306663236429,0.02984306663236429,0.02984306663236429,0.029928822570963038,0.029928822570963038,0.030529114141154275,0.03078638195695052,0.03250150072892548,0.03250150072892548,0.032587256667524225,0.032587256667524225,0.033444816053511704,0.03353057199211045,0.03353057199211045,0.0336163279307092,0.0336163279307092,0.03387359574650545,0.03387359574650545,0.03404510762370294,0.03464539919389418,0.037218077351856615,0.03730383329045536,0.037732612983449106,0.037732612983449106,0.03781836892204785,0.03781836892204785,0.03781836892204785,0.0379041248606466,0.03798988079924535,0.03798988079924535,0.03798988079924535,0.03798988079924535,0.03798988079924535,0.03798988079924535,0.038075636737844094,0.03824714861504159,0.03824714861504159,0.03867592830803533,0.03876168424663408,0.03876168424663408,0.03876168424663408,0.03919046393962782,0.03919046393962782,0.03919046393962782,0.03919046393962782,0.03919046393962782,0.03927621987822657,0.03927621987822657,0.03927621987822657,0.039361975816825316,0.04047680301860904,0.04047680301860904,0.04047680301860904,0.04064831489580654,0.04064831489580654,0.040734070834405285,0.040734070834405285,0.040734070834405285,0.040734070834405285,0.040734070834405285,0.041077094588800274,0.041077094588800274,0.041334362404596516,0.041334362404596516,0.04142011834319527,0.04142011834319527,0.04142011834319527,0.04176314209759026,0.041848898036189006,0.041848898036189006,0.04227767772918275,0.04227767772918275,0.04253494554497899,0.04279221336077523,0.04279221336077523,0.04279221336077523,0.04279221336077523,0.04279221336077523,0.04304948117657148,0.043478260869565216,0.043478260869565216,0.04390704056255896,0.0441643083783552,0.04476459994854644,0.045193379641540174,0.045193379641540174,0.04527913558013893,0.04553640339593517,0.04579367121173141,0.04579367121173141,0.04579367121173141,0.04579367121173141,0.046222450904725154,0.046222450904725154,0.046222450904725154,0.046479718720521396,0.04656547465912014,0.046822742474916385,0.046994254352113886,0.046994254352113886,0.04725152216791013,0.047337278106508875,0.04742303404510762,0.047594545922305116,0.04768030186090387,0.04853786124689135,0.048623617185490096,0.048623617185490096,0.04888088500128634,0.04930966469428008,0.04965268844867507,0.04965268844867507,0.049824200325872564,0.04990995626447131,0.04990995626447131,0.04990995626447131,0.049995712203070065,0.049995712203070065,0.05008146814166881,0.05016722408026756,0.05016722408026756,0.05051024783466255,0.050596003773261296,0.050596003773261296,0.05102478346625504,0.05102478346625504,0.05102478346625504,0.051110539404853786,0.051110539404853786,0.051110539404853786,0.051110539404853786,0.051110539404853786,0.05179658691364377,0.05179658691364377,0.05205385472944001,0.05239687848383501,0.05239687848383501,0.05239687848383501,0.0525683903610325,0.052825658176828744,0.052825658176828744,0.05291141411542749,0.05299717005402624,0.05299717005402624,0.05316868193122374,0.05316868193122374,0.05325443786982249,0.05325443786982249,0.05325443786982249,0.05325443786982249,0.05325443786982249,0.053340193808421234,0.053340193808421234,0.053340193808421234,0.053340193808421234,0.053340193808421234,0.05342594974701998,0.05342594974701998,0.05351170568561873,0.053597461624217475,0.0545407769488037,0.0545407769488037,0.054626532887402456,0.05531258039619243,0.05531258039619243,0.05531258039619243,0.05531258039619243,0.055484092273389934,0.05556984821198868,0.05556984821198868,0.05556984821198868,0.05565560415058743,0.05565560415058743,0.05565560415058743,0.055741360089186176,0.056084383843581165,0.056084383843581165,0.056084383843581165,0.05634165165937741,0.05634165165937741,0.056598919475173655,0.056598919475173655,0.0566846754137724,0.0566846754137724,0.056856187290969896,0.056856187290969896,0.05694194322956865,0.05694194322956865,0.05719921104536489,0.05737072292256239,0.05754223479975988,0.05762799073835863,0.05762799073835863,0.05762799073835863,0.05762799073835863,0.057971014492753624,0.05977188920332733,0.05977188920332733,0.05977188920332733,0.059857645141926076,0.059857645141926076,0.059857645141926076,0.059857645141926076,0.06002915701912358,0.06002915701912358,0.06148700797530229,0.06148700797530229,0.06148700797530229,0.06148700797530229,0.062087299545493524,0.062087299545493524,0.062087299545493524,0.06225881142269102,0.06225881142269102,0.06225881142269102,0.06234456736128977,0.06234456736128977,0.06234456736128977,0.06243032329988852,0.06268759111568475,0.06277334705428352,0.06320212674727725,0.06337363862447475,0.06337363862447475,0.06345939456307349,0.06354515050167224,0.06371666237886973,0.06414544207186347,0.06414544207186347,0.0652602692736472,0.06534602521224595,0.06534602521224595,0.06534602521224595,0.06543178115084469,0.06551753708944344,0.06560329302804219,0.06560329302804219,0.06568904896664093,0.06594631678243719,0.06594631678243719,0.06594631678243719,0.06611782865963468,0.06646085241402967,0.06654660835262842,0.06654660835262842,0.06663236429122717,0.06663236429122717,0.06663236429122717,0.06671812022982591,0.06671812022982591,0.06671812022982591,0.06671812022982591,0.06680387616842466,0.06688963210702341,0.06688963210702341,0.06697538804562216,0.06697538804562216,0.0672326558614184,0.0672326558614184,0.06748992367721465,0.0675756796158134,0.0675756796158134,0.0677471914930109,0.06817597118600463,0.06817597118600463,0.06817597118600463,0.06826172712460338,0.06869050681759711,0.06877626275619586,0.06894777463339337,0.06894777463339337,0.06903353057199212,0.06903353057199212,0.06920504244918961,0.0695480662035846,0.0695480662035846,0.0697195780807821,0.06980533401938084,0.06980533401938084,0.06997684589657834,0.07014835777377583,0.07031986965097332,0.07031986965097332,0.07040562558957207,0.07040562558957207,0.0715204527913558,0.0715204527913558,0.07220650030014579,0.07254952405454078,0.07254952405454078,0.07254952405454078,0.07254952405454078,0.07254952405454078,0.07289254780893577,0.07289254780893577,0.07306405968613326,0.07306405968613326,0.07314981562473201,0.07314981562473201,0.07314981562473201,0.07323557156333076,0.07323557156333076,0.0733213275019295,0.0733213275019295,0.0736643512563245,0.0740073750107195,0.07409313094931824,0.07417888688791699,0.07426464282651574,0.07460766658091073,0.07486493439670697,0.07520795815110197,0.07546522596689821,0.0758082497212932,0.0758082497212932,0.07589400565989195,0.07632278535288568,0.07640854129148443,0.07700883286167567,0.07718034473887317,0.07735185661607066,0.0776091244318669,0.0782951719406569,0.07846668381785439,0.07940999914244061,0.07949575508103936,0.07966726695823685,0.08052482634422434,0.08069633822142183,0.08326901637938428,0.08344052825658177,0.08361204013377926,0.0838693079495755,0.084040819826773,0.0842123317039705,0.084383843581168,0.08446959951976675,0.08464111139696424,1.0],\"y\":[0.0,0.08907945397269863,0.08942947147357368,0.08942947147357368,0.0896044802240112,0.0896044802240112,0.09012950647532376,0.09012950647532376,0.09030451522576129,0.09100455022751137,0.0911795589779489,0.09152957647882394,0.09222961148057403,0.09327966398319916,0.09345467273363668,0.09345467273363668,0.09345467273363668,0.09345467273363668,0.09380469023451173,0.09397969898494925,0.09450472523626181,0.09555477773888695,0.09572978648932447,0.09572978648932447,0.09660483024151208,0.09765488274413721,0.09870493524676234,0.09905495274763738,0.0992299614980749,0.09940497024851243,0.1023801190059503,0.10255512775638782,0.10255512775638782,0.10290514525726287,0.10360518025901296,0.1064053202660133,0.10658032901645083,0.10658032901645083,0.10675533776688835,0.10920546027301366,0.10955547777388869,0.11340567028351417,0.11375568778438921,0.11393069653482674,0.11428071403570178,0.11673083654182709,0.11708085404270213,0.1176058802940147,0.11813090654532726,0.11848092404620231,0.11883094154707735,0.11970598529926496,0.11970598529926496,0.12005600280014,0.12023101155057753,0.12093104655232761,0.12198109905495275,0.12215610780539027,0.12408120406020301,0.12425621281064053,0.12443122156107805,0.12460623031151558,0.12548127406370319,0.12705635281764088,0.12740637031851593,0.1279313965698285,0.12810640532026601,0.13458172908645433,0.13475673783689185,0.13545677283864194,0.13563178158907946,0.13720686034301716,0.13860693034651733,0.14193209660483025,0.1471823591179559,0.15645782289114454,0.15838291914595728,0.1585579278963948,0.15978298914945746,0.16660833041652082,0.17430871543577178,0.17535876793839691,0.17535876793839691,0.18253412670633531,0.18270913545677284,0.1839341967098355,0.18463423171158558,0.18585929296464823,0.18603430171508575,0.18655932796639832,0.18760938046902345,0.18970948547427371,0.19408470423521176,0.19408470423521176,0.1951347567378369,0.20406020301015051,0.2063353167658383,0.2070353517675884,0.21176058802940148,0.21386069303465174,0.21631081554077705,0.21701085054252714,0.2182359117955898,0.22873643682184108,0.2289114455722786,0.23031151557577878,0.23503675183759187,0.2367868393419671,0.24273713685684284,0.24291214560728036,0.24781239061953098,0.2488624431221561,0.24973748687434372,0.24991249562478124,0.25726286314315716,0.25813790689534477,0.2583129156457823,0.26128806440322017,0.263913195659783,0.26513825691284565,0.2751137556877844,0.2752887644382219,0.2789639481974099,0.2793139656982849,0.2807140357017851,0.28106405320266015,0.2901645082254113,0.2908645432271614,0.2910395519775989,0.2952397619880994,0.2989149457472874,0.30171508575428774,0.3108155407770389,0.31151557577878897,0.31834091704585227,0.3186909345467273,0.320266013300665,0.32044102205110253,0.3218410920546027,0.3227161358067903,0.324291214560728,0.3249912495624781,0.32761638081904093,0.33181659082954146,0.331991599579979,0.33426671333566677,0.33496674833741685,0.3479173958697935,0.348092404620231,0.3494924746237312,0.3496674833741687,0.3503675183759188,0.35176758837941896,0.3519425971298565,0.3608680434021701,0.36139306965348267,0.368218410920546,0.36839341967098355,0.3694434721736087,0.3704935246762338,0.37171858592929646,0.3806440322016101,0.3846692334616731,0.3848442422121106,0.3853692684634232,0.3864193209660483,0.38659432971648583,0.3881694084704235,0.3944697234861743,0.396044802240112,0.39621981099054954,0.3967448372418621,0.3983199159957998,0.3990199509975499,0.4046202310115506,0.4047952397619881,0.41302065103255164,0.41319565978298917,0.41319565978298917,0.4135456772838642,0.41634581729086456,0.4168708435421771,0.42352117605880296,0.4285964298214911,0.42912145607280366,0.42982149107455375,0.4299964998249913,0.43524676233811693,0.4357717885894295,0.4396219810990549,0.4410220511025551,0.44137206860343015,0.4488974448722436,0.4497724886244312,0.45029751487574377,0.4537976898844942,0.45502275113755686,0.4583479173958698,0.4586979348967448,0.4593979698984949,0.46289814490724535,0.4639481974098705,0.464123206160308,0.464123206160308,0.4648232411620581,0.46779838991949596,0.4690234511725586,0.46937346867343366,0.4767238361918096,0.4770738536926846,0.47742387119355967,0.4782989149457473,0.4847742387119356,0.4896744837241862,0.4936996849842492,0.4968498424921246,0.4982499124956248,0.5035001750087504,0.5063003150157508,0.5131256562828141,0.5133006650332517,0.5140007000350018,0.5206510325516276,0.5210010500525026,0.5245012250612531,0.5264263213160658,0.5266013300665033,0.5288764438221911,0.5330766538326916,0.5407770388519426,0.5413020651032552,0.543927196359818,0.5444522226111306,0.5462023101155058,0.5465523276163808,0.5474273713685684,0.5493524676233812,0.5500525026251313,0.5502275113755688,0.56002800140007,0.5603780189009451,0.563703185159258,0.564053202660133,0.5707035351767589,0.5708785439271964,0.5710535526776339,0.5715785789289465,0.5817290864543228,0.5820791039551978,0.5827791389569479,0.583829191459573,0.5899544977248863,0.5904795239761989,0.5906545327266364,0.5913545677283865,0.5913545677283865,0.5948547427371369,0.5953797689884495,0.5990549527476374,0.599229961498075,0.599929996499825,0.6016800840042003,0.6020301015050753,0.6030801540077004,0.603255162758138,0.6044802240112006,0.6051802590129507,0.6062303115155758,0.6069303465173259,0.607980399019951,0.6099054952747638,0.615680784039202,0.6158557927896395,0.6163808190409521,0.6186559327966399,0.6202310115505776,0.6207560378018901,0.6211060553027652,0.6260063003150157,0.6265313265663283,0.6274063703185159,0.628456422821141,0.6291564578228911,0.6291564578228911,0.6312565628281414,0.632131606580329,0.6337066853342667,0.6347567378368918,0.6410570528526426,0.6422821141057052,0.644207210360518,0.6454322716135806,0.6471823591179559,0.648582429121456,0.6489324466223311,0.6501575078753937,0.6534826741337066,0.6541827091354567,0.6552327616380819,0.6557577878893944,0.660658032901645,0.6652082604130206,0.6653832691634581,0.6666083304165208,0.6666083304165208,0.6669583479173958,0.6676583829191459,0.6678333916695834,0.6681834091704585,0.6697584879243962,0.6699334966748337,0.6709835491774588,0.6771088554427721,0.6799089954497725,0.68008400420021,0.6809590479523976,0.6865593279663983,0.6867343367168358,0.6874343717185859,0.687784389219461,0.6888344417220861,0.6890094504725236,0.6890094504725236,0.6911095554777739,0.6947847392369618,0.6967098354917746,0.7126356317815891,0.7128106405320266,0.7128106405320266,0.7129856492824641,0.72033601680084,0.7215610780539027,0.7233111655582779,0.7238361918095905,0.7241862093104655,0.7269863493174659,0.7271613580679034,0.7285614280714036,0.7359117955897795,0.7520126006300315,0.7553377668883444,0.7597129856492825,0.7611130556527826,0.7614630731536577,0.7616380819040952,0.7625131256562828,0.7632131606580329,0.7632131606580329,0.7633881694084704,0.764263213160658,0.7644382219110956,0.7647882394119706,0.7684634231711586,0.7684634231711586,0.7684634231711586,0.7691634581729087,0.7721386069303465,0.7730136506825341,0.7735386769338467,0.7737136856842842,0.7745887294364718,0.7754637731886594,0.775638781939097,0.7766888344417221,0.7772138606930347,0.7775638781939097,0.7779138956947848,0.7786139306965348,0.7793139656982849,0.7922646132306616,0.7924396219810991,0.7927896394819741,0.7938396919845992,0.7940147007350368,0.7950647532376619,0.7952397619880994,0.7959397969898495,0.7962898144907246,0.7978648932446623,0.8003150157507876,0.8008400420021001,0.8031151557577879,0.8032901645082254,0.803465173258663,0.803815190759538,0.8039901995099755,0.8141407070353518,0.8151907595379769,0.815890794539727,0.8179908995449773,0.81991599579979,0.8209660483024152,0.8228911445572279,0.8237661883094155,0.8241162058102905,0.8244662233111656,0.8249912495624782,0.8283164158207911,0.8330416520826042,0.8333916695834792,0.8346167308365419,0.8377668883444173,0.8423171158557928,0.8470423521176059,0.8479173958697935,0.8487924396219811,0.8512425621281065,0.8526426321316066,0.8528176408820441,0.8535176758837942,0.8538676933846693,0.8552677633881695,0.8556177808890445,0.8559677983899195,0.8566678333916696,0.8570178508925447,0.8587679383969199,0.8599929996499825,0.8603430171508576,0.8619180959047953,0.8633181659082955,0.8641932096604831,0.8647182359117956,0.8654182709135457,0.8706685334266714,0.8722436121806091,0.8732936646832342,0.8764438221911095,0.8788939446972348,0.8809940497024851,0.8811690584529226,0.8827441372068603,0.8829191459572978,0.8841442072103605,0.884319215960798,0.884319215960798,0.8857192859642982,0.8858942947147357,0.8865943297164858,0.8867693384669233,0.8890444522226111,0.8899194959747987,0.8900945047252362,0.8948197409870493,0.8953447672383619,0.8956947847392369,0.8958697934896744,0.896044802240112,0.8965698284914245,0.8969198459922996,0.8970948547427371,0.900070003500175,0.9002450122506125,0.9009450472523626,0.9021701085054252,0.9025201260063003,0.9026951347567378,0.9033951697584879,0.9063703185159258,0.9065453272663633,0.9067203360168008,0.9072453622681134,0.9074203710185509,0.9077703885194259,0.908470423521176,0.9088204410220511,0.9089954497724886,0.9100455022751137,0.9102205110255512,0.9117955897794889,0.9123206160308015,0.9135456772838642,0.9137206860343017,0.9142457122856142,0.9144207210360518,0.9144207210360518,0.9154707735386769,0.916170808540427,0.9163458172908645,0.9182709135456772,0.9184459222961148,0.9189709485474273,0.9235211760588029,0.924221211060553,0.9252712635631781,0.9254462723136156,0.9257962898144907,0.9257962898144907,0.9264963248162408,0.9266713335666783,0.9268463423171158,0.9270213510675533,0.9273713685684284,0.9273713685684284,0.9287714385719286,0.9289464473223661,0.9296464823241162,0.9308715435771788,0.931921596079804,0.9334966748337417,0.9336716835841792,0.9336716835841792,0.9340217010850542,0.9361218060903045,0.936296814840742,0.936296814840742,0.9371718585929296,0.9385719285964298,0.9390969548477424,0.9392719635981799,0.9425971298564928,0.9431221561078054,0.943997199859993,0.9441722086104305,0.9455722786139307,0.9518725936296815,0.952397619880994,0.9527476373818691,0.9530976548827441,0.9532726636331816,0.9539726986349317,0.9541477073853692,0.9543227161358068,0.9544977248862443,0.9574728736436822,0.9579978998949947,0.9581729086454323,0.9585229261463073,0.9634231711585579,0.9635981799089954,0.9639481974098705,0.964123206160308,0.9649982499124956,0.9651732586629331,0.9651732586629331,0.9655232761638082,0.9658732936646832,0.9662233111655583,0.9676233811690584,0.967798389919496,0.9688484424221211,0.9688484424221211,0.9690234511725586,0.9695484774238712,0.9695484774238712,0.9695484774238712,0.9705985299264963,0.9707735386769338,0.975848792439622,0.976198809940497,0.9765488274413721,0.9767238361918096,0.9767238361918096,0.9772488624431221,0.9775988799439972,0.9779488974448722,0.9781239061953098,0.9782989149457473,0.9786489324466223,0.9793489674483724,0.9800490024501225,0.9807490374518726,0.9809240462023101,0.9812740637031852,0.9812740637031852,0.9814490724536227,0.9817990899544977,0.9819740987049352,0.9821491074553728,0.9828491424571228,0.9833741687084354,0.9833741687084354,0.984249212460623,0.9845992299614981,0.9845992299614981,0.9847742387119356,0.9852992649632482,0.9859992999649982,0.9865243262163108,0.9865243262163108,0.9866993349667483,0.9866993349667483,0.9870493524676234,0.9872243612180609,0.9875743787189359,0.9880994049702485,0.9886244312215611,0.9891494574728736,0.9896744837241862,0.9898494924746237,0.9898494924746237,0.9901995099754988,0.9908995449772489,0.9908995449772489,0.9910745537276864,0.9910745537276864,0.9910745537276864,0.991599579978999,0.991599579978999,0.9921246062303115,0.992299614980749,0.9924746237311866,0.9926496324816241,0.9931746587329366,0.9935246762338117,0.9940497024851243,0.9947497374868743,0.9950997549877494,0.9952747637381869,0.995974798739937,0.9961498074903745,0.9964998249912496,0.9970248512425621,0.9971998599929996,0.9971998599929996,0.9975498774938747,0.9975498774938747,0.9978998949947497,0.9980749037451873,0.9984249212460623,0.9985999299964998,0.9987749387469373,0.9989499474973749,0.9989499474973749,0.9991249562478124,0.9991249562478124,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.9994749737486874,0.999649982499125,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"XGBoost (AUC = 0.98)\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.00034302375439499187,0.00034302375439499187,0.00034302375439499187,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0005145356315924878,0.0005145356315924878,0.0005145356315924878,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0008575593859874796,0.0008575593859874796,0.0008575593859874796,0.0010290712631849757,0.0010290712631849757,0.0010290712631849757,0.0011148272017837235,0.0012005831403824716,0.0012863390789812194,0.0012863390789812194,0.0012863390789812194,0.0012863390789812194,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0014578509561787153,0.0014578509561787153,0.0014578509561787153,0.0014578509561787153,0.0015436068947774634,0.0015436068947774634,0.0015436068947774634,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0017151187719749593,0.0017151187719749593,0.0017151187719749593,0.0017151187719749593,0.0018008747105737071,0.0018866306491724552,0.0018866306491724552,0.0018866306491724552,0.0019723865877712033,0.002143898464968699,0.002229654403567447,0.002229654403567447,0.002229654403567447,0.002229654403567447,0.002315410342166195,0.002658434096561187,0.002829945973758683,0.002829945973758683,0.002829945973758683,0.0029157019123574306,0.003172969728153675,0.0032587256667524224,0.0032587256667524224,0.0032587256667524224,0.0033444816053511705,0.0033444816053511705,0.0033444816053511705,0.0034302375439499186,0.0034302375439499186,0.0035159934825486666,0.0036017494211474143,0.0036875053597461623,0.0036875053597461623,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0038590172369436584,0.0038590172369436584,0.004030529114141155,0.004116285052739903,0.004116285052739903,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.00463082068433239,0.00463082068433239,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.00505960037732613,0.00505960037732613,0.00505960037732613,0.005231112254523626,0.005316868193122374,0.005316868193122374,0.005316868193122374,0.005316868193122374,0.005402624131721122,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005745647886116113,0.005831403824714861,0.005831403824714861,0.005917159763313609,0.005917159763313609,0.005917159763313609,0.005917159763313609,0.0061744275791098535,0.0061744275791098535,0.0061744275791098535,0.006260183517708602,0.006260183517708602,0.00634593945630735,0.00634593945630735,0.006431695394906097,0.006431695394906097,0.006431695394906097,0.006517451333504845,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006774719149301089,0.006774719149301089,0.006774719149301089,0.0072034988422948285,0.007289254780893577,0.007460766658091073,0.007546522596689821,0.00797530228968356,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008747105737072292,0.008747105737072292,0.00883286167567104,0.00883286167567104,0.00883286167567104,0.008918617614269788,0.008918617614269788,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009175885430066031,0.00926164136866478,0.009347397307263527,0.009433153245862276,0.009433153245862276,0.009433153245862276,0.009604665123059773,0.009604665123059773,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010290712631849755,0.010376468570448504,0.010376468570448504,0.010547980447646,0.010547980447646,0.010547980447646,0.010633736386244748,0.010633736386244748,0.010719492324843495,0.010719492324843495,0.010719492324843495,0.011234027956435983,0.011234027956435983,0.011319783895034732,0.011319783895034732,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.011577051710830975,0.011577051710830975,0.011662807649429723,0.011662807649429723,0.011748563588028471,0.011834319526627219,0.011834319526627219,0.012005831403824715,0.012177343281022211,0.012348855158219707,0.012348855158219707,0.012348855158219707,0.0126918789126147,0.012777634851213446,0.012777634851213446,0.012863390789812194,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.01303490266700969,0.013120658605608439,0.013120658605608439,0.013292170482805935,0.013292170482805935,0.013292170482805935,0.013635194237200925,0.013806706114398421,0.014235485807392163,0.014235485807392163,0.01432124174599091,0.01432124174599091,0.014406997684589657,0.014835777377583398,0.014835777377583398,0.015178801131978389,0.015178801131978389,0.015178801131978389,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015521824886373381,0.015521824886373381,0.01560758082497213,0.015779092702169626,0.015779092702169626,0.016036360517965868,0.016036360517965868,0.016207872395163365,0.016207872395163365,0.01637938427236086,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016722408026755852,0.0168081639653546,0.0168081639653546,0.01689391990395335,0.01689391990395335,0.017065431781150844,0.017065431781150844,0.017236943658348342,0.017236943658348342,0.01732269959694709,0.01732269959694709,0.017494211474144584,0.017494211474144584,0.017494211474144584,0.01775147928994083,0.01775147928994083,0.01775147928994083,0.017922991167138323,0.017922991167138323,0.018437526798730813,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.018609038675928308,0.018609038675928308,0.018609038675928308,0.018694794614527055,0.018780550553125806,0.018780550553125806,0.018780550553125806,0.019123574307520794,0.01938084212331704,0.01938084212331704,0.019466598061915787,0.019466598061915787,0.019552354000514537,0.019981133693508277,0.019981133693508277,0.019981133693508277,0.020066889632107024,0.020066889632107024,0.02023840150930452,0.020495669325100763,0.02058142526369951,0.02058142526369951,0.020838693079495756,0.020838693079495756,0.02101020495669325,0.021610496526884487,0.021696252465483234,0.021696252465483234,0.02178200840408198,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.022039276219878227,0.022125032158476974,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022468055912871966,0.022468055912871966,0.022896835605865706,0.022982591544464453,0.023239859360260698,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023668639053254437,0.023668639053254437,0.024097418746248177,0.024097418746248177,0.024097418746248177,0.024526198439241916,0.024526198439241916,0.024526198439241916,0.024611954377840667,0.024611954377840667,0.02478346625503816,0.02478346625503816,0.02478346625503816,0.02486922219363691,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.0253837578252294,0.0253837578252294,0.025812537518223138,0.02606980533401938,0.02606980533401938,0.026412829088414372,0.026412829088414372,0.026412829088414372,0.02658434096561187,0.02658434096561187,0.02658434096561187,0.02658434096561187,0.026670096904210617,0.026670096904210617,0.026927364720006862,0.02701312065860561,0.027184632535803104,0.027184632535803104,0.02727038847440185,0.02727038847440185,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.02744190035159935,0.02744190035159935,0.027956435983191835,0.027956435983191835,0.027956435983191835,0.028127947860389333,0.028556727553383073,0.029157019123574306,0.029157019123574306,0.029242775062173057,0.029242775062173057,0.02984306663236429,0.02984306663236429,0.03070062601835177,0.03070062601835177,0.03078638195695052,0.031472429465740505,0.03155818540433925,0.03155818540433925,0.03155818540433925,0.031643941342938,0.031729697281536746,0.031815453220135494,0.03190120915873424,0.03190120915873424,0.03190120915873424,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03250150072892548,0.03250150072892548,0.03250150072892548,0.03275876854472172,0.03284452448332047,0.03284452448332047,0.03284452448332047,0.03455964325529543,0.03455964325529543,0.034731155132492926,0.034731155132492926,0.03730383329045536,0.03738958922905411,0.03738958922905411,0.03781836892204785,0.038075636737844094,0.038075636737844094,0.03833290455364034,0.03833290455364034,0.03841866049223909,0.038847440185232826,0.038847440185232826,0.03893319612383157,0.03901895206243032,0.03901895206243032,0.03927621987822657,0.039361975816825316,0.03961924363262156,0.039704999571220305,0.039704999571220305,0.039704999571220305,0.039704999571220305,0.03996226738701655,0.0400480233256153,0.040219535202812795,0.040219535202812795,0.040219535202812795,0.040562558957207784,0.040562558957207784,0.040562558957207784,0.04064831489580654,0.04064831489580654,0.04064831489580654,0.04064831489580654,0.04081982677300403,0.04081982677300403,0.04090558271160278,0.04090558271160278,0.04099133865020153,0.04116285052739902,0.04116285052739902,0.04116285052739902,0.04150587428179402,0.04150587428179402,0.04167738615899151,0.04167738615899151,0.04210616585198525,0.04210616585198525,0.04210616585198525,0.042191921790584,0.042363433667781496,0.04244918960638024,0.04356401680816396,0.04382128462396021,0.04425006431695395,0.04425006431695395,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.044507332132750196,0.044850355887145185,0.044850355887145185,0.044850355887145185,0.044850355887145185,0.04493611182574393,0.045193379641540174,0.04527913558013893,0.04527913558013893,0.045364891518737675,0.045364891518737675,0.045364891518737675,0.045965183088928906,0.04605093902752765,0.04613669496612641,0.04613669496612641,0.04656547465912014,0.046822742474916385,0.04708001029071263,0.04708001029071263,0.04750878998370637,0.04750878998370637,0.047594545922305116,0.047594545922305116,0.04776605779950262,0.047851813738101365,0.047851813738101365,0.0482805934310951,0.04836634936969385,0.04836634936969385,0.0484521053082926,0.04913815281708258,0.049223908755681334,0.049223908755681334,0.049223908755681334,0.049481176571477575,0.049481176571477575,0.049481176571477575,0.04990995626447131,0.049995712203070065,0.05016722408026756,0.05025298001886631,0.051110539404853786,0.051110539404853786,0.05128205128205128,0.05136780722065003,0.05136780722065003,0.05136780722065003,0.05205385472944001,0.05205385472944001,0.05213961066803876,0.052482634422433755,0.052482634422433755,0.0525683903610325,0.05265414629963125,0.05265414629963125,0.052825658176828744,0.052825658176828744,0.05299717005402624,0.053340193808421234,0.053340193808421234,0.05342594974701998,0.053854729440013724,0.05394048537861247,0.05419775319440871,0.05419775319440871,0.05428350913300746,0.054626532887402456,0.054626532887402456,0.0548838007031987,0.0548838007031987,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05514106851899494,0.05539833633479119,0.05539833633479119,0.05539833633479119,0.05565560415058743,0.055741360089186176,0.05582711602778492,0.05582711602778492,0.05591287196638367,0.05599862790498242,0.056084383843581165,0.056084383843581165,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05642740759797616,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.0566846754137724,0.0566846754137724,0.05677043135237115,0.05677043135237115,0.05677043135237115,0.056856187290969896,0.057113455106766145,0.057113455106766145,0.05728496698396364,0.05737072292256239,0.05737072292256239,0.05737072292256239,0.05737072292256239,0.05779950261555613,0.05814252636995112,0.058485550124346114,0.05942886544893234,0.059600377326129834,0.05968613326472858,0.05977188920332733,0.06157276391390104,0.06157276391390104,0.06183003172969728,0.06183003172969728,0.062087299545493524,0.06225881142269102,0.06225881142269102,0.06371666237886973,0.06371666237886973,0.06431695394906098,0.06431695394906098,0.06440270988765973,0.06474573364205471,0.06474573364205471,0.06483148958065346,0.06491724551925221,0.06603207272103594,0.06646085241402967,0.06646085241402967,0.06654660835262842,0.06654660835262842,0.06663236429122717,0.06663236429122717,0.06663236429122717,0.06671812022982591,0.06697538804562216,0.06714689992281965,0.0672326558614184,0.0672326558614184,0.06731841180001714,0.0674041677386159,0.0674041677386159,0.0675756796158134,0.0675756796158134,0.0675756796158134,0.0677471914930109,0.0677471914930109,0.0677471914930109,0.06809021524740588,0.06809021524740588,0.06826172712460338,0.06834748306320212,0.06834748306320212,0.06843323900180087,0.06843323900180087,0.06843323900180087,0.06851899494039962,0.06851899494039962,0.06869050681759711,0.06886201869479461,0.06894777463339337,0.06903353057199212,0.06903353057199212,0.06911928651059086,0.06920504244918961,0.06920504244918961,0.06929079838778836,0.06929079838778836,0.0697195780807821,0.06980533401938084,0.06997684589657834,0.07040562558957207,0.07057713746676958,0.07066289340536833,0.0713489409141583,0.0715204527913558,0.0715204527913558,0.07263527999313953,0.07280679187033702,0.07314981562473201,0.073492839379127,0.073835863133522,0.073835863133522,0.07392161907212075,0.07392161907212075,0.07409313094931824,0.07435039876511448,0.07435039876511448,0.07443615470371323,0.07452191064231198,0.07452191064231198,0.07460766658091073,0.07460766658091073,0.07486493439670697,0.07546522596689821,0.07555098190549696,0.0759797615984907,0.07606551753708944,0.07623702941428694,0.07649429723008318,0.07666580910728069,0.07692307692307693,0.07880970757224938,0.07923848726524312,0.07932424320384186,0.07966726695823685,0.07975302289683561,0.0800960466512306,0.0802675585284281,0.08069633822142183,0.08078209416002058,0.08232570105479804,0.08249721293199554,0.08344052825658177,0.08412657576537175,0.08472686733556299,0.08489837921276049,0.08498413515135923,0.08515564702855673,0.08592745047594547,0.08644198610753795,0.08687076580053169,0.08704227767772918,0.08738530143212417,0.08747105737072293,0.08807134894091416,0.08841437269530915,0.08901466426550039,0.08910042020409914,0.08927193208129663,0.08952919989709288,0.08970071177429037,0.08987222365148786,0.0901294914672841,0.09107280679187034,0.09124431866906783,0.09150158648486408,0.09201612211645656,0.0921018780550553,0.09227338993225281,0.09235914587085156,0.0926164136866478,0.09295943744104279,0.09313094931824029,0.09321670525683903,0.09338821713403653,0.09347397307263527,0.09364548494983277,0.09390275276562902,0.09407426464282652,0.09433153245862276,0.09450304433582025,0.09467455621301775,0.09836206157276392,0.09853357344996141,0.09870508532715891,0.09879084126575766,0.09913386502015265,0.09947688877454763,0.09999142440614013,0.10024869222193637,0.10050596003773261,0.1008489837921276,0.1010204956693251,0.10119200754652259,0.10136351942372009,0.10187805505531258,0.10213532287110882,0.10230683474830632,0.10256410256410256,0.10307863819569506,0.10325015007289255,0.1033359060114913,0.10367892976588629,0.10393619758168253,0.10402195352028128,0.10419346539747877,0.10436497727467627,0.10453648915187377,0.10539404853786125,0.10547980447645999,0.10590858416945373,0.10642311980104623,0.1072806791870337,0.10736643512563245,0.10770945888002745,0.1077952148186262,0.10796672669582369,0.10805248263442244,0.10856701826601492,0.10882428608181116,0.10933882171340366,0.1094245776520024,0.10985335734499614,0.11302632707314982,0.11328359488894606,0.1133693508275448,0.1138838864591373,0.11396964239773605,0.11414115427493354,0.11422691021353229,0.11439842209072978,0.11456993396792728,0.11482720178372352,0.11499871366092101,0.11517022553811851,0.11577051710830975,0.11654232055569848,0.11662807649429723,0.1176571477574822,0.1178286596346797,0.1180001715118772,0.11817168338907469,0.11851470714346969,0.11860046308206844,0.11877197495926593,0.12005831403824715,0.12022982591544465,0.12057284966983964,0.12100162936283337,0.12108738530143212,0.12134465311722836,0.12143040905582711,0.12177343281022211,0.1219449446874196,0.1221164565646171,0.12245948031901209,0.12357430752079582,0.12383157533659206,0.12400308721378955,0.12443186690678329,0.12451762284538204,0.12477489066117829,0.12494640253837579,0.12511791441557327,0.12520367035417201,0.1253751822313695,0.12563245004716578,0.12700454506474573,0.12709030100334448,0.12726181288054197,0.12743332475773947,0.1278621044507332,0.12846239602092444,0.12880541977531945,0.1288911757139182,0.12931995540691194,0.1300060029157019,0.1301775147928994,0.13189263356487438,0.13206414544207187,0.1326644370122631,0.1330074607666581,0.13360775233684932,0.13369350827544807,0.13386502015264556,0.1341222879684418,0.13437955578423805,0.13472257953863306,0.13635194237200926,0.13832432895978047,0.13858159677557672,0.1394391561615642,0.13961066803876168,0.13995369179315667,0.14012520367035416,0.14046822742474915,0.14063973930194665,0.14098276305634166,0.14115427493353916,0.1412400308721379,0.1414115427493354,0.14149729868793415,0.14218334619672413,0.14226910213532287,0.14261212588971786,0.14295514964411285,0.14389846496869907,0.14424148872309409,0.14467026841608782,0.14484178029328532,0.14492753623188406,0.1456993396792728,0.14578509561787154,0.14655689906526026,0.14672841094245775,0.14741445845124776,0.14835777377583398,0.14844352971443273,0.14904382128462396,0.1491295772232227,0.14955835691621644,0.14990138067061143,0.15007289254780892,0.15024440442500644,0.15153074350398765,0.15204527913558014,0.15238830288997512,0.15316010633736385,0.15333161821456137,0.15401766572335135,0.15418917760054884,0.15444644541634509,0.15496098104793757,0.15513249292513506,0.15564702855672755,0.15581854043392504,0.1567618557585113,0.15719063545150502,0.15744790326730126,0.15779092702169625,0.15830546265328874,0.15839121859188748,0.15873424234628247,0.1591630220392762,0.1593345339164737,0.16010633736386246,0.1601920933024612,0.1603636051796587,0.16079238487265243,0.16130692050424492,0.16139267644284366,0.16156418832004116,0.1616499442586399,0.1619929680130349,0.1621644798902324,0.16242174770602863,0.16276477146042365,0.16293628333762114,0.16345081896921362,0.16362233084641112,0.16370808678500987,0.16387959866220736,0.16456564617099734,0.16482291398679358,0.16499442586399107,0.16610925306577481,0.16619500900437356,0.16670954463596604,0.16713832432895978,0.16722408026755853,0.1682531515307435,0.16833890746934224,0.16868193122373723,0.16902495497813225,0.16936797873252724,0.16953949060972473,0.16962524654832348,0.16979675842552097,0.16988251436411972,0.1703970499957122,0.17065431781150844,0.17091158562730469,0.17099734156590343,0.17134036532029842,0.17142612125889717,0.17176914501329216,0.17236943658348342,0.17245519252208216,0.17296972815367465,0.17331275190806963,0.17382728753966212,0.17391304347826086,0.1746848469256496,0.17485635880284708,0.17511362661864335,0.1751993825572421,0.1755424063116371,0.17579967412743333,0.17597118600463083,0.17605694194322957,0.17622845382042707,0.17657147757482206,0.17674298945201955,0.1770002572678158,0.17811508446959953,0.17837235228539577,0.17854386416259327,0.17862962010119202,0.1788011319783895,0.17888688791698826,0.17905839979418575,0.1791441557327845,0.1794871794871795,0.17974444730297573,0.18008747105737072,0.18017322699596947,0.18051625075036445,0.1809450304433582,0.1812880541977532,0.1814595660749507,0.1873767258382643,0.1875482377154618,0.18823428522425178,0.18857730897864677,0.18909184461023926,0.18926335648743675,0.19046393962781924,0.19192179058399794,0.1920075465225967,0.19217905839979418,0.19355115341737417,0.1938084212331704,0.1939799331103679,0.19500900437355287,0.1952662721893491,0.1954377840665466,0.1959523196981391,0.19620958751393533,0.20126918789126147,0.20144069976845896,0.20229825915444644,0.2181631077952148,0.21833461967241233,0.21850613154960982,0.21867764342680732,0.21893491124260356,0.21927793499699855,0.2193636909355973,0.2216791012777635,0.221850613154961,0.22202212503215848,0.22862533230426207,0.23085498670782953,0.23102649858502702,0.23728668210273562,0.23737243804133437,0.23805848555012435,0.25692479204184887,0.2584683989366264,0.2613841008489838,0.26164136866478005,0.26198439241917504,0.2624989280507675,0.26361375525255126,0.26369951119115,0.26429980276134124,0.26447131463853873,0.26524311808592743,0.2653288740245262,0.2660149215333162,0.26644370122630995,0.26678672498070494,0.26704399279650115,0.2674727724894949,0.26755852842809363,0.2703884744018523,0.27527656290198094,0.275619586656376,0.2775062173055484,0.2778492410599434,0.2781922648143384,0.2782780207529371,0.2787068004459309,0.27896406826172715,0.28025040734070833,0.2811079667266958,0.28453820427064574,0.28462396020924446,0.2881399536917932,0.2883972215075894,0.2887402452619844,0.29431438127090304,0.29440013720950176,0.30168939199039535,0.30177514792899407,0.3025469513763828,0.30434782608695654,0.3050338735957465,0.3054626532887402,0.3205556984821199,0.32201354943829863,0.3221850613154961,0.3228711088242861,0.325272275105051,0.3282737329560072,0.3352199639825058,0.3509990566846754,0.3510848126232742,0.35202812794786037,0.35245690764085413,0.3609467455621302,0.3613755252551239,0.36146128119372267,0.3634336677814939,0.36643512563245006,0.36677814938684505,0.39473458537003686,0.3956779006946231,0.3959351685104193,0.3961066803876168,0.39644970414201186,0.3965354600806106,0.39687848383500557,0.39713575165080184,0.41505874281794014,0.41523025469513763,0.4157447903267301,0.42080439070405623,0.42449189606380244,0.4251779435725924,0.4271503301603636,0.42757910985335734,0.42792213360775233,0.4410427922133608,0.4411285481519595,0.4415573278449533,0.4509904810908155,0.4585370036875054,0.45922305119629536,0.4593088071348941,0.461023925906869,0.463768115942029,0.4768030186090387,0.47697453048623617,0.48923762970585716,0.49198181974101707,0.4966983963639482,0.4968699082411457,0.4986707829517194,0.4987565388903182,0.5034731155132492,0.5035588714518481,0.5245690764085413,0.5281708258296887,0.5283423377068862,0.5289426292770775,0.5304004802332561,0.5304862361718549,0.5307435039876511,0.5343452534087986,0.5351170568561873,0.5487522510933882,0.5488380070319869,0.5616156418832005,0.5660749506903353,0.5681330932167052,0.5691621644798902,0.570276991681674,0.5744790326730126,0.5745647886116113,0.5748220564274076,0.5775662464625675,0.598833719235057,0.5989194751736557,0.6128976931652517,0.6130692050424492,0.6249892805076751,0.6407683732098448,0.6640082325701054,0.6673527141754566,0.6680387616842466,0.6681245176228454,0.6684675413772404,0.6685532973158391,0.6687248091930366,0.6690678329474317,0.6759283080353314,0.6875053597461624,0.715204527913558,0.7153760397907555,0.715547551667953,0.716233599176743,0.7246376811594203,0.7248091930366178,0.7249807049138153,0.7251522167910128,0.7286682102735614,0.7287539662121602,0.7300403052911414,0.7338135665894864,0.7354429294228625,0.73561444130006,0.7357859531772575,0.7385301432124175,0.7405882857387874,0.7423891604493611,0.744447302975731,0.7460766658091073,0.7464196895635022,0.7465912014406998,0.7469342251950948,0.747191493010891,0.749592659291656,0.7500214389846497,0.7501071949232484,0.7507932424320384,0.7513077780636309,0.7513935340022296,0.7519080696338222,0.7531086527742046,0.7534516765285996,0.7536231884057971,0.7540519680987908,0.7719749592659292,0.7855243975645313,0.7856959094417288,0.7860389331961238,0.7866392247663151,0.7876682960295001,0.7877540519680988,0.7910127776348512,0.79109853357345,0.7948717948717948,0.7953005745647886,0.7953863305033874,0.7959008661349799,0.7959866220735786,0.7964154017665723,0.7965869136437699,0.796844181459566,0.7970156933367636,0.7971014492753623,0.7976159849069548,0.7984735442929423,0.7987308121087385,0.7990738358631335,0.7992453477403311,0.7995883714947261,0.800274419003516,0.8007889546351085,0.8013892462052997,0.8017322699596947,0.801989537775491,0.8021610496526884,0.8027613412228797,0.8035331446702684,0.8056770431352371,0.8061915787668296,0.8075636737844095,0.8088500128633908,0.8091930366177857,0.8092787925563846,0.809450304433582,0.8100505960037733,0.810136351942372,0.810822399451162,0.8109939113283595,0.8127947860389332,0.8128805419775319,0.8135665894863219,0.8138238573021181,0.8150244404425007,0.8151959523196981,0.8154532201354944,0.8155389760740931,0.8158819998284881,0.8160535117056856,0.8176828745390619,0.8178543864162593,0.8182831661092531,0.8183689220478518,0.8187977017408455,0.8188834576794443,0.8190549695566418,0.8200840408198268,0.8202555526970242,0.8209416002058143,0.821713403653203,0.8235142783637767,0.8247148615041592,0.8249721293199554,0.8251436411971529,0.8252293971357516,0.8257439327673441,0.8259154446445416,0.8272017837235228,0.8289169024954978,0.8290026584340966,0.8297744618814853,0.829860217820084,0.831232312837664,0.8314038247148615,0.831575336592059,0.8317468484692565,0.8320041162850528,0.833633479118429,0.8338907469342252,0.833976502872824,0.8377497641711689,0.8384358116799588,0.8385215676185576,0.8388645913729526,0.8401509304519338,0.8404081982677301,0.840751222022125,0.841437269530915,0.8416087814081125,0.84178029328531,0.8428951204870937,0.8430666323642912,0.8438384358116799,0.8439241917502787,0.8456393105222537,0.8458108223994512,0.8462396020924449,0.8463253580310437,0.8464968699082411,0.8465826258468399,0.8468398936626361,0.8473544292942287,0.8475259411714261,0.8483835005574136,0.8484692564960123,0.8594460166366521,0.8601320641454421,0.8605608438384358,0.8641625932595832,0.8643341051367808,0.8652774204613669,0.8657062001543607,0.8667352714175457,0.8699082411456993,0.8713660921018781,0.8719663836720692,0.872738187119458,0.8728239430580568,0.8733384786896493,0.8761684246634079,0.8763399365406055,0.8767687162335992,0.8791698825143641,0.8799416859617528,0.881571048795129,0.8819998284881228,0.8845725066460852,0.886030357602264,0.8895463510848126,0.8898893748392076,0.8902323985936026,0.8903181545322013,0.8939199039533487,0.8951204870937313,0.8952919989709287,0.8970071177429036,0.8971786296201012,0.9003515993482548,0.9017236943658349,0.9018952062430323,0.9021524740588286,0.9025812537518223,0.9056684675413772,0.9082411456993397,0.9111568476116971,0.9112426035502958,0.914930108910042,0.9151016207872396,0.9178458108223995,0.9180173226995969,0.9182745905153932,0.9186176142697882,0.9193894177171769,0.9195609295943744,0.926850184375268,0.9347397307263527,0.9351685104193466,0.9616670954463596,0.9641540176657234,0.9644112854815196,0.9658691364376983,0.9662121601920933,0.9664694280078896,0.9668124517622846,0.966983963639482,0.9692136180430495,0.9692993739816482,0.9698139096132408,0.9699854214904382,0.9700711774290369,0.9703284452448332,0.9706714689992282,0.9707572249378269,0.9709287368150245,0.9781322356573192,0.978217991595918,0.9839636394820341,0.9843066632364291,0.9844781751136266,0.9845639310522254,0.9852499785610154,0.9856787582540091,0.987736900780379,0.9879941685961753,0.9885087042277678,0.9885944601663665,0.9893662636137552,0.9896235314295515,0.989795043306749,0.9898807992453478,0.9901380670611439,0.9903095789383415,0.9905668467541378,0.9909956264471315,0.9916816739559214,0.9917674298945202,0.992539233341909,0.992882257096304,0.9935683046050939,0.9940828402366864,0.9941685961752852,0.9943401080524826,0.9945973758682789,0.9947688877454763,0.9951119114998713,0.9951976674384702,0.9963124946402538,0.9963982505788526,0.9966555183946488,0.9973415659034388,0.9976845896578338,0.9983706371666238,0.99862790498242,0.998970928736815,0.9994854643684075,0.9995712203070063,1.0],\"y\":[0.0,0.00035001750087504374,0.0005250262513125656,0.001400070003500175,0.0017500875043752187,0.002450122506125306,0.002625131256562828,0.002975148757437872,0.003325166258312916,0.0036751837591879593,0.0038501925096254812,0.004550227511375569,0.004725236261813091,0.005075253762688134,0.0056002800140007,0.005950297514875744,0.006300315015750788,0.0077003850192509625,0.008050402520126006,0.008750437521876094,0.00927546377318866,0.009625481274063704,0.011025551277563878,0.011725586279313966,0.011900595029751488,0.012250612530626532,0.013125656282814141,0.013300665033251663,0.013650682534126707,0.014175708785439271,0.014350717535876793,0.015050752537626881,0.015400770038501925,0.015575778788939447,0.01662583129156458,0.017325866293314666,0.017325866293314666,0.018375918795939798,0.018725936296814842,0.018725936296814842,0.01942597129856493,0.01960098004900245,0.019950997549877492,0.020826041302065102,0.021351067553377668,0.021701085054252712,0.02327616380819041,0.023626181309065453,0.023801190059502975,0.02520126006300315,0.02520126006300315,0.025726286314315717,0.027826391319565977,0.02887644382219111,0.029576478823941196,0.030101505075253762,0.03080154007700385,0.031151557577878894,0.03237661883094155,0.03255162758137907,0.033076653832691635,0.03447672383619181,0.034826741337066855,0.03902695134756738,0.03972698634931746,0.04060203010150507,0.040777038851942594,0.04112705635281764,0.04130206510325516,0.04235211760588029,0.043402170108505424,0.044277213860693033,0.044452222611130555,0.048127406370318516,0.04917745887294365,0.05110255512775639,0.051977598879944,0.053377668883444175,0.05372768638431922,0.05390269513475674,0.05390269513475674,0.05407770388519426,0.05460273013650683,0.054777738886944344,0.05530276513825691,0.05652782639131956,0.056702835141757085,0.05722786139306965,0.05775288764438222,0.05827791389569478,0.05862793139656983,0.05880294014700735,0.0616030801540077,0.06177808890444522,0.06230311515575779,0.06370318515925796,0.06370318515925796,0.06492824641232062,0.06510325516275814,0.0656282814140707,0.06580329016450823,0.06650332516625831,0.06685334266713336,0.06737836891844592,0.06772838641932097,0.06807840392019601,0.06825341267063353,0.06860343017150858,0.0687784389219461,0.07000350017500875,0.07070353517675884,0.07087854392719636,0.07157857892894645,0.07262863143157158,0.07297864893244663,0.07350367518375919,0.07367868393419671,0.07402870143507176,0.07507875393769689,0.07525376268813441,0.07577878893944698,0.07962898144907245,0.08032901645082254,0.0808540427021351,0.0808540427021351,0.0808540427021351,0.08732936646832341,0.08767938396919846,0.08837941897094855,0.08872943647182359,0.0992299614980749,0.10850542527126357,0.11008050402520125,0.11025551277563878,0.11130556527826391,0.11148057402870143,0.11830591529576479,0.1191809590479524,0.12198109905495275,0.12285614280714036,0.12373118655932797,0.12443122156107805,0.12530626531326566,0.12863143157157858,0.1288064403220161,0.1288064403220161,0.13773188659432972,0.13790689534476724,0.1382569128456423,0.13983199159957999,0.14700735036751839,0.147882394119706,0.14805740287014352,0.1487574378718936,0.14928246412320617,0.15838291914595728,0.16275813790689533,0.16275813790689533,0.16835841792089604,0.16870843542177108,0.1688834441722086,0.1697584879243962,0.16993349667483373,0.17028351417570878,0.17098354917745887,0.17185859292964648,0.17220861043052152,0.17238361918095904,0.17273363668183409,0.17938396919845992,0.18043402170108505,0.1807840392019601,0.18130906545327266,0.18253412670633531,0.18288414420721036,0.18760938046902345,0.18813440672033602,0.18830941547077354,0.19723486174308716,0.19793489674483725,0.2031851592579629,0.20336016800840043,0.20406020301015051,0.20406020301015051,0.2047602380119006,0.20598529926496326,0.2063353167658383,0.20651032551627582,0.21141057052852644,0.21176058802940148,0.211935596779839,0.21701085054252714,0.21963598179908994,0.21981099054952746,0.2201610080504025,0.22068603430171507,0.22313615680784038,0.22996149807490374,0.23013650682534126,0.23083654182709135,0.2313615680784039,0.2376618830941547,0.23888694434721736,0.23906195309765488,0.24151207560378019,0.24221211060553027,0.2432621631081554,0.24361218060903045,0.24833741687084354,0.25568778438921946,0.255862793139657,0.25726286314315716,0.2574378718935947,0.2577878893944697,0.2590129506475324,0.2591879593979699,0.2607630381519076,0.2609380469023451,0.2637381869093455,0.2728386419320966,0.27458872943647183,0.2752887644382219,0.27633881694084705,0.2766888344417221,0.27703885194259714,0.27721386069303466,0.2775638781939097,0.2782639131956598,0.27948897444872245,0.283689184459223,0.28438921946097306,0.2845642282114106,0.29051452572628633,0.2933146657332867,0.29401470073503677,0.29558977948897447,0.3055652782639132,0.31326566328316413,0.31414070703535174,0.31571578578928944,0.3160658032901645,0.31659082954147705,0.3295414770738537,0.3297164858242912,0.3304165208260413,0.3307665383269163,0.33094154707735385,0.33339166958347916,0.3335666783339167,0.3375918795939797,0.3377668883444172,0.3379418970948547,0.33811690584529225,0.3384669233461673,0.33881694084704234,0.33899194959747986,0.34196709835491773,0.3424921246062303,0.34284214210710534,0.34354217710885543,0.34371718585929295,0.34441722086104304,0.34529226461323065,0.3456422821141057,0.3472173608680434,0.35404270213510675,0.35421771088554427,0.3545677283864193,0.3552677633881694,0.3552677633881694,0.3554427721386069,0.3584179208960448,0.35964298214910745,0.3615680784039202,0.3704935246762338,0.37066853342667133,0.3781939096954848,0.3783689184459223,0.3790689534476724,0.37941897094854743,0.37959397969898495,0.38029401470073504,0.3806440322016101,0.38116905845292265,0.3853692684634232,0.3857192859642982,0.3862443122156108,0.3862443122156108,0.3864193209660483,0.38851942597129857,0.3886944347217361,0.3893944697234862,0.3976198809940497,0.39796989849492476,0.3984949247462373,0.39866993349667484,0.3990199509975499,0.3991949597479874,0.3997199859993,0.400070003500175,0.40182009100455024,0.40287014350717537,0.40357017850892546,0.40444522226111307,0.4047952397619881,0.40514525726286316,0.4086454322716136,0.4096954847742387,0.4109205460273014,0.41144557227861395,0.41319565978298917,0.4135456772838642,0.41389569478473925,0.41879593979698987,0.4229961498074904,0.42824641232061605,0.42964648232411623,0.4299964998249913,0.4326216310815541,0.43367168358417924,0.4343717185859293,0.4366468323416171,0.43752187609380466,0.4376968848442422,0.4387469373468673,0.4396219810990549,0.440322016100805,0.44067203360168006,0.4427721386069303,0.44294714735736784,0.44452222611130554,0.44452222611130554,0.44469723486174306,0.44697234861743085,0.44767238361918094,0.44784739236961846,0.4515225761288064,0.45414770738536925,0.4609730486524326,0.46132306615330765,0.4616730836541827,0.46219810990549526,0.4670983549177459,0.47234861743087153,0.4728736436821841,0.48022401120056,0.48039901995099754,0.48109905495274763,0.4823241162058103,0.4833741687084354,0.48354917745887294,0.483899194959748,0.48459922996149807,0.4912495624781239,0.4914245712285614,0.4914245712285614,0.491949597479874,0.4929996499824991,0.4992999649982499,0.49947497374868743,0.5026251312565628,0.5028001400070004,0.5028001400070004,0.5105005250262513,0.5106755337766888,0.5140007000350018,0.5145257262863143,0.5147007350367518,0.5150507525376269,0.5152257612880644,0.5164508225411271,0.5169758487924396,0.5171508575428772,0.5178508925446272,0.5180259012950648,0.5190759537976899,0.520126006300315,0.5217010850542527,0.5218760938046902,0.5220511025551278,0.5222261113055653,0.524151207560378,0.5243262163108156,0.5246762338116906,0.5285264263213161,0.5290514525726286,0.5297514875743787,0.5364018200910046,0.5369268463423171,0.5427021351067554,0.5441022051102555,0.544277213860693,0.5448022401120056,0.5484774238711936,0.5493524676233812,0.5505775288764438,0.5509275463773189,0.5512775638781939,0.5514525726286315,0.5542527126356318,0.5546027301365068,0.5549527476373819,0.5584529226461323,0.5612530626531327,0.5616030801540077,0.5631781589079454,0.563353167658383,0.5656282814140707,0.572103605180259,0.5722786139306966,0.5726286314315716,0.5726286314315716,0.5740287014350718,0.575778788939447,0.5792789639481974,0.5799789989499475,0.5806790339516976,0.5908295414770739,0.5910045502275114,0.5924046202310116,0.6022051102555128,0.6078053902695135,0.611305565278264,0.6132306615330767,0.6135806790339517,0.6137556877843893,0.6144557227861394,0.6149807490374519,0.6151557577878894,0.6167308365418271,0.6169058452922647,0.6174308715435772,0.6177808890444523,0.6177808890444523,0.6184809240462024,0.6184809240462024,0.6186559327966399,0.6200560028001401,0.6205810290514526,0.6207560378018901,0.6226811340567029,0.6228561428071404,0.6268813440672033,0.6279313965698284,0.6298564928246412,0.6305565278263913,0.6316065803290164,0.632131606580329,0.632831641582079,0.6331816590829541,0.6333566678333916,0.6342317115855792,0.6347567378368918,0.6375568778438921,0.6407070353517675,0.6415820791039551,0.6464823241162058,0.6473573678683934,0.6477073853692684,0.648582429121456,0.6489324466223311,0.6510325516275813,0.6520826041302065,0.652257612880644,0.655932796639832,0.6576828841442072,0.6578578928946447,0.6578578928946447,0.6592579628981449,0.6594329716485824,0.6597829891494574,0.66030801540077,0.6604830241512075,0.6610080504025201,0.6611830591529576,0.6624081204060203,0.664333216660833,0.6650332516625831,0.6660833041652082,0.6671333566678334,0.6678333916695834,0.6690584529226461,0.6692334616730836,0.6699334966748337,0.6704585229261463,0.6737836891844592,0.6741337066853342,0.6751837591879594,0.6755337766888344,0.6771088554427721,0.6774588729436472,0.6778088904445222,0.6792089604480224,0.6802590129506475,0.6807840392019601,0.6814840742037102,0.6830591529576479,0.684459222961148,0.684459222961148,0.6846342317115856,0.6907595379768988,0.6911095554777739,0.6912845642282114,0.691809590479524,0.696534826741337,0.703885194259713,0.7052852642632131,0.7054602730136507,0.7058102905145257,0.7094854742737137,0.7101855092754638,0.7175358767938397,0.7177108855442772,0.7177108855442772,0.7238361918095905,0.7271613580679034,0.7275113755687784,0.727686384319216,0.7310115505775289,0.7313615680784039,0.732411620581029,0.7331116555827791,0.7332866643332167,0.7339866993349667,0.7385369268463423,0.7388869443472174,0.7395869793489674,0.7399369968498425,0.7409870493524676,0.7411620581029051,0.7416870843542177,0.7418620931046552,0.7423871193559678,0.7427371368568428,0.7429121456072804,0.743787189359468,0.7451872593629681,0.7474623731186559,0.7476373818690935,0.7479873993699685,0.763913195659783,0.7653132656632832,0.7663633181659083,0.7665383269163458,0.7826391319565978,0.7828141407070354,0.7829891494574729,0.7845642282114106,0.7864893244662233,0.7868393419670984,0.7875393769688485,0.787714385719286,0.788064403220161,0.7910395519775989,0.7912145607280364,0.7913895694784739,0.7913895694784739,0.7915645782289115,0.7933146657332867,0.7936646832341617,0.7955897794889745,0.7966398319915996,0.7969898494924746,0.7973398669933497,0.7976898844942247,0.7999649982499125,0.7999649982499125,0.80014000700035,0.8004900245012251,0.8010150507525376,0.8111655582779139,0.8123906195309766,0.8125656282814141,0.8141407070353518,0.8143157157857893,0.8148407420371019,0.815890794539727,0.8162408120406021,0.8165908295414771,0.8171158557927897,0.8172908645432272,0.8172908645432272,0.8179908995449773,0.8183409170458523,0.8188659432971649,0.8213160658032902,0.8218410920546028,0.8230661533076654,0.823591179558978,0.8260413020651033,0.8265663283164159,0.8272663633181659,0.827616380819041,0.827966398319916,0.8297164858242912,0.8426671333566679,0.843367168358418,0.848092404620231,0.8486174308715436,0.8507175358767939,0.851767588379419,0.8535176758837942,0.8540427021351068,0.8547427371368569,0.8550927546377319,0.8550927546377319,0.855442772138607,0.8556177808890445,0.8575428771438572,0.8580679033951698,0.8587679383969199,0.8620931046552328,0.8633181659082955,0.863493174658733,0.8640182009100456,0.8652432621631082,0.8654182709135457,0.8699684984249213,0.8708435421771089,0.8713685684284215,0.8722436121806091,0.8769688484424221,0.8785439271963598,0.8809940497024851,0.8811690584529226,0.8855442772138606,0.8864193209660483,0.8865943297164858,0.8867693384669233,0.8867693384669233,0.8876443822191109,0.887994399719986,0.8892194609730486,0.8893944697234861,0.8897444872243612,0.8897444872243612,0.892719635981799,0.8928946447322366,0.8930696534826741,0.8939446972348617,0.8970948547427371,0.8972698634931746,0.8976198809940497,0.8984949247462373,0.8984949247462373,0.8997199859992999,0.8997199859992999,0.9049702485124256,0.9051452572628631,0.9067203360168008,0.9072453622681134,0.9079453972698635,0.908120406020301,0.9126706335316765,0.9130206510325516,0.9137206860343017,0.9159957997899895,0.916170808540427,0.9163458172908645,0.9163458172908645,0.916520826041302,0.9172208610430521,0.9175708785439272,0.9182709135456772,0.9196709835491774,0.9200210010500525,0.9200210010500525,0.9221211060553027,0.9224711235561778,0.9254462723136156,0.9257962898144907,0.9257962898144907,0.927896394819741,0.9280714035701785,0.9287714385719286,0.9289464473223661,0.9294714735736787,0.9298214910745537,0.9301715085754287,0.9305215260763038,0.9317465873293664,0.931921596079804,0.932271613580679,0.932621631081554,0.9333216660833041,0.9348967448372418,0.9350717535876794,0.9354217710885544,0.9368218410920546,0.9369968498424921,0.9378718935946797,0.9380469023451172,0.9382219110955548,0.9382219110955548,0.9396219810990549,0.9401470073503675,0.9413720686034301,0.9417220861043052,0.9422471123556178,0.9425971298564928,0.9427721386069303,0.9427721386069303,0.9460973048652432,0.9469723486174308,0.9471473573678684,0.9476723836191809,0.948372418620931,0.948372418620931,0.9485474273713685,0.9490724536226811,0.9495974798739937,0.9497724886244312,0.9499474973748687,0.9501225061253062,0.9502975148757438,0.9504725236261813,0.9506475323766188,0.9509975498774939,0.9511725586279314,0.9515225761288064,0.9525726286314316,0.9539726986349317,0.9551977598879944,0.9571228561428071,0.9571228561428071,0.9572978648932446,0.9576478823941197,0.9639481974098705,0.964473223661183,0.9658732936646832,0.9662233111655583,0.9667483374168708,0.9669233461673084,0.9672733636681834,0.9702485124256213,0.9704235211760588,0.9753237661883094,0.9754987749387469,0.975848792439622,0.9765488274413721,0.9767238361918096,0.9772488624431221,0.9779488974448722,0.9830241512075604,0.9840742037101855,0.984249212460623,0.9847742387119356,0.9854742737136857,0.9854742737136857,0.9859992999649982,0.9861743087154358,0.9863493174658733,0.9868743437171859,0.9872243612180609,0.9880994049702485,0.9884494224711236,0.9884494224711236,0.9889744487224361,0.9901995099754988,0.9901995099754988,0.9905495274763738,0.9907245362268113,0.9907245362268113,0.9910745537276864,0.9912495624781239,0.991599579978999,0.9917745887294365,0.9917745887294365,0.9917745887294365,0.992299614980749,0.9924746237311866,0.9926496324816241,0.9929996499824991,0.9929996499824991,0.9931746587329366,0.9936996849842492,0.9936996849842492,0.9938746937346867,0.9938746937346867,0.9942247112355618,0.9945747287364368,0.9945747287364368,0.9947497374868743,0.9947497374868743,0.9950997549877494,0.9954497724886244,0.9954497724886244,0.995974798739937,0.9964998249912496,0.9966748337416871,0.9966748337416871,0.9973748687434372,0.9973748687434372,0.9977248862443122,0.9980749037451873,0.9980749037451873,0.9980749037451873,0.9982499124956248,0.9987749387469373,0.9989499474973749,0.9991249562478124,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9994749737486874,0.999649982499125,0.999649982499125,0.9998249912495625,0.9998249912495625,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"ROC Curve\"},\"xaxis\":{\"title\":{\"text\":\"False Positive Rate\"}},\"yaxis\":{\"title\":{\"text\":\"True Positive Rate\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5c0521af-d303-46df-9f09-766f3f72dc65');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.graph_objs as go\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(x_train, y_train)\n",
        "\n",
        "# Calculate ROC curve and AUC for Random Forest\n",
        "rf_probs = rf_model.predict_proba(x_test)[:, 1]\n",
        "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
        "rf_auc = auc(rf_fpr, rf_tpr)\n",
        "\n",
        "# Calculate ROC curve and AUC for XGBoost\n",
        "xgb_probs = xgb_model.predict_proba(x_test)[:, 1]\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "xgb_auc = auc(xgb_fpr, xgb_tpr)\n",
        "\n",
        "# Plot ROC curve using Plotly\n",
        "rf_trace = go.Scatter(x=rf_fpr, y=rf_tpr, mode='lines', name=f'Random Forest (AUC = {rf_auc:.2f})')\n",
        "xgb_trace = go.Scatter(x=xgb_fpr, y=xgb_tpr, mode='lines', name=f'XGBoost (AUC = {xgb_auc:.2f})')\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='ROC Curve',\n",
        "    xaxis=dict(title='False Positive Rate'),\n",
        "    yaxis=dict(title='True Positive Rate')\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[rf_trace, xgb_trace], layout=layout)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdXBoaQEDQxJ",
        "outputId": "1d316d12-953b-4769-a956-fe5a2b3904bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"da9791bc-c4fb-4edf-85a9-805968d6cf63\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"da9791bc-c4fb-4edf-85a9-805968d6cf63\")) {                    Plotly.newPlot(                        \"da9791bc-c4fb-4edf-85a9-805968d6cf63\",                        [{\"mode\":\"lines\",\"name\":\"XGBoost (AUC = 0.98)\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.00034302375439499187,0.00034302375439499187,0.00034302375439499187,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0005145356315924878,0.0005145356315924878,0.0005145356315924878,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0008575593859874796,0.0008575593859874796,0.0008575593859874796,0.0010290712631849757,0.0010290712631849757,0.0010290712631849757,0.0011148272017837235,0.0012005831403824716,0.0012863390789812194,0.0012863390789812194,0.0012863390789812194,0.0012863390789812194,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0014578509561787153,0.0014578509561787153,0.0014578509561787153,0.0014578509561787153,0.0015436068947774634,0.0015436068947774634,0.0015436068947774634,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0017151187719749593,0.0017151187719749593,0.0017151187719749593,0.0017151187719749593,0.0018008747105737071,0.0018866306491724552,0.0018866306491724552,0.0018866306491724552,0.0019723865877712033,0.002143898464968699,0.002229654403567447,0.002229654403567447,0.002229654403567447,0.002229654403567447,0.002315410342166195,0.002658434096561187,0.002829945973758683,0.002829945973758683,0.002829945973758683,0.0029157019123574306,0.003172969728153675,0.0032587256667524224,0.0032587256667524224,0.0032587256667524224,0.0033444816053511705,0.0033444816053511705,0.0033444816053511705,0.0034302375439499186,0.0034302375439499186,0.0035159934825486666,0.0036017494211474143,0.0036875053597461623,0.0036875053597461623,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0038590172369436584,0.0038590172369436584,0.004030529114141155,0.004116285052739903,0.004116285052739903,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.00463082068433239,0.00463082068433239,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.00505960037732613,0.00505960037732613,0.00505960037732613,0.005231112254523626,0.005316868193122374,0.005316868193122374,0.005316868193122374,0.005316868193122374,0.005402624131721122,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005745647886116113,0.005831403824714861,0.005831403824714861,0.005917159763313609,0.005917159763313609,0.005917159763313609,0.005917159763313609,0.0061744275791098535,0.0061744275791098535,0.0061744275791098535,0.006260183517708602,0.006260183517708602,0.00634593945630735,0.00634593945630735,0.006431695394906097,0.006431695394906097,0.006431695394906097,0.006517451333504845,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006774719149301089,0.006774719149301089,0.006774719149301089,0.0072034988422948285,0.007289254780893577,0.007460766658091073,0.007546522596689821,0.00797530228968356,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008747105737072292,0.008747105737072292,0.00883286167567104,0.00883286167567104,0.00883286167567104,0.008918617614269788,0.008918617614269788,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009175885430066031,0.00926164136866478,0.009347397307263527,0.009433153245862276,0.009433153245862276,0.009433153245862276,0.009604665123059773,0.009604665123059773,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010290712631849755,0.010376468570448504,0.010376468570448504,0.010547980447646,0.010547980447646,0.010547980447646,0.010633736386244748,0.010633736386244748,0.010719492324843495,0.010719492324843495,0.010719492324843495,0.011234027956435983,0.011234027956435983,0.011319783895034732,0.011319783895034732,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.011577051710830975,0.011577051710830975,0.011662807649429723,0.011662807649429723,0.011748563588028471,0.011834319526627219,0.011834319526627219,0.012005831403824715,0.012177343281022211,0.012348855158219707,0.012348855158219707,0.012348855158219707,0.0126918789126147,0.012777634851213446,0.012777634851213446,0.012863390789812194,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.01303490266700969,0.013120658605608439,0.013120658605608439,0.013292170482805935,0.013292170482805935,0.013292170482805935,0.013635194237200925,0.013806706114398421,0.014235485807392163,0.014235485807392163,0.01432124174599091,0.01432124174599091,0.014406997684589657,0.014835777377583398,0.014835777377583398,0.015178801131978389,0.015178801131978389,0.015178801131978389,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015521824886373381,0.015521824886373381,0.01560758082497213,0.015779092702169626,0.015779092702169626,0.016036360517965868,0.016036360517965868,0.016207872395163365,0.016207872395163365,0.01637938427236086,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016722408026755852,0.0168081639653546,0.0168081639653546,0.01689391990395335,0.01689391990395335,0.017065431781150844,0.017065431781150844,0.017236943658348342,0.017236943658348342,0.01732269959694709,0.01732269959694709,0.017494211474144584,0.017494211474144584,0.017494211474144584,0.01775147928994083,0.01775147928994083,0.01775147928994083,0.017922991167138323,0.017922991167138323,0.018437526798730813,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.018609038675928308,0.018609038675928308,0.018609038675928308,0.018694794614527055,0.018780550553125806,0.018780550553125806,0.018780550553125806,0.019123574307520794,0.01938084212331704,0.01938084212331704,0.019466598061915787,0.019466598061915787,0.019552354000514537,0.019981133693508277,0.019981133693508277,0.019981133693508277,0.020066889632107024,0.020066889632107024,0.02023840150930452,0.020495669325100763,0.02058142526369951,0.02058142526369951,0.020838693079495756,0.020838693079495756,0.02101020495669325,0.021610496526884487,0.021696252465483234,0.021696252465483234,0.02178200840408198,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.022039276219878227,0.022125032158476974,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022468055912871966,0.022468055912871966,0.022896835605865706,0.022982591544464453,0.023239859360260698,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023668639053254437,0.023668639053254437,0.024097418746248177,0.024097418746248177,0.024097418746248177,0.024526198439241916,0.024526198439241916,0.024526198439241916,0.024611954377840667,0.024611954377840667,0.02478346625503816,0.02478346625503816,0.02478346625503816,0.02486922219363691,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.0253837578252294,0.0253837578252294,0.025812537518223138,0.02606980533401938,0.02606980533401938,0.026412829088414372,0.026412829088414372,0.026412829088414372,0.02658434096561187,0.02658434096561187,0.02658434096561187,0.02658434096561187,0.026670096904210617,0.026670096904210617,0.026927364720006862,0.02701312065860561,0.027184632535803104,0.027184632535803104,0.02727038847440185,0.02727038847440185,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.02744190035159935,0.02744190035159935,0.027956435983191835,0.027956435983191835,0.027956435983191835,0.028127947860389333,0.028556727553383073,0.029157019123574306,0.029157019123574306,0.029242775062173057,0.029242775062173057,0.02984306663236429,0.02984306663236429,0.03070062601835177,0.03070062601835177,0.03078638195695052,0.031472429465740505,0.03155818540433925,0.03155818540433925,0.03155818540433925,0.031643941342938,0.031729697281536746,0.031815453220135494,0.03190120915873424,0.03190120915873424,0.03190120915873424,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03250150072892548,0.03250150072892548,0.03250150072892548,0.03275876854472172,0.03284452448332047,0.03284452448332047,0.03284452448332047,0.03455964325529543,0.03455964325529543,0.034731155132492926,0.034731155132492926,0.03730383329045536,0.03738958922905411,0.03738958922905411,0.03781836892204785,0.038075636737844094,0.038075636737844094,0.03833290455364034,0.03833290455364034,0.03841866049223909,0.038847440185232826,0.038847440185232826,0.03893319612383157,0.03901895206243032,0.03901895206243032,0.03927621987822657,0.039361975816825316,0.03961924363262156,0.039704999571220305,0.039704999571220305,0.039704999571220305,0.039704999571220305,0.03996226738701655,0.0400480233256153,0.040219535202812795,0.040219535202812795,0.040219535202812795,0.040562558957207784,0.040562558957207784,0.040562558957207784,0.04064831489580654,0.04064831489580654,0.04064831489580654,0.04064831489580654,0.04081982677300403,0.04081982677300403,0.04090558271160278,0.04090558271160278,0.04099133865020153,0.04116285052739902,0.04116285052739902,0.04116285052739902,0.04150587428179402,0.04150587428179402,0.04167738615899151,0.04167738615899151,0.04210616585198525,0.04210616585198525,0.04210616585198525,0.042191921790584,0.042363433667781496,0.04244918960638024,0.04356401680816396,0.04382128462396021,0.04425006431695395,0.04425006431695395,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.044507332132750196,0.044850355887145185,0.044850355887145185,0.044850355887145185,0.044850355887145185,0.04493611182574393,0.045193379641540174,0.04527913558013893,0.04527913558013893,0.045364891518737675,0.045364891518737675,0.045364891518737675,0.045965183088928906,0.04605093902752765,0.04613669496612641,0.04613669496612641,0.04656547465912014,0.046822742474916385,0.04708001029071263,0.04708001029071263,0.04750878998370637,0.04750878998370637,0.047594545922305116,0.047594545922305116,0.04776605779950262,0.047851813738101365,0.047851813738101365,0.0482805934310951,0.04836634936969385,0.04836634936969385,0.0484521053082926,0.04913815281708258,0.049223908755681334,0.049223908755681334,0.049223908755681334,0.049481176571477575,0.049481176571477575,0.049481176571477575,0.04990995626447131,0.049995712203070065,0.05016722408026756,0.05025298001886631,0.051110539404853786,0.051110539404853786,0.05128205128205128,0.05136780722065003,0.05136780722065003,0.05136780722065003,0.05205385472944001,0.05205385472944001,0.05213961066803876,0.052482634422433755,0.052482634422433755,0.0525683903610325,0.05265414629963125,0.05265414629963125,0.052825658176828744,0.052825658176828744,0.05299717005402624,0.053340193808421234,0.053340193808421234,0.05342594974701998,0.053854729440013724,0.05394048537861247,0.05419775319440871,0.05419775319440871,0.05428350913300746,0.054626532887402456,0.054626532887402456,0.0548838007031987,0.0548838007031987,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05514106851899494,0.05539833633479119,0.05539833633479119,0.05539833633479119,0.05565560415058743,0.055741360089186176,0.05582711602778492,0.05582711602778492,0.05591287196638367,0.05599862790498242,0.056084383843581165,0.056084383843581165,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05642740759797616,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.0566846754137724,0.0566846754137724,0.05677043135237115,0.05677043135237115,0.05677043135237115,0.056856187290969896,0.057113455106766145,0.057113455106766145,0.05728496698396364,0.05737072292256239,0.05737072292256239,0.05737072292256239,0.05737072292256239,0.05779950261555613,0.05814252636995112,0.058485550124346114,0.05942886544893234,0.059600377326129834,0.05968613326472858,0.05977188920332733,0.06157276391390104,0.06157276391390104,0.06183003172969728,0.06183003172969728,0.062087299545493524,0.06225881142269102,0.06225881142269102,0.06371666237886973,0.06371666237886973,0.06431695394906098,0.06431695394906098,0.06440270988765973,0.06474573364205471,0.06474573364205471,0.06483148958065346,0.06491724551925221,0.06603207272103594,0.06646085241402967,0.06646085241402967,0.06654660835262842,0.06654660835262842,0.06663236429122717,0.06663236429122717,0.06663236429122717,0.06671812022982591,0.06697538804562216,0.06714689992281965,0.0672326558614184,0.0672326558614184,0.06731841180001714,0.0674041677386159,0.0674041677386159,0.0675756796158134,0.0675756796158134,0.0675756796158134,0.0677471914930109,0.0677471914930109,0.0677471914930109,0.06809021524740588,0.06809021524740588,0.06826172712460338,0.06834748306320212,0.06834748306320212,0.06843323900180087,0.06843323900180087,0.06843323900180087,0.06851899494039962,0.06851899494039962,0.06869050681759711,0.06886201869479461,0.06894777463339337,0.06903353057199212,0.06903353057199212,0.06911928651059086,0.06920504244918961,0.06920504244918961,0.06929079838778836,0.06929079838778836,0.0697195780807821,0.06980533401938084,0.06997684589657834,0.07040562558957207,0.07057713746676958,0.07066289340536833,0.0713489409141583,0.0715204527913558,0.0715204527913558,0.07263527999313953,0.07280679187033702,0.07314981562473201,0.073492839379127,0.073835863133522,0.073835863133522,0.07392161907212075,0.07392161907212075,0.07409313094931824,0.07435039876511448,0.07435039876511448,0.07443615470371323,0.07452191064231198,0.07452191064231198,0.07460766658091073,0.07460766658091073,0.07486493439670697,0.07546522596689821,0.07555098190549696,0.0759797615984907,0.07606551753708944,0.07623702941428694,0.07649429723008318,0.07666580910728069,0.07692307692307693,0.07880970757224938,0.07923848726524312,0.07932424320384186,0.07966726695823685,0.07975302289683561,0.0800960466512306,0.0802675585284281,0.08069633822142183,0.08078209416002058,0.08232570105479804,0.08249721293199554,0.08344052825658177,0.08412657576537175,0.08472686733556299,0.08489837921276049,0.08498413515135923,0.08515564702855673,0.08592745047594547,0.08644198610753795,0.08687076580053169,0.08704227767772918,0.08738530143212417,0.08747105737072293,0.08807134894091416,0.08841437269530915,0.08901466426550039,0.08910042020409914,0.08927193208129663,0.08952919989709288,0.08970071177429037,0.08987222365148786,0.0901294914672841,0.09107280679187034,0.09124431866906783,0.09150158648486408,0.09201612211645656,0.0921018780550553,0.09227338993225281,0.09235914587085156,0.0926164136866478,0.09295943744104279,0.09313094931824029,0.09321670525683903,0.09338821713403653,0.09347397307263527,0.09364548494983277,0.09390275276562902,0.09407426464282652,0.09433153245862276,0.09450304433582025,0.09467455621301775,0.09836206157276392,0.09853357344996141,0.09870508532715891,0.09879084126575766,0.09913386502015265,0.09947688877454763,0.09999142440614013,0.10024869222193637,0.10050596003773261,0.1008489837921276,0.1010204956693251,0.10119200754652259,0.10136351942372009,0.10187805505531258,0.10213532287110882,0.10230683474830632,0.10256410256410256,0.10307863819569506,0.10325015007289255,0.1033359060114913,0.10367892976588629,0.10393619758168253,0.10402195352028128,0.10419346539747877,0.10436497727467627,0.10453648915187377,0.10539404853786125,0.10547980447645999,0.10590858416945373,0.10642311980104623,0.1072806791870337,0.10736643512563245,0.10770945888002745,0.1077952148186262,0.10796672669582369,0.10805248263442244,0.10856701826601492,0.10882428608181116,0.10933882171340366,0.1094245776520024,0.10985335734499614,0.11302632707314982,0.11328359488894606,0.1133693508275448,0.1138838864591373,0.11396964239773605,0.11414115427493354,0.11422691021353229,0.11439842209072978,0.11456993396792728,0.11482720178372352,0.11499871366092101,0.11517022553811851,0.11577051710830975,0.11654232055569848,0.11662807649429723,0.1176571477574822,0.1178286596346797,0.1180001715118772,0.11817168338907469,0.11851470714346969,0.11860046308206844,0.11877197495926593,0.12005831403824715,0.12022982591544465,0.12057284966983964,0.12100162936283337,0.12108738530143212,0.12134465311722836,0.12143040905582711,0.12177343281022211,0.1219449446874196,0.1221164565646171,0.12245948031901209,0.12357430752079582,0.12383157533659206,0.12400308721378955,0.12443186690678329,0.12451762284538204,0.12477489066117829,0.12494640253837579,0.12511791441557327,0.12520367035417201,0.1253751822313695,0.12563245004716578,0.12700454506474573,0.12709030100334448,0.12726181288054197,0.12743332475773947,0.1278621044507332,0.12846239602092444,0.12880541977531945,0.1288911757139182,0.12931995540691194,0.1300060029157019,0.1301775147928994,0.13189263356487438,0.13206414544207187,0.1326644370122631,0.1330074607666581,0.13360775233684932,0.13369350827544807,0.13386502015264556,0.1341222879684418,0.13437955578423805,0.13472257953863306,0.13635194237200926,0.13832432895978047,0.13858159677557672,0.1394391561615642,0.13961066803876168,0.13995369179315667,0.14012520367035416,0.14046822742474915,0.14063973930194665,0.14098276305634166,0.14115427493353916,0.1412400308721379,0.1414115427493354,0.14149729868793415,0.14218334619672413,0.14226910213532287,0.14261212588971786,0.14295514964411285,0.14389846496869907,0.14424148872309409,0.14467026841608782,0.14484178029328532,0.14492753623188406,0.1456993396792728,0.14578509561787154,0.14655689906526026,0.14672841094245775,0.14741445845124776,0.14835777377583398,0.14844352971443273,0.14904382128462396,0.1491295772232227,0.14955835691621644,0.14990138067061143,0.15007289254780892,0.15024440442500644,0.15153074350398765,0.15204527913558014,0.15238830288997512,0.15316010633736385,0.15333161821456137,0.15401766572335135,0.15418917760054884,0.15444644541634509,0.15496098104793757,0.15513249292513506,0.15564702855672755,0.15581854043392504,0.1567618557585113,0.15719063545150502,0.15744790326730126,0.15779092702169625,0.15830546265328874,0.15839121859188748,0.15873424234628247,0.1591630220392762,0.1593345339164737,0.16010633736386246,0.1601920933024612,0.1603636051796587,0.16079238487265243,0.16130692050424492,0.16139267644284366,0.16156418832004116,0.1616499442586399,0.1619929680130349,0.1621644798902324,0.16242174770602863,0.16276477146042365,0.16293628333762114,0.16345081896921362,0.16362233084641112,0.16370808678500987,0.16387959866220736,0.16456564617099734,0.16482291398679358,0.16499442586399107,0.16610925306577481,0.16619500900437356,0.16670954463596604,0.16713832432895978,0.16722408026755853,0.1682531515307435,0.16833890746934224,0.16868193122373723,0.16902495497813225,0.16936797873252724,0.16953949060972473,0.16962524654832348,0.16979675842552097,0.16988251436411972,0.1703970499957122,0.17065431781150844,0.17091158562730469,0.17099734156590343,0.17134036532029842,0.17142612125889717,0.17176914501329216,0.17236943658348342,0.17245519252208216,0.17296972815367465,0.17331275190806963,0.17382728753966212,0.17391304347826086,0.1746848469256496,0.17485635880284708,0.17511362661864335,0.1751993825572421,0.1755424063116371,0.17579967412743333,0.17597118600463083,0.17605694194322957,0.17622845382042707,0.17657147757482206,0.17674298945201955,0.1770002572678158,0.17811508446959953,0.17837235228539577,0.17854386416259327,0.17862962010119202,0.1788011319783895,0.17888688791698826,0.17905839979418575,0.1791441557327845,0.1794871794871795,0.17974444730297573,0.18008747105737072,0.18017322699596947,0.18051625075036445,0.1809450304433582,0.1812880541977532,0.1814595660749507,0.1873767258382643,0.1875482377154618,0.18823428522425178,0.18857730897864677,0.18909184461023926,0.18926335648743675,0.19046393962781924,0.19192179058399794,0.1920075465225967,0.19217905839979418,0.19355115341737417,0.1938084212331704,0.1939799331103679,0.19500900437355287,0.1952662721893491,0.1954377840665466,0.1959523196981391,0.19620958751393533,0.20126918789126147,0.20144069976845896,0.20229825915444644,0.2181631077952148,0.21833461967241233,0.21850613154960982,0.21867764342680732,0.21893491124260356,0.21927793499699855,0.2193636909355973,0.2216791012777635,0.221850613154961,0.22202212503215848,0.22862533230426207,0.23085498670782953,0.23102649858502702,0.23728668210273562,0.23737243804133437,0.23805848555012435,0.25692479204184887,0.2584683989366264,0.2613841008489838,0.26164136866478005,0.26198439241917504,0.2624989280507675,0.26361375525255126,0.26369951119115,0.26429980276134124,0.26447131463853873,0.26524311808592743,0.2653288740245262,0.2660149215333162,0.26644370122630995,0.26678672498070494,0.26704399279650115,0.2674727724894949,0.26755852842809363,0.2703884744018523,0.27527656290198094,0.275619586656376,0.2775062173055484,0.2778492410599434,0.2781922648143384,0.2782780207529371,0.2787068004459309,0.27896406826172715,0.28025040734070833,0.2811079667266958,0.28453820427064574,0.28462396020924446,0.2881399536917932,0.2883972215075894,0.2887402452619844,0.29431438127090304,0.29440013720950176,0.30168939199039535,0.30177514792899407,0.3025469513763828,0.30434782608695654,0.3050338735957465,0.3054626532887402,0.3205556984821199,0.32201354943829863,0.3221850613154961,0.3228711088242861,0.325272275105051,0.3282737329560072,0.3352199639825058,0.3509990566846754,0.3510848126232742,0.35202812794786037,0.35245690764085413,0.3609467455621302,0.3613755252551239,0.36146128119372267,0.3634336677814939,0.36643512563245006,0.36677814938684505,0.39473458537003686,0.3956779006946231,0.3959351685104193,0.3961066803876168,0.39644970414201186,0.3965354600806106,0.39687848383500557,0.39713575165080184,0.41505874281794014,0.41523025469513763,0.4157447903267301,0.42080439070405623,0.42449189606380244,0.4251779435725924,0.4271503301603636,0.42757910985335734,0.42792213360775233,0.4410427922133608,0.4411285481519595,0.4415573278449533,0.4509904810908155,0.4585370036875054,0.45922305119629536,0.4593088071348941,0.461023925906869,0.463768115942029,0.4768030186090387,0.47697453048623617,0.48923762970585716,0.49198181974101707,0.4966983963639482,0.4968699082411457,0.4986707829517194,0.4987565388903182,0.5034731155132492,0.5035588714518481,0.5245690764085413,0.5281708258296887,0.5283423377068862,0.5289426292770775,0.5304004802332561,0.5304862361718549,0.5307435039876511,0.5343452534087986,0.5351170568561873,0.5487522510933882,0.5488380070319869,0.5616156418832005,0.5660749506903353,0.5681330932167052,0.5691621644798902,0.570276991681674,0.5744790326730126,0.5745647886116113,0.5748220564274076,0.5775662464625675,0.598833719235057,0.5989194751736557,0.6128976931652517,0.6130692050424492,0.6249892805076751,0.6407683732098448,0.6640082325701054,0.6673527141754566,0.6680387616842466,0.6681245176228454,0.6684675413772404,0.6685532973158391,0.6687248091930366,0.6690678329474317,0.6759283080353314,0.6875053597461624,0.715204527913558,0.7153760397907555,0.715547551667953,0.716233599176743,0.7246376811594203,0.7248091930366178,0.7249807049138153,0.7251522167910128,0.7286682102735614,0.7287539662121602,0.7300403052911414,0.7338135665894864,0.7354429294228625,0.73561444130006,0.7357859531772575,0.7385301432124175,0.7405882857387874,0.7423891604493611,0.744447302975731,0.7460766658091073,0.7464196895635022,0.7465912014406998,0.7469342251950948,0.747191493010891,0.749592659291656,0.7500214389846497,0.7501071949232484,0.7507932424320384,0.7513077780636309,0.7513935340022296,0.7519080696338222,0.7531086527742046,0.7534516765285996,0.7536231884057971,0.7540519680987908,0.7719749592659292,0.7855243975645313,0.7856959094417288,0.7860389331961238,0.7866392247663151,0.7876682960295001,0.7877540519680988,0.7910127776348512,0.79109853357345,0.7948717948717948,0.7953005745647886,0.7953863305033874,0.7959008661349799,0.7959866220735786,0.7964154017665723,0.7965869136437699,0.796844181459566,0.7970156933367636,0.7971014492753623,0.7976159849069548,0.7984735442929423,0.7987308121087385,0.7990738358631335,0.7992453477403311,0.7995883714947261,0.800274419003516,0.8007889546351085,0.8013892462052997,0.8017322699596947,0.801989537775491,0.8021610496526884,0.8027613412228797,0.8035331446702684,0.8056770431352371,0.8061915787668296,0.8075636737844095,0.8088500128633908,0.8091930366177857,0.8092787925563846,0.809450304433582,0.8100505960037733,0.810136351942372,0.810822399451162,0.8109939113283595,0.8127947860389332,0.8128805419775319,0.8135665894863219,0.8138238573021181,0.8150244404425007,0.8151959523196981,0.8154532201354944,0.8155389760740931,0.8158819998284881,0.8160535117056856,0.8176828745390619,0.8178543864162593,0.8182831661092531,0.8183689220478518,0.8187977017408455,0.8188834576794443,0.8190549695566418,0.8200840408198268,0.8202555526970242,0.8209416002058143,0.821713403653203,0.8235142783637767,0.8247148615041592,0.8249721293199554,0.8251436411971529,0.8252293971357516,0.8257439327673441,0.8259154446445416,0.8272017837235228,0.8289169024954978,0.8290026584340966,0.8297744618814853,0.829860217820084,0.831232312837664,0.8314038247148615,0.831575336592059,0.8317468484692565,0.8320041162850528,0.833633479118429,0.8338907469342252,0.833976502872824,0.8377497641711689,0.8384358116799588,0.8385215676185576,0.8388645913729526,0.8401509304519338,0.8404081982677301,0.840751222022125,0.841437269530915,0.8416087814081125,0.84178029328531,0.8428951204870937,0.8430666323642912,0.8438384358116799,0.8439241917502787,0.8456393105222537,0.8458108223994512,0.8462396020924449,0.8463253580310437,0.8464968699082411,0.8465826258468399,0.8468398936626361,0.8473544292942287,0.8475259411714261,0.8483835005574136,0.8484692564960123,0.8594460166366521,0.8601320641454421,0.8605608438384358,0.8641625932595832,0.8643341051367808,0.8652774204613669,0.8657062001543607,0.8667352714175457,0.8699082411456993,0.8713660921018781,0.8719663836720692,0.872738187119458,0.8728239430580568,0.8733384786896493,0.8761684246634079,0.8763399365406055,0.8767687162335992,0.8791698825143641,0.8799416859617528,0.881571048795129,0.8819998284881228,0.8845725066460852,0.886030357602264,0.8895463510848126,0.8898893748392076,0.8902323985936026,0.8903181545322013,0.8939199039533487,0.8951204870937313,0.8952919989709287,0.8970071177429036,0.8971786296201012,0.9003515993482548,0.9017236943658349,0.9018952062430323,0.9021524740588286,0.9025812537518223,0.9056684675413772,0.9082411456993397,0.9111568476116971,0.9112426035502958,0.914930108910042,0.9151016207872396,0.9178458108223995,0.9180173226995969,0.9182745905153932,0.9186176142697882,0.9193894177171769,0.9195609295943744,0.926850184375268,0.9347397307263527,0.9351685104193466,0.9616670954463596,0.9641540176657234,0.9644112854815196,0.9658691364376983,0.9662121601920933,0.9664694280078896,0.9668124517622846,0.966983963639482,0.9692136180430495,0.9692993739816482,0.9698139096132408,0.9699854214904382,0.9700711774290369,0.9703284452448332,0.9706714689992282,0.9707572249378269,0.9709287368150245,0.9781322356573192,0.978217991595918,0.9839636394820341,0.9843066632364291,0.9844781751136266,0.9845639310522254,0.9852499785610154,0.9856787582540091,0.987736900780379,0.9879941685961753,0.9885087042277678,0.9885944601663665,0.9893662636137552,0.9896235314295515,0.989795043306749,0.9898807992453478,0.9901380670611439,0.9903095789383415,0.9905668467541378,0.9909956264471315,0.9916816739559214,0.9917674298945202,0.992539233341909,0.992882257096304,0.9935683046050939,0.9940828402366864,0.9941685961752852,0.9943401080524826,0.9945973758682789,0.9947688877454763,0.9951119114998713,0.9951976674384702,0.9963124946402538,0.9963982505788526,0.9966555183946488,0.9973415659034388,0.9976845896578338,0.9983706371666238,0.99862790498242,0.998970928736815,0.9994854643684075,0.9995712203070063,1.0],\"y\":[0.0,0.00035001750087504374,0.0005250262513125656,0.001400070003500175,0.0017500875043752187,0.002450122506125306,0.002625131256562828,0.002975148757437872,0.003325166258312916,0.0036751837591879593,0.0038501925096254812,0.004550227511375569,0.004725236261813091,0.005075253762688134,0.0056002800140007,0.005950297514875744,0.006300315015750788,0.0077003850192509625,0.008050402520126006,0.008750437521876094,0.00927546377318866,0.009625481274063704,0.011025551277563878,0.011725586279313966,0.011900595029751488,0.012250612530626532,0.013125656282814141,0.013300665033251663,0.013650682534126707,0.014175708785439271,0.014350717535876793,0.015050752537626881,0.015400770038501925,0.015575778788939447,0.01662583129156458,0.017325866293314666,0.017325866293314666,0.018375918795939798,0.018725936296814842,0.018725936296814842,0.01942597129856493,0.01960098004900245,0.019950997549877492,0.020826041302065102,0.021351067553377668,0.021701085054252712,0.02327616380819041,0.023626181309065453,0.023801190059502975,0.02520126006300315,0.02520126006300315,0.025726286314315717,0.027826391319565977,0.02887644382219111,0.029576478823941196,0.030101505075253762,0.03080154007700385,0.031151557577878894,0.03237661883094155,0.03255162758137907,0.033076653832691635,0.03447672383619181,0.034826741337066855,0.03902695134756738,0.03972698634931746,0.04060203010150507,0.040777038851942594,0.04112705635281764,0.04130206510325516,0.04235211760588029,0.043402170108505424,0.044277213860693033,0.044452222611130555,0.048127406370318516,0.04917745887294365,0.05110255512775639,0.051977598879944,0.053377668883444175,0.05372768638431922,0.05390269513475674,0.05390269513475674,0.05407770388519426,0.05460273013650683,0.054777738886944344,0.05530276513825691,0.05652782639131956,0.056702835141757085,0.05722786139306965,0.05775288764438222,0.05827791389569478,0.05862793139656983,0.05880294014700735,0.0616030801540077,0.06177808890444522,0.06230311515575779,0.06370318515925796,0.06370318515925796,0.06492824641232062,0.06510325516275814,0.0656282814140707,0.06580329016450823,0.06650332516625831,0.06685334266713336,0.06737836891844592,0.06772838641932097,0.06807840392019601,0.06825341267063353,0.06860343017150858,0.0687784389219461,0.07000350017500875,0.07070353517675884,0.07087854392719636,0.07157857892894645,0.07262863143157158,0.07297864893244663,0.07350367518375919,0.07367868393419671,0.07402870143507176,0.07507875393769689,0.07525376268813441,0.07577878893944698,0.07962898144907245,0.08032901645082254,0.0808540427021351,0.0808540427021351,0.0808540427021351,0.08732936646832341,0.08767938396919846,0.08837941897094855,0.08872943647182359,0.0992299614980749,0.10850542527126357,0.11008050402520125,0.11025551277563878,0.11130556527826391,0.11148057402870143,0.11830591529576479,0.1191809590479524,0.12198109905495275,0.12285614280714036,0.12373118655932797,0.12443122156107805,0.12530626531326566,0.12863143157157858,0.1288064403220161,0.1288064403220161,0.13773188659432972,0.13790689534476724,0.1382569128456423,0.13983199159957999,0.14700735036751839,0.147882394119706,0.14805740287014352,0.1487574378718936,0.14928246412320617,0.15838291914595728,0.16275813790689533,0.16275813790689533,0.16835841792089604,0.16870843542177108,0.1688834441722086,0.1697584879243962,0.16993349667483373,0.17028351417570878,0.17098354917745887,0.17185859292964648,0.17220861043052152,0.17238361918095904,0.17273363668183409,0.17938396919845992,0.18043402170108505,0.1807840392019601,0.18130906545327266,0.18253412670633531,0.18288414420721036,0.18760938046902345,0.18813440672033602,0.18830941547077354,0.19723486174308716,0.19793489674483725,0.2031851592579629,0.20336016800840043,0.20406020301015051,0.20406020301015051,0.2047602380119006,0.20598529926496326,0.2063353167658383,0.20651032551627582,0.21141057052852644,0.21176058802940148,0.211935596779839,0.21701085054252714,0.21963598179908994,0.21981099054952746,0.2201610080504025,0.22068603430171507,0.22313615680784038,0.22996149807490374,0.23013650682534126,0.23083654182709135,0.2313615680784039,0.2376618830941547,0.23888694434721736,0.23906195309765488,0.24151207560378019,0.24221211060553027,0.2432621631081554,0.24361218060903045,0.24833741687084354,0.25568778438921946,0.255862793139657,0.25726286314315716,0.2574378718935947,0.2577878893944697,0.2590129506475324,0.2591879593979699,0.2607630381519076,0.2609380469023451,0.2637381869093455,0.2728386419320966,0.27458872943647183,0.2752887644382219,0.27633881694084705,0.2766888344417221,0.27703885194259714,0.27721386069303466,0.2775638781939097,0.2782639131956598,0.27948897444872245,0.283689184459223,0.28438921946097306,0.2845642282114106,0.29051452572628633,0.2933146657332867,0.29401470073503677,0.29558977948897447,0.3055652782639132,0.31326566328316413,0.31414070703535174,0.31571578578928944,0.3160658032901645,0.31659082954147705,0.3295414770738537,0.3297164858242912,0.3304165208260413,0.3307665383269163,0.33094154707735385,0.33339166958347916,0.3335666783339167,0.3375918795939797,0.3377668883444172,0.3379418970948547,0.33811690584529225,0.3384669233461673,0.33881694084704234,0.33899194959747986,0.34196709835491773,0.3424921246062303,0.34284214210710534,0.34354217710885543,0.34371718585929295,0.34441722086104304,0.34529226461323065,0.3456422821141057,0.3472173608680434,0.35404270213510675,0.35421771088554427,0.3545677283864193,0.3552677633881694,0.3552677633881694,0.3554427721386069,0.3584179208960448,0.35964298214910745,0.3615680784039202,0.3704935246762338,0.37066853342667133,0.3781939096954848,0.3783689184459223,0.3790689534476724,0.37941897094854743,0.37959397969898495,0.38029401470073504,0.3806440322016101,0.38116905845292265,0.3853692684634232,0.3857192859642982,0.3862443122156108,0.3862443122156108,0.3864193209660483,0.38851942597129857,0.3886944347217361,0.3893944697234862,0.3976198809940497,0.39796989849492476,0.3984949247462373,0.39866993349667484,0.3990199509975499,0.3991949597479874,0.3997199859993,0.400070003500175,0.40182009100455024,0.40287014350717537,0.40357017850892546,0.40444522226111307,0.4047952397619881,0.40514525726286316,0.4086454322716136,0.4096954847742387,0.4109205460273014,0.41144557227861395,0.41319565978298917,0.4135456772838642,0.41389569478473925,0.41879593979698987,0.4229961498074904,0.42824641232061605,0.42964648232411623,0.4299964998249913,0.4326216310815541,0.43367168358417924,0.4343717185859293,0.4366468323416171,0.43752187609380466,0.4376968848442422,0.4387469373468673,0.4396219810990549,0.440322016100805,0.44067203360168006,0.4427721386069303,0.44294714735736784,0.44452222611130554,0.44452222611130554,0.44469723486174306,0.44697234861743085,0.44767238361918094,0.44784739236961846,0.4515225761288064,0.45414770738536925,0.4609730486524326,0.46132306615330765,0.4616730836541827,0.46219810990549526,0.4670983549177459,0.47234861743087153,0.4728736436821841,0.48022401120056,0.48039901995099754,0.48109905495274763,0.4823241162058103,0.4833741687084354,0.48354917745887294,0.483899194959748,0.48459922996149807,0.4912495624781239,0.4914245712285614,0.4914245712285614,0.491949597479874,0.4929996499824991,0.4992999649982499,0.49947497374868743,0.5026251312565628,0.5028001400070004,0.5028001400070004,0.5105005250262513,0.5106755337766888,0.5140007000350018,0.5145257262863143,0.5147007350367518,0.5150507525376269,0.5152257612880644,0.5164508225411271,0.5169758487924396,0.5171508575428772,0.5178508925446272,0.5180259012950648,0.5190759537976899,0.520126006300315,0.5217010850542527,0.5218760938046902,0.5220511025551278,0.5222261113055653,0.524151207560378,0.5243262163108156,0.5246762338116906,0.5285264263213161,0.5290514525726286,0.5297514875743787,0.5364018200910046,0.5369268463423171,0.5427021351067554,0.5441022051102555,0.544277213860693,0.5448022401120056,0.5484774238711936,0.5493524676233812,0.5505775288764438,0.5509275463773189,0.5512775638781939,0.5514525726286315,0.5542527126356318,0.5546027301365068,0.5549527476373819,0.5584529226461323,0.5612530626531327,0.5616030801540077,0.5631781589079454,0.563353167658383,0.5656282814140707,0.572103605180259,0.5722786139306966,0.5726286314315716,0.5726286314315716,0.5740287014350718,0.575778788939447,0.5792789639481974,0.5799789989499475,0.5806790339516976,0.5908295414770739,0.5910045502275114,0.5924046202310116,0.6022051102555128,0.6078053902695135,0.611305565278264,0.6132306615330767,0.6135806790339517,0.6137556877843893,0.6144557227861394,0.6149807490374519,0.6151557577878894,0.6167308365418271,0.6169058452922647,0.6174308715435772,0.6177808890444523,0.6177808890444523,0.6184809240462024,0.6184809240462024,0.6186559327966399,0.6200560028001401,0.6205810290514526,0.6207560378018901,0.6226811340567029,0.6228561428071404,0.6268813440672033,0.6279313965698284,0.6298564928246412,0.6305565278263913,0.6316065803290164,0.632131606580329,0.632831641582079,0.6331816590829541,0.6333566678333916,0.6342317115855792,0.6347567378368918,0.6375568778438921,0.6407070353517675,0.6415820791039551,0.6464823241162058,0.6473573678683934,0.6477073853692684,0.648582429121456,0.6489324466223311,0.6510325516275813,0.6520826041302065,0.652257612880644,0.655932796639832,0.6576828841442072,0.6578578928946447,0.6578578928946447,0.6592579628981449,0.6594329716485824,0.6597829891494574,0.66030801540077,0.6604830241512075,0.6610080504025201,0.6611830591529576,0.6624081204060203,0.664333216660833,0.6650332516625831,0.6660833041652082,0.6671333566678334,0.6678333916695834,0.6690584529226461,0.6692334616730836,0.6699334966748337,0.6704585229261463,0.6737836891844592,0.6741337066853342,0.6751837591879594,0.6755337766888344,0.6771088554427721,0.6774588729436472,0.6778088904445222,0.6792089604480224,0.6802590129506475,0.6807840392019601,0.6814840742037102,0.6830591529576479,0.684459222961148,0.684459222961148,0.6846342317115856,0.6907595379768988,0.6911095554777739,0.6912845642282114,0.691809590479524,0.696534826741337,0.703885194259713,0.7052852642632131,0.7054602730136507,0.7058102905145257,0.7094854742737137,0.7101855092754638,0.7175358767938397,0.7177108855442772,0.7177108855442772,0.7238361918095905,0.7271613580679034,0.7275113755687784,0.727686384319216,0.7310115505775289,0.7313615680784039,0.732411620581029,0.7331116555827791,0.7332866643332167,0.7339866993349667,0.7385369268463423,0.7388869443472174,0.7395869793489674,0.7399369968498425,0.7409870493524676,0.7411620581029051,0.7416870843542177,0.7418620931046552,0.7423871193559678,0.7427371368568428,0.7429121456072804,0.743787189359468,0.7451872593629681,0.7474623731186559,0.7476373818690935,0.7479873993699685,0.763913195659783,0.7653132656632832,0.7663633181659083,0.7665383269163458,0.7826391319565978,0.7828141407070354,0.7829891494574729,0.7845642282114106,0.7864893244662233,0.7868393419670984,0.7875393769688485,0.787714385719286,0.788064403220161,0.7910395519775989,0.7912145607280364,0.7913895694784739,0.7913895694784739,0.7915645782289115,0.7933146657332867,0.7936646832341617,0.7955897794889745,0.7966398319915996,0.7969898494924746,0.7973398669933497,0.7976898844942247,0.7999649982499125,0.7999649982499125,0.80014000700035,0.8004900245012251,0.8010150507525376,0.8111655582779139,0.8123906195309766,0.8125656282814141,0.8141407070353518,0.8143157157857893,0.8148407420371019,0.815890794539727,0.8162408120406021,0.8165908295414771,0.8171158557927897,0.8172908645432272,0.8172908645432272,0.8179908995449773,0.8183409170458523,0.8188659432971649,0.8213160658032902,0.8218410920546028,0.8230661533076654,0.823591179558978,0.8260413020651033,0.8265663283164159,0.8272663633181659,0.827616380819041,0.827966398319916,0.8297164858242912,0.8426671333566679,0.843367168358418,0.848092404620231,0.8486174308715436,0.8507175358767939,0.851767588379419,0.8535176758837942,0.8540427021351068,0.8547427371368569,0.8550927546377319,0.8550927546377319,0.855442772138607,0.8556177808890445,0.8575428771438572,0.8580679033951698,0.8587679383969199,0.8620931046552328,0.8633181659082955,0.863493174658733,0.8640182009100456,0.8652432621631082,0.8654182709135457,0.8699684984249213,0.8708435421771089,0.8713685684284215,0.8722436121806091,0.8769688484424221,0.8785439271963598,0.8809940497024851,0.8811690584529226,0.8855442772138606,0.8864193209660483,0.8865943297164858,0.8867693384669233,0.8867693384669233,0.8876443822191109,0.887994399719986,0.8892194609730486,0.8893944697234861,0.8897444872243612,0.8897444872243612,0.892719635981799,0.8928946447322366,0.8930696534826741,0.8939446972348617,0.8970948547427371,0.8972698634931746,0.8976198809940497,0.8984949247462373,0.8984949247462373,0.8997199859992999,0.8997199859992999,0.9049702485124256,0.9051452572628631,0.9067203360168008,0.9072453622681134,0.9079453972698635,0.908120406020301,0.9126706335316765,0.9130206510325516,0.9137206860343017,0.9159957997899895,0.916170808540427,0.9163458172908645,0.9163458172908645,0.916520826041302,0.9172208610430521,0.9175708785439272,0.9182709135456772,0.9196709835491774,0.9200210010500525,0.9200210010500525,0.9221211060553027,0.9224711235561778,0.9254462723136156,0.9257962898144907,0.9257962898144907,0.927896394819741,0.9280714035701785,0.9287714385719286,0.9289464473223661,0.9294714735736787,0.9298214910745537,0.9301715085754287,0.9305215260763038,0.9317465873293664,0.931921596079804,0.932271613580679,0.932621631081554,0.9333216660833041,0.9348967448372418,0.9350717535876794,0.9354217710885544,0.9368218410920546,0.9369968498424921,0.9378718935946797,0.9380469023451172,0.9382219110955548,0.9382219110955548,0.9396219810990549,0.9401470073503675,0.9413720686034301,0.9417220861043052,0.9422471123556178,0.9425971298564928,0.9427721386069303,0.9427721386069303,0.9460973048652432,0.9469723486174308,0.9471473573678684,0.9476723836191809,0.948372418620931,0.948372418620931,0.9485474273713685,0.9490724536226811,0.9495974798739937,0.9497724886244312,0.9499474973748687,0.9501225061253062,0.9502975148757438,0.9504725236261813,0.9506475323766188,0.9509975498774939,0.9511725586279314,0.9515225761288064,0.9525726286314316,0.9539726986349317,0.9551977598879944,0.9571228561428071,0.9571228561428071,0.9572978648932446,0.9576478823941197,0.9639481974098705,0.964473223661183,0.9658732936646832,0.9662233111655583,0.9667483374168708,0.9669233461673084,0.9672733636681834,0.9702485124256213,0.9704235211760588,0.9753237661883094,0.9754987749387469,0.975848792439622,0.9765488274413721,0.9767238361918096,0.9772488624431221,0.9779488974448722,0.9830241512075604,0.9840742037101855,0.984249212460623,0.9847742387119356,0.9854742737136857,0.9854742737136857,0.9859992999649982,0.9861743087154358,0.9863493174658733,0.9868743437171859,0.9872243612180609,0.9880994049702485,0.9884494224711236,0.9884494224711236,0.9889744487224361,0.9901995099754988,0.9901995099754988,0.9905495274763738,0.9907245362268113,0.9907245362268113,0.9910745537276864,0.9912495624781239,0.991599579978999,0.9917745887294365,0.9917745887294365,0.9917745887294365,0.992299614980749,0.9924746237311866,0.9926496324816241,0.9929996499824991,0.9929996499824991,0.9931746587329366,0.9936996849842492,0.9936996849842492,0.9938746937346867,0.9938746937346867,0.9942247112355618,0.9945747287364368,0.9945747287364368,0.9947497374868743,0.9947497374868743,0.9950997549877494,0.9954497724886244,0.9954497724886244,0.995974798739937,0.9964998249912496,0.9966748337416871,0.9966748337416871,0.9973748687434372,0.9973748687434372,0.9977248862443122,0.9980749037451873,0.9980749037451873,0.9980749037451873,0.9982499124956248,0.9987749387469373,0.9989499474973749,0.9991249562478124,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9994749737486874,0.999649982499125,0.999649982499125,0.9998249912495625,0.9998249912495625,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"type\":\"scatter\"}],                        {\"height\":500,\"title\":{\"text\":\"ROC Curve for XGBoost\"},\"width\":800,\"xaxis\":{\"title\":{\"text\":\"False Positive Rate\"}},\"yaxis\":{\"title\":{\"text\":\"True Positive Rate\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('da9791bc-c4fb-4edf-85a9-805968d6cf63');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.graph_objs as go\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(x_train, y_train)\n",
        "\n",
        "# Calculate ROC curve and AUC for XGBoost\n",
        "xgb_probs = xgb_model.predict_proba(x_test)[:, 1]\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "xgb_auc = auc(xgb_fpr, xgb_tpr)\n",
        "\n",
        "# Plot ROC curve using Plotly\n",
        "xgb_trace = go.Scatter(x=xgb_fpr, y=xgb_tpr, mode='lines', name=f'XGBoost (AUC = {xgb_auc:.2f})')\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='ROC Curve for XGBoost',\n",
        "    xaxis=dict(title='False Positive Rate'),\n",
        "    yaxis=dict(title='True Positive Rate'),\n",
        "    height=500,  # Specify height\n",
        "    width=800    # Specify width\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[xgb_trace], layout=layout)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptgIvjBKDQxJ",
        "outputId": "00b1b74f-fe38-452b-d68f-2b133b991735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"275841ca-2af3-4d69-a93c-d04c1d1e3cd2\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"275841ca-2af3-4d69-a93c-d04c1d1e3cd2\")) {                    Plotly.newPlot(                        \"275841ca-2af3-4d69-a93c-d04c1d1e3cd2\",                        [{\"mode\":\"lines\",\"name\":\"AUC Curve (AUC = 0.98)\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,8.575593859874797e-05,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.00017151187719749593,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.0002572678157962439,0.00034302375439499187,0.00034302375439499187,0.00034302375439499187,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0004287796929937398,0.0005145356315924878,0.0005145356315924878,0.0005145356315924878,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0006002915701912358,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0007718034473887317,0.0008575593859874796,0.0008575593859874796,0.0008575593859874796,0.0010290712631849757,0.0010290712631849757,0.0010290712631849757,0.0011148272017837235,0.0012005831403824716,0.0012863390789812194,0.0012863390789812194,0.0012863390789812194,0.0012863390789812194,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0013720950175799675,0.0014578509561787153,0.0014578509561787153,0.0014578509561787153,0.0014578509561787153,0.0015436068947774634,0.0015436068947774634,0.0015436068947774634,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0016293628333762112,0.0017151187719749593,0.0017151187719749593,0.0017151187719749593,0.0017151187719749593,0.0018008747105737071,0.0018866306491724552,0.0018866306491724552,0.0018866306491724552,0.0019723865877712033,0.002143898464968699,0.002229654403567447,0.002229654403567447,0.002229654403567447,0.002229654403567447,0.002315410342166195,0.002658434096561187,0.002829945973758683,0.002829945973758683,0.002829945973758683,0.0029157019123574306,0.003172969728153675,0.0032587256667524224,0.0032587256667524224,0.0032587256667524224,0.0033444816053511705,0.0033444816053511705,0.0033444816053511705,0.0034302375439499186,0.0034302375439499186,0.0035159934825486666,0.0036017494211474143,0.0036875053597461623,0.0036875053597461623,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0037732612983449104,0.0038590172369436584,0.0038590172369436584,0.004030529114141155,0.004116285052739903,0.004116285052739903,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.00420204099133865,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004287796929937398,0.004459308807134894,0.004459308807134894,0.004459308807134894,0.00463082068433239,0.00463082068433239,0.004802332561529886,0.004802332561529886,0.004802332561529886,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.004888088500128634,0.00505960037732613,0.00505960037732613,0.00505960037732613,0.005231112254523626,0.005316868193122374,0.005316868193122374,0.005316868193122374,0.005316868193122374,0.005402624131721122,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005659891947517366,0.005745647886116113,0.005831403824714861,0.005831403824714861,0.005917159763313609,0.005917159763313609,0.005917159763313609,0.005917159763313609,0.0061744275791098535,0.0061744275791098535,0.0061744275791098535,0.006260183517708602,0.006260183517708602,0.00634593945630735,0.00634593945630735,0.006431695394906097,0.006431695394906097,0.006431695394906097,0.006517451333504845,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006688963210702341,0.006774719149301089,0.006774719149301089,0.006774719149301089,0.0072034988422948285,0.007289254780893577,0.007460766658091073,0.007546522596689821,0.00797530228968356,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008146814166881056,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008661349798473545,0.008747105737072292,0.008747105737072292,0.00883286167567104,0.00883286167567104,0.00883286167567104,0.008918617614269788,0.008918617614269788,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009004373552868537,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009090129491467284,0.009175885430066031,0.00926164136866478,0.009347397307263527,0.009433153245862276,0.009433153245862276,0.009433153245862276,0.009604665123059773,0.009604665123059773,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010204956693251008,0.010290712631849755,0.010376468570448504,0.010376468570448504,0.010547980447646,0.010547980447646,0.010547980447646,0.010633736386244748,0.010633736386244748,0.010719492324843495,0.010719492324843495,0.010719492324843495,0.011234027956435983,0.011234027956435983,0.011319783895034732,0.011319783895034732,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.01140553983363348,0.011577051710830975,0.011577051710830975,0.011662807649429723,0.011662807649429723,0.011748563588028471,0.011834319526627219,0.011834319526627219,0.012005831403824715,0.012177343281022211,0.012348855158219707,0.012348855158219707,0.012348855158219707,0.0126918789126147,0.012777634851213446,0.012777634851213446,0.012863390789812194,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.012949146728410943,0.01303490266700969,0.013120658605608439,0.013120658605608439,0.013292170482805935,0.013292170482805935,0.013292170482805935,0.013635194237200925,0.013806706114398421,0.014235485807392163,0.014235485807392163,0.01432124174599091,0.01432124174599091,0.014406997684589657,0.014835777377583398,0.014835777377583398,0.015178801131978389,0.015178801131978389,0.015178801131978389,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015264557070577138,0.015521824886373381,0.015521824886373381,0.01560758082497213,0.015779092702169626,0.015779092702169626,0.016036360517965868,0.016036360517965868,0.016207872395163365,0.016207872395163365,0.01637938427236086,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016636652088157105,0.016722408026755852,0.0168081639653546,0.0168081639653546,0.01689391990395335,0.01689391990395335,0.017065431781150844,0.017065431781150844,0.017236943658348342,0.017236943658348342,0.01732269959694709,0.01732269959694709,0.017494211474144584,0.017494211474144584,0.017494211474144584,0.01775147928994083,0.01775147928994083,0.01775147928994083,0.017922991167138323,0.017922991167138323,0.018437526798730813,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.01852328273732956,0.018609038675928308,0.018609038675928308,0.018609038675928308,0.018694794614527055,0.018780550553125806,0.018780550553125806,0.018780550553125806,0.019123574307520794,0.01938084212331704,0.01938084212331704,0.019466598061915787,0.019466598061915787,0.019552354000514537,0.019981133693508277,0.019981133693508277,0.019981133693508277,0.020066889632107024,0.020066889632107024,0.02023840150930452,0.020495669325100763,0.02058142526369951,0.02058142526369951,0.020838693079495756,0.020838693079495756,0.02101020495669325,0.021610496526884487,0.021696252465483234,0.021696252465483234,0.02178200840408198,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.021867764342680732,0.022039276219878227,0.022125032158476974,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022296544035674472,0.022468055912871966,0.022468055912871966,0.022896835605865706,0.022982591544464453,0.023239859360260698,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023411371237458192,0.023668639053254437,0.023668639053254437,0.024097418746248177,0.024097418746248177,0.024097418746248177,0.024526198439241916,0.024526198439241916,0.024526198439241916,0.024611954377840667,0.024611954377840667,0.02478346625503816,0.02478346625503816,0.02478346625503816,0.02486922219363691,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.025298001886630648,0.0253837578252294,0.0253837578252294,0.025812537518223138,0.02606980533401938,0.02606980533401938,0.026412829088414372,0.026412829088414372,0.026412829088414372,0.02658434096561187,0.02658434096561187,0.02658434096561187,0.02658434096561187,0.026670096904210617,0.026670096904210617,0.026927364720006862,0.02701312065860561,0.027184632535803104,0.027184632535803104,0.02727038847440185,0.02727038847440185,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.0273561444130006,0.02744190035159935,0.02744190035159935,0.027956435983191835,0.027956435983191835,0.027956435983191835,0.028127947860389333,0.028556727553383073,0.029157019123574306,0.029157019123574306,0.029242775062173057,0.029242775062173057,0.02984306663236429,0.02984306663236429,0.03070062601835177,0.03070062601835177,0.03078638195695052,0.031472429465740505,0.03155818540433925,0.03155818540433925,0.03155818540433925,0.031643941342938,0.031729697281536746,0.031815453220135494,0.03190120915873424,0.03190120915873424,0.03190120915873424,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03241574479032673,0.03250150072892548,0.03250150072892548,0.03250150072892548,0.03275876854472172,0.03284452448332047,0.03284452448332047,0.03284452448332047,0.03455964325529543,0.03455964325529543,0.034731155132492926,0.034731155132492926,0.03730383329045536,0.03738958922905411,0.03738958922905411,0.03781836892204785,0.038075636737844094,0.038075636737844094,0.03833290455364034,0.03833290455364034,0.03841866049223909,0.038847440185232826,0.038847440185232826,0.03893319612383157,0.03901895206243032,0.03901895206243032,0.03927621987822657,0.039361975816825316,0.03961924363262156,0.039704999571220305,0.039704999571220305,0.039704999571220305,0.039704999571220305,0.03996226738701655,0.0400480233256153,0.040219535202812795,0.040219535202812795,0.040219535202812795,0.040562558957207784,0.040562558957207784,0.040562558957207784,0.04064831489580654,0.04064831489580654,0.04064831489580654,0.04064831489580654,0.04081982677300403,0.04081982677300403,0.04090558271160278,0.04090558271160278,0.04099133865020153,0.04116285052739902,0.04116285052739902,0.04116285052739902,0.04150587428179402,0.04150587428179402,0.04167738615899151,0.04167738615899151,0.04210616585198525,0.04210616585198525,0.04210616585198525,0.042191921790584,0.042363433667781496,0.04244918960638024,0.04356401680816396,0.04382128462396021,0.04425006431695395,0.04425006431695395,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.04442157619415144,0.044507332132750196,0.044850355887145185,0.044850355887145185,0.044850355887145185,0.044850355887145185,0.04493611182574393,0.045193379641540174,0.04527913558013893,0.04527913558013893,0.045364891518737675,0.045364891518737675,0.045364891518737675,0.045965183088928906,0.04605093902752765,0.04613669496612641,0.04613669496612641,0.04656547465912014,0.046822742474916385,0.04708001029071263,0.04708001029071263,0.04750878998370637,0.04750878998370637,0.047594545922305116,0.047594545922305116,0.04776605779950262,0.047851813738101365,0.047851813738101365,0.0482805934310951,0.04836634936969385,0.04836634936969385,0.0484521053082926,0.04913815281708258,0.049223908755681334,0.049223908755681334,0.049223908755681334,0.049481176571477575,0.049481176571477575,0.049481176571477575,0.04990995626447131,0.049995712203070065,0.05016722408026756,0.05025298001886631,0.051110539404853786,0.051110539404853786,0.05128205128205128,0.05136780722065003,0.05136780722065003,0.05136780722065003,0.05205385472944001,0.05205385472944001,0.05213961066803876,0.052482634422433755,0.052482634422433755,0.0525683903610325,0.05265414629963125,0.05265414629963125,0.052825658176828744,0.052825658176828744,0.05299717005402624,0.053340193808421234,0.053340193808421234,0.05342594974701998,0.053854729440013724,0.05394048537861247,0.05419775319440871,0.05419775319440871,0.05428350913300746,0.054626532887402456,0.054626532887402456,0.0548838007031987,0.0548838007031987,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05505531258039619,0.05514106851899494,0.05539833633479119,0.05539833633479119,0.05539833633479119,0.05565560415058743,0.055741360089186176,0.05582711602778492,0.05582711602778492,0.05591287196638367,0.05599862790498242,0.056084383843581165,0.056084383843581165,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05634165165937741,0.05642740759797616,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.05651316353657491,0.0566846754137724,0.0566846754137724,0.05677043135237115,0.05677043135237115,0.05677043135237115,0.056856187290969896,0.057113455106766145,0.057113455106766145,0.05728496698396364,0.05737072292256239,0.05737072292256239,0.05737072292256239,0.05737072292256239,0.05779950261555613,0.05814252636995112,0.058485550124346114,0.05942886544893234,0.059600377326129834,0.05968613326472858,0.05977188920332733,0.06157276391390104,0.06157276391390104,0.06183003172969728,0.06183003172969728,0.062087299545493524,0.06225881142269102,0.06225881142269102,0.06371666237886973,0.06371666237886973,0.06431695394906098,0.06431695394906098,0.06440270988765973,0.06474573364205471,0.06474573364205471,0.06483148958065346,0.06491724551925221,0.06603207272103594,0.06646085241402967,0.06646085241402967,0.06654660835262842,0.06654660835262842,0.06663236429122717,0.06663236429122717,0.06663236429122717,0.06671812022982591,0.06697538804562216,0.06714689992281965,0.0672326558614184,0.0672326558614184,0.06731841180001714,0.0674041677386159,0.0674041677386159,0.0675756796158134,0.0675756796158134,0.0675756796158134,0.0677471914930109,0.0677471914930109,0.0677471914930109,0.06809021524740588,0.06809021524740588,0.06826172712460338,0.06834748306320212,0.06834748306320212,0.06843323900180087,0.06843323900180087,0.06843323900180087,0.06851899494039962,0.06851899494039962,0.06869050681759711,0.06886201869479461,0.06894777463339337,0.06903353057199212,0.06903353057199212,0.06911928651059086,0.06920504244918961,0.06920504244918961,0.06929079838778836,0.06929079838778836,0.0697195780807821,0.06980533401938084,0.06997684589657834,0.07040562558957207,0.07057713746676958,0.07066289340536833,0.0713489409141583,0.0715204527913558,0.0715204527913558,0.07263527999313953,0.07280679187033702,0.07314981562473201,0.073492839379127,0.073835863133522,0.073835863133522,0.07392161907212075,0.07392161907212075,0.07409313094931824,0.07435039876511448,0.07435039876511448,0.07443615470371323,0.07452191064231198,0.07452191064231198,0.07460766658091073,0.07460766658091073,0.07486493439670697,0.07546522596689821,0.07555098190549696,0.0759797615984907,0.07606551753708944,0.07623702941428694,0.07649429723008318,0.07666580910728069,0.07692307692307693,0.07880970757224938,0.07923848726524312,0.07932424320384186,0.07966726695823685,0.07975302289683561,0.0800960466512306,0.0802675585284281,0.08069633822142183,0.08078209416002058,0.08232570105479804,0.08249721293199554,0.08344052825658177,0.08412657576537175,0.08472686733556299,0.08489837921276049,0.08498413515135923,0.08515564702855673,0.08592745047594547,0.08644198610753795,0.08687076580053169,0.08704227767772918,0.08738530143212417,0.08747105737072293,0.08807134894091416,0.08841437269530915,0.08901466426550039,0.08910042020409914,0.08927193208129663,0.08952919989709288,0.08970071177429037,0.08987222365148786,0.0901294914672841,0.09107280679187034,0.09124431866906783,0.09150158648486408,0.09201612211645656,0.0921018780550553,0.09227338993225281,0.09235914587085156,0.0926164136866478,0.09295943744104279,0.09313094931824029,0.09321670525683903,0.09338821713403653,0.09347397307263527,0.09364548494983277,0.09390275276562902,0.09407426464282652,0.09433153245862276,0.09450304433582025,0.09467455621301775,0.09836206157276392,0.09853357344996141,0.09870508532715891,0.09879084126575766,0.09913386502015265,0.09947688877454763,0.09999142440614013,0.10024869222193637,0.10050596003773261,0.1008489837921276,0.1010204956693251,0.10119200754652259,0.10136351942372009,0.10187805505531258,0.10213532287110882,0.10230683474830632,0.10256410256410256,0.10307863819569506,0.10325015007289255,0.1033359060114913,0.10367892976588629,0.10393619758168253,0.10402195352028128,0.10419346539747877,0.10436497727467627,0.10453648915187377,0.10539404853786125,0.10547980447645999,0.10590858416945373,0.10642311980104623,0.1072806791870337,0.10736643512563245,0.10770945888002745,0.1077952148186262,0.10796672669582369,0.10805248263442244,0.10856701826601492,0.10882428608181116,0.10933882171340366,0.1094245776520024,0.10985335734499614,0.11302632707314982,0.11328359488894606,0.1133693508275448,0.1138838864591373,0.11396964239773605,0.11414115427493354,0.11422691021353229,0.11439842209072978,0.11456993396792728,0.11482720178372352,0.11499871366092101,0.11517022553811851,0.11577051710830975,0.11654232055569848,0.11662807649429723,0.1176571477574822,0.1178286596346797,0.1180001715118772,0.11817168338907469,0.11851470714346969,0.11860046308206844,0.11877197495926593,0.12005831403824715,0.12022982591544465,0.12057284966983964,0.12100162936283337,0.12108738530143212,0.12134465311722836,0.12143040905582711,0.12177343281022211,0.1219449446874196,0.1221164565646171,0.12245948031901209,0.12357430752079582,0.12383157533659206,0.12400308721378955,0.12443186690678329,0.12451762284538204,0.12477489066117829,0.12494640253837579,0.12511791441557327,0.12520367035417201,0.1253751822313695,0.12563245004716578,0.12700454506474573,0.12709030100334448,0.12726181288054197,0.12743332475773947,0.1278621044507332,0.12846239602092444,0.12880541977531945,0.1288911757139182,0.12931995540691194,0.1300060029157019,0.1301775147928994,0.13189263356487438,0.13206414544207187,0.1326644370122631,0.1330074607666581,0.13360775233684932,0.13369350827544807,0.13386502015264556,0.1341222879684418,0.13437955578423805,0.13472257953863306,0.13635194237200926,0.13832432895978047,0.13858159677557672,0.1394391561615642,0.13961066803876168,0.13995369179315667,0.14012520367035416,0.14046822742474915,0.14063973930194665,0.14098276305634166,0.14115427493353916,0.1412400308721379,0.1414115427493354,0.14149729868793415,0.14218334619672413,0.14226910213532287,0.14261212588971786,0.14295514964411285,0.14389846496869907,0.14424148872309409,0.14467026841608782,0.14484178029328532,0.14492753623188406,0.1456993396792728,0.14578509561787154,0.14655689906526026,0.14672841094245775,0.14741445845124776,0.14835777377583398,0.14844352971443273,0.14904382128462396,0.1491295772232227,0.14955835691621644,0.14990138067061143,0.15007289254780892,0.15024440442500644,0.15153074350398765,0.15204527913558014,0.15238830288997512,0.15316010633736385,0.15333161821456137,0.15401766572335135,0.15418917760054884,0.15444644541634509,0.15496098104793757,0.15513249292513506,0.15564702855672755,0.15581854043392504,0.1567618557585113,0.15719063545150502,0.15744790326730126,0.15779092702169625,0.15830546265328874,0.15839121859188748,0.15873424234628247,0.1591630220392762,0.1593345339164737,0.16010633736386246,0.1601920933024612,0.1603636051796587,0.16079238487265243,0.16130692050424492,0.16139267644284366,0.16156418832004116,0.1616499442586399,0.1619929680130349,0.1621644798902324,0.16242174770602863,0.16276477146042365,0.16293628333762114,0.16345081896921362,0.16362233084641112,0.16370808678500987,0.16387959866220736,0.16456564617099734,0.16482291398679358,0.16499442586399107,0.16610925306577481,0.16619500900437356,0.16670954463596604,0.16713832432895978,0.16722408026755853,0.1682531515307435,0.16833890746934224,0.16868193122373723,0.16902495497813225,0.16936797873252724,0.16953949060972473,0.16962524654832348,0.16979675842552097,0.16988251436411972,0.1703970499957122,0.17065431781150844,0.17091158562730469,0.17099734156590343,0.17134036532029842,0.17142612125889717,0.17176914501329216,0.17236943658348342,0.17245519252208216,0.17296972815367465,0.17331275190806963,0.17382728753966212,0.17391304347826086,0.1746848469256496,0.17485635880284708,0.17511362661864335,0.1751993825572421,0.1755424063116371,0.17579967412743333,0.17597118600463083,0.17605694194322957,0.17622845382042707,0.17657147757482206,0.17674298945201955,0.1770002572678158,0.17811508446959953,0.17837235228539577,0.17854386416259327,0.17862962010119202,0.1788011319783895,0.17888688791698826,0.17905839979418575,0.1791441557327845,0.1794871794871795,0.17974444730297573,0.18008747105737072,0.18017322699596947,0.18051625075036445,0.1809450304433582,0.1812880541977532,0.1814595660749507,0.1873767258382643,0.1875482377154618,0.18823428522425178,0.18857730897864677,0.18909184461023926,0.18926335648743675,0.19046393962781924,0.19192179058399794,0.1920075465225967,0.19217905839979418,0.19355115341737417,0.1938084212331704,0.1939799331103679,0.19500900437355287,0.1952662721893491,0.1954377840665466,0.1959523196981391,0.19620958751393533,0.20126918789126147,0.20144069976845896,0.20229825915444644,0.2181631077952148,0.21833461967241233,0.21850613154960982,0.21867764342680732,0.21893491124260356,0.21927793499699855,0.2193636909355973,0.2216791012777635,0.221850613154961,0.22202212503215848,0.22862533230426207,0.23085498670782953,0.23102649858502702,0.23728668210273562,0.23737243804133437,0.23805848555012435,0.25692479204184887,0.2584683989366264,0.2613841008489838,0.26164136866478005,0.26198439241917504,0.2624989280507675,0.26361375525255126,0.26369951119115,0.26429980276134124,0.26447131463853873,0.26524311808592743,0.2653288740245262,0.2660149215333162,0.26644370122630995,0.26678672498070494,0.26704399279650115,0.2674727724894949,0.26755852842809363,0.2703884744018523,0.27527656290198094,0.275619586656376,0.2775062173055484,0.2778492410599434,0.2781922648143384,0.2782780207529371,0.2787068004459309,0.27896406826172715,0.28025040734070833,0.2811079667266958,0.28453820427064574,0.28462396020924446,0.2881399536917932,0.2883972215075894,0.2887402452619844,0.29431438127090304,0.29440013720950176,0.30168939199039535,0.30177514792899407,0.3025469513763828,0.30434782608695654,0.3050338735957465,0.3054626532887402,0.3205556984821199,0.32201354943829863,0.3221850613154961,0.3228711088242861,0.325272275105051,0.3282737329560072,0.3352199639825058,0.3509990566846754,0.3510848126232742,0.35202812794786037,0.35245690764085413,0.3609467455621302,0.3613755252551239,0.36146128119372267,0.3634336677814939,0.36643512563245006,0.36677814938684505,0.39473458537003686,0.3956779006946231,0.3959351685104193,0.3961066803876168,0.39644970414201186,0.3965354600806106,0.39687848383500557,0.39713575165080184,0.41505874281794014,0.41523025469513763,0.4157447903267301,0.42080439070405623,0.42449189606380244,0.4251779435725924,0.4271503301603636,0.42757910985335734,0.42792213360775233,0.4410427922133608,0.4411285481519595,0.4415573278449533,0.4509904810908155,0.4585370036875054,0.45922305119629536,0.4593088071348941,0.461023925906869,0.463768115942029,0.4768030186090387,0.47697453048623617,0.48923762970585716,0.49198181974101707,0.4966983963639482,0.4968699082411457,0.4986707829517194,0.4987565388903182,0.5034731155132492,0.5035588714518481,0.5245690764085413,0.5281708258296887,0.5283423377068862,0.5289426292770775,0.5304004802332561,0.5304862361718549,0.5307435039876511,0.5343452534087986,0.5351170568561873,0.5487522510933882,0.5488380070319869,0.5616156418832005,0.5660749506903353,0.5681330932167052,0.5691621644798902,0.570276991681674,0.5744790326730126,0.5745647886116113,0.5748220564274076,0.5775662464625675,0.598833719235057,0.5989194751736557,0.6128976931652517,0.6130692050424492,0.6249892805076751,0.6407683732098448,0.6640082325701054,0.6673527141754566,0.6680387616842466,0.6681245176228454,0.6684675413772404,0.6685532973158391,0.6687248091930366,0.6690678329474317,0.6759283080353314,0.6875053597461624,0.715204527913558,0.7153760397907555,0.715547551667953,0.716233599176743,0.7246376811594203,0.7248091930366178,0.7249807049138153,0.7251522167910128,0.7286682102735614,0.7287539662121602,0.7300403052911414,0.7338135665894864,0.7354429294228625,0.73561444130006,0.7357859531772575,0.7385301432124175,0.7405882857387874,0.7423891604493611,0.744447302975731,0.7460766658091073,0.7464196895635022,0.7465912014406998,0.7469342251950948,0.747191493010891,0.749592659291656,0.7500214389846497,0.7501071949232484,0.7507932424320384,0.7513077780636309,0.7513935340022296,0.7519080696338222,0.7531086527742046,0.7534516765285996,0.7536231884057971,0.7540519680987908,0.7719749592659292,0.7855243975645313,0.7856959094417288,0.7860389331961238,0.7866392247663151,0.7876682960295001,0.7877540519680988,0.7910127776348512,0.79109853357345,0.7948717948717948,0.7953005745647886,0.7953863305033874,0.7959008661349799,0.7959866220735786,0.7964154017665723,0.7965869136437699,0.796844181459566,0.7970156933367636,0.7971014492753623,0.7976159849069548,0.7984735442929423,0.7987308121087385,0.7990738358631335,0.7992453477403311,0.7995883714947261,0.800274419003516,0.8007889546351085,0.8013892462052997,0.8017322699596947,0.801989537775491,0.8021610496526884,0.8027613412228797,0.8035331446702684,0.8056770431352371,0.8061915787668296,0.8075636737844095,0.8088500128633908,0.8091930366177857,0.8092787925563846,0.809450304433582,0.8100505960037733,0.810136351942372,0.810822399451162,0.8109939113283595,0.8127947860389332,0.8128805419775319,0.8135665894863219,0.8138238573021181,0.8150244404425007,0.8151959523196981,0.8154532201354944,0.8155389760740931,0.8158819998284881,0.8160535117056856,0.8176828745390619,0.8178543864162593,0.8182831661092531,0.8183689220478518,0.8187977017408455,0.8188834576794443,0.8190549695566418,0.8200840408198268,0.8202555526970242,0.8209416002058143,0.821713403653203,0.8235142783637767,0.8247148615041592,0.8249721293199554,0.8251436411971529,0.8252293971357516,0.8257439327673441,0.8259154446445416,0.8272017837235228,0.8289169024954978,0.8290026584340966,0.8297744618814853,0.829860217820084,0.831232312837664,0.8314038247148615,0.831575336592059,0.8317468484692565,0.8320041162850528,0.833633479118429,0.8338907469342252,0.833976502872824,0.8377497641711689,0.8384358116799588,0.8385215676185576,0.8388645913729526,0.8401509304519338,0.8404081982677301,0.840751222022125,0.841437269530915,0.8416087814081125,0.84178029328531,0.8428951204870937,0.8430666323642912,0.8438384358116799,0.8439241917502787,0.8456393105222537,0.8458108223994512,0.8462396020924449,0.8463253580310437,0.8464968699082411,0.8465826258468399,0.8468398936626361,0.8473544292942287,0.8475259411714261,0.8483835005574136,0.8484692564960123,0.8594460166366521,0.8601320641454421,0.8605608438384358,0.8641625932595832,0.8643341051367808,0.8652774204613669,0.8657062001543607,0.8667352714175457,0.8699082411456993,0.8713660921018781,0.8719663836720692,0.872738187119458,0.8728239430580568,0.8733384786896493,0.8761684246634079,0.8763399365406055,0.8767687162335992,0.8791698825143641,0.8799416859617528,0.881571048795129,0.8819998284881228,0.8845725066460852,0.886030357602264,0.8895463510848126,0.8898893748392076,0.8902323985936026,0.8903181545322013,0.8939199039533487,0.8951204870937313,0.8952919989709287,0.8970071177429036,0.8971786296201012,0.9003515993482548,0.9017236943658349,0.9018952062430323,0.9021524740588286,0.9025812537518223,0.9056684675413772,0.9082411456993397,0.9111568476116971,0.9112426035502958,0.914930108910042,0.9151016207872396,0.9178458108223995,0.9180173226995969,0.9182745905153932,0.9186176142697882,0.9193894177171769,0.9195609295943744,0.926850184375268,0.9347397307263527,0.9351685104193466,0.9616670954463596,0.9641540176657234,0.9644112854815196,0.9658691364376983,0.9662121601920933,0.9664694280078896,0.9668124517622846,0.966983963639482,0.9692136180430495,0.9692993739816482,0.9698139096132408,0.9699854214904382,0.9700711774290369,0.9703284452448332,0.9706714689992282,0.9707572249378269,0.9709287368150245,0.9781322356573192,0.978217991595918,0.9839636394820341,0.9843066632364291,0.9844781751136266,0.9845639310522254,0.9852499785610154,0.9856787582540091,0.987736900780379,0.9879941685961753,0.9885087042277678,0.9885944601663665,0.9893662636137552,0.9896235314295515,0.989795043306749,0.9898807992453478,0.9901380670611439,0.9903095789383415,0.9905668467541378,0.9909956264471315,0.9916816739559214,0.9917674298945202,0.992539233341909,0.992882257096304,0.9935683046050939,0.9940828402366864,0.9941685961752852,0.9943401080524826,0.9945973758682789,0.9947688877454763,0.9951119114998713,0.9951976674384702,0.9963124946402538,0.9963982505788526,0.9966555183946488,0.9973415659034388,0.9976845896578338,0.9983706371666238,0.99862790498242,0.998970928736815,0.9994854643684075,0.9995712203070063,1.0],\"y\":[0.0,0.00035001750087504374,0.0005250262513125656,0.001400070003500175,0.0017500875043752187,0.002450122506125306,0.002625131256562828,0.002975148757437872,0.003325166258312916,0.0036751837591879593,0.0038501925096254812,0.004550227511375569,0.004725236261813091,0.005075253762688134,0.0056002800140007,0.005950297514875744,0.006300315015750788,0.0077003850192509625,0.008050402520126006,0.008750437521876094,0.00927546377318866,0.009625481274063704,0.011025551277563878,0.011725586279313966,0.011900595029751488,0.012250612530626532,0.013125656282814141,0.013300665033251663,0.013650682534126707,0.014175708785439271,0.014350717535876793,0.015050752537626881,0.015400770038501925,0.015575778788939447,0.01662583129156458,0.017325866293314666,0.017325866293314666,0.018375918795939798,0.018725936296814842,0.018725936296814842,0.01942597129856493,0.01960098004900245,0.019950997549877492,0.020826041302065102,0.021351067553377668,0.021701085054252712,0.02327616380819041,0.023626181309065453,0.023801190059502975,0.02520126006300315,0.02520126006300315,0.025726286314315717,0.027826391319565977,0.02887644382219111,0.029576478823941196,0.030101505075253762,0.03080154007700385,0.031151557577878894,0.03237661883094155,0.03255162758137907,0.033076653832691635,0.03447672383619181,0.034826741337066855,0.03902695134756738,0.03972698634931746,0.04060203010150507,0.040777038851942594,0.04112705635281764,0.04130206510325516,0.04235211760588029,0.043402170108505424,0.044277213860693033,0.044452222611130555,0.048127406370318516,0.04917745887294365,0.05110255512775639,0.051977598879944,0.053377668883444175,0.05372768638431922,0.05390269513475674,0.05390269513475674,0.05407770388519426,0.05460273013650683,0.054777738886944344,0.05530276513825691,0.05652782639131956,0.056702835141757085,0.05722786139306965,0.05775288764438222,0.05827791389569478,0.05862793139656983,0.05880294014700735,0.0616030801540077,0.06177808890444522,0.06230311515575779,0.06370318515925796,0.06370318515925796,0.06492824641232062,0.06510325516275814,0.0656282814140707,0.06580329016450823,0.06650332516625831,0.06685334266713336,0.06737836891844592,0.06772838641932097,0.06807840392019601,0.06825341267063353,0.06860343017150858,0.0687784389219461,0.07000350017500875,0.07070353517675884,0.07087854392719636,0.07157857892894645,0.07262863143157158,0.07297864893244663,0.07350367518375919,0.07367868393419671,0.07402870143507176,0.07507875393769689,0.07525376268813441,0.07577878893944698,0.07962898144907245,0.08032901645082254,0.0808540427021351,0.0808540427021351,0.0808540427021351,0.08732936646832341,0.08767938396919846,0.08837941897094855,0.08872943647182359,0.0992299614980749,0.10850542527126357,0.11008050402520125,0.11025551277563878,0.11130556527826391,0.11148057402870143,0.11830591529576479,0.1191809590479524,0.12198109905495275,0.12285614280714036,0.12373118655932797,0.12443122156107805,0.12530626531326566,0.12863143157157858,0.1288064403220161,0.1288064403220161,0.13773188659432972,0.13790689534476724,0.1382569128456423,0.13983199159957999,0.14700735036751839,0.147882394119706,0.14805740287014352,0.1487574378718936,0.14928246412320617,0.15838291914595728,0.16275813790689533,0.16275813790689533,0.16835841792089604,0.16870843542177108,0.1688834441722086,0.1697584879243962,0.16993349667483373,0.17028351417570878,0.17098354917745887,0.17185859292964648,0.17220861043052152,0.17238361918095904,0.17273363668183409,0.17938396919845992,0.18043402170108505,0.1807840392019601,0.18130906545327266,0.18253412670633531,0.18288414420721036,0.18760938046902345,0.18813440672033602,0.18830941547077354,0.19723486174308716,0.19793489674483725,0.2031851592579629,0.20336016800840043,0.20406020301015051,0.20406020301015051,0.2047602380119006,0.20598529926496326,0.2063353167658383,0.20651032551627582,0.21141057052852644,0.21176058802940148,0.211935596779839,0.21701085054252714,0.21963598179908994,0.21981099054952746,0.2201610080504025,0.22068603430171507,0.22313615680784038,0.22996149807490374,0.23013650682534126,0.23083654182709135,0.2313615680784039,0.2376618830941547,0.23888694434721736,0.23906195309765488,0.24151207560378019,0.24221211060553027,0.2432621631081554,0.24361218060903045,0.24833741687084354,0.25568778438921946,0.255862793139657,0.25726286314315716,0.2574378718935947,0.2577878893944697,0.2590129506475324,0.2591879593979699,0.2607630381519076,0.2609380469023451,0.2637381869093455,0.2728386419320966,0.27458872943647183,0.2752887644382219,0.27633881694084705,0.2766888344417221,0.27703885194259714,0.27721386069303466,0.2775638781939097,0.2782639131956598,0.27948897444872245,0.283689184459223,0.28438921946097306,0.2845642282114106,0.29051452572628633,0.2933146657332867,0.29401470073503677,0.29558977948897447,0.3055652782639132,0.31326566328316413,0.31414070703535174,0.31571578578928944,0.3160658032901645,0.31659082954147705,0.3295414770738537,0.3297164858242912,0.3304165208260413,0.3307665383269163,0.33094154707735385,0.33339166958347916,0.3335666783339167,0.3375918795939797,0.3377668883444172,0.3379418970948547,0.33811690584529225,0.3384669233461673,0.33881694084704234,0.33899194959747986,0.34196709835491773,0.3424921246062303,0.34284214210710534,0.34354217710885543,0.34371718585929295,0.34441722086104304,0.34529226461323065,0.3456422821141057,0.3472173608680434,0.35404270213510675,0.35421771088554427,0.3545677283864193,0.3552677633881694,0.3552677633881694,0.3554427721386069,0.3584179208960448,0.35964298214910745,0.3615680784039202,0.3704935246762338,0.37066853342667133,0.3781939096954848,0.3783689184459223,0.3790689534476724,0.37941897094854743,0.37959397969898495,0.38029401470073504,0.3806440322016101,0.38116905845292265,0.3853692684634232,0.3857192859642982,0.3862443122156108,0.3862443122156108,0.3864193209660483,0.38851942597129857,0.3886944347217361,0.3893944697234862,0.3976198809940497,0.39796989849492476,0.3984949247462373,0.39866993349667484,0.3990199509975499,0.3991949597479874,0.3997199859993,0.400070003500175,0.40182009100455024,0.40287014350717537,0.40357017850892546,0.40444522226111307,0.4047952397619881,0.40514525726286316,0.4086454322716136,0.4096954847742387,0.4109205460273014,0.41144557227861395,0.41319565978298917,0.4135456772838642,0.41389569478473925,0.41879593979698987,0.4229961498074904,0.42824641232061605,0.42964648232411623,0.4299964998249913,0.4326216310815541,0.43367168358417924,0.4343717185859293,0.4366468323416171,0.43752187609380466,0.4376968848442422,0.4387469373468673,0.4396219810990549,0.440322016100805,0.44067203360168006,0.4427721386069303,0.44294714735736784,0.44452222611130554,0.44452222611130554,0.44469723486174306,0.44697234861743085,0.44767238361918094,0.44784739236961846,0.4515225761288064,0.45414770738536925,0.4609730486524326,0.46132306615330765,0.4616730836541827,0.46219810990549526,0.4670983549177459,0.47234861743087153,0.4728736436821841,0.48022401120056,0.48039901995099754,0.48109905495274763,0.4823241162058103,0.4833741687084354,0.48354917745887294,0.483899194959748,0.48459922996149807,0.4912495624781239,0.4914245712285614,0.4914245712285614,0.491949597479874,0.4929996499824991,0.4992999649982499,0.49947497374868743,0.5026251312565628,0.5028001400070004,0.5028001400070004,0.5105005250262513,0.5106755337766888,0.5140007000350018,0.5145257262863143,0.5147007350367518,0.5150507525376269,0.5152257612880644,0.5164508225411271,0.5169758487924396,0.5171508575428772,0.5178508925446272,0.5180259012950648,0.5190759537976899,0.520126006300315,0.5217010850542527,0.5218760938046902,0.5220511025551278,0.5222261113055653,0.524151207560378,0.5243262163108156,0.5246762338116906,0.5285264263213161,0.5290514525726286,0.5297514875743787,0.5364018200910046,0.5369268463423171,0.5427021351067554,0.5441022051102555,0.544277213860693,0.5448022401120056,0.5484774238711936,0.5493524676233812,0.5505775288764438,0.5509275463773189,0.5512775638781939,0.5514525726286315,0.5542527126356318,0.5546027301365068,0.5549527476373819,0.5584529226461323,0.5612530626531327,0.5616030801540077,0.5631781589079454,0.563353167658383,0.5656282814140707,0.572103605180259,0.5722786139306966,0.5726286314315716,0.5726286314315716,0.5740287014350718,0.575778788939447,0.5792789639481974,0.5799789989499475,0.5806790339516976,0.5908295414770739,0.5910045502275114,0.5924046202310116,0.6022051102555128,0.6078053902695135,0.611305565278264,0.6132306615330767,0.6135806790339517,0.6137556877843893,0.6144557227861394,0.6149807490374519,0.6151557577878894,0.6167308365418271,0.6169058452922647,0.6174308715435772,0.6177808890444523,0.6177808890444523,0.6184809240462024,0.6184809240462024,0.6186559327966399,0.6200560028001401,0.6205810290514526,0.6207560378018901,0.6226811340567029,0.6228561428071404,0.6268813440672033,0.6279313965698284,0.6298564928246412,0.6305565278263913,0.6316065803290164,0.632131606580329,0.632831641582079,0.6331816590829541,0.6333566678333916,0.6342317115855792,0.6347567378368918,0.6375568778438921,0.6407070353517675,0.6415820791039551,0.6464823241162058,0.6473573678683934,0.6477073853692684,0.648582429121456,0.6489324466223311,0.6510325516275813,0.6520826041302065,0.652257612880644,0.655932796639832,0.6576828841442072,0.6578578928946447,0.6578578928946447,0.6592579628981449,0.6594329716485824,0.6597829891494574,0.66030801540077,0.6604830241512075,0.6610080504025201,0.6611830591529576,0.6624081204060203,0.664333216660833,0.6650332516625831,0.6660833041652082,0.6671333566678334,0.6678333916695834,0.6690584529226461,0.6692334616730836,0.6699334966748337,0.6704585229261463,0.6737836891844592,0.6741337066853342,0.6751837591879594,0.6755337766888344,0.6771088554427721,0.6774588729436472,0.6778088904445222,0.6792089604480224,0.6802590129506475,0.6807840392019601,0.6814840742037102,0.6830591529576479,0.684459222961148,0.684459222961148,0.6846342317115856,0.6907595379768988,0.6911095554777739,0.6912845642282114,0.691809590479524,0.696534826741337,0.703885194259713,0.7052852642632131,0.7054602730136507,0.7058102905145257,0.7094854742737137,0.7101855092754638,0.7175358767938397,0.7177108855442772,0.7177108855442772,0.7238361918095905,0.7271613580679034,0.7275113755687784,0.727686384319216,0.7310115505775289,0.7313615680784039,0.732411620581029,0.7331116555827791,0.7332866643332167,0.7339866993349667,0.7385369268463423,0.7388869443472174,0.7395869793489674,0.7399369968498425,0.7409870493524676,0.7411620581029051,0.7416870843542177,0.7418620931046552,0.7423871193559678,0.7427371368568428,0.7429121456072804,0.743787189359468,0.7451872593629681,0.7474623731186559,0.7476373818690935,0.7479873993699685,0.763913195659783,0.7653132656632832,0.7663633181659083,0.7665383269163458,0.7826391319565978,0.7828141407070354,0.7829891494574729,0.7845642282114106,0.7864893244662233,0.7868393419670984,0.7875393769688485,0.787714385719286,0.788064403220161,0.7910395519775989,0.7912145607280364,0.7913895694784739,0.7913895694784739,0.7915645782289115,0.7933146657332867,0.7936646832341617,0.7955897794889745,0.7966398319915996,0.7969898494924746,0.7973398669933497,0.7976898844942247,0.7999649982499125,0.7999649982499125,0.80014000700035,0.8004900245012251,0.8010150507525376,0.8111655582779139,0.8123906195309766,0.8125656282814141,0.8141407070353518,0.8143157157857893,0.8148407420371019,0.815890794539727,0.8162408120406021,0.8165908295414771,0.8171158557927897,0.8172908645432272,0.8172908645432272,0.8179908995449773,0.8183409170458523,0.8188659432971649,0.8213160658032902,0.8218410920546028,0.8230661533076654,0.823591179558978,0.8260413020651033,0.8265663283164159,0.8272663633181659,0.827616380819041,0.827966398319916,0.8297164858242912,0.8426671333566679,0.843367168358418,0.848092404620231,0.8486174308715436,0.8507175358767939,0.851767588379419,0.8535176758837942,0.8540427021351068,0.8547427371368569,0.8550927546377319,0.8550927546377319,0.855442772138607,0.8556177808890445,0.8575428771438572,0.8580679033951698,0.8587679383969199,0.8620931046552328,0.8633181659082955,0.863493174658733,0.8640182009100456,0.8652432621631082,0.8654182709135457,0.8699684984249213,0.8708435421771089,0.8713685684284215,0.8722436121806091,0.8769688484424221,0.8785439271963598,0.8809940497024851,0.8811690584529226,0.8855442772138606,0.8864193209660483,0.8865943297164858,0.8867693384669233,0.8867693384669233,0.8876443822191109,0.887994399719986,0.8892194609730486,0.8893944697234861,0.8897444872243612,0.8897444872243612,0.892719635981799,0.8928946447322366,0.8930696534826741,0.8939446972348617,0.8970948547427371,0.8972698634931746,0.8976198809940497,0.8984949247462373,0.8984949247462373,0.8997199859992999,0.8997199859992999,0.9049702485124256,0.9051452572628631,0.9067203360168008,0.9072453622681134,0.9079453972698635,0.908120406020301,0.9126706335316765,0.9130206510325516,0.9137206860343017,0.9159957997899895,0.916170808540427,0.9163458172908645,0.9163458172908645,0.916520826041302,0.9172208610430521,0.9175708785439272,0.9182709135456772,0.9196709835491774,0.9200210010500525,0.9200210010500525,0.9221211060553027,0.9224711235561778,0.9254462723136156,0.9257962898144907,0.9257962898144907,0.927896394819741,0.9280714035701785,0.9287714385719286,0.9289464473223661,0.9294714735736787,0.9298214910745537,0.9301715085754287,0.9305215260763038,0.9317465873293664,0.931921596079804,0.932271613580679,0.932621631081554,0.9333216660833041,0.9348967448372418,0.9350717535876794,0.9354217710885544,0.9368218410920546,0.9369968498424921,0.9378718935946797,0.9380469023451172,0.9382219110955548,0.9382219110955548,0.9396219810990549,0.9401470073503675,0.9413720686034301,0.9417220861043052,0.9422471123556178,0.9425971298564928,0.9427721386069303,0.9427721386069303,0.9460973048652432,0.9469723486174308,0.9471473573678684,0.9476723836191809,0.948372418620931,0.948372418620931,0.9485474273713685,0.9490724536226811,0.9495974798739937,0.9497724886244312,0.9499474973748687,0.9501225061253062,0.9502975148757438,0.9504725236261813,0.9506475323766188,0.9509975498774939,0.9511725586279314,0.9515225761288064,0.9525726286314316,0.9539726986349317,0.9551977598879944,0.9571228561428071,0.9571228561428071,0.9572978648932446,0.9576478823941197,0.9639481974098705,0.964473223661183,0.9658732936646832,0.9662233111655583,0.9667483374168708,0.9669233461673084,0.9672733636681834,0.9702485124256213,0.9704235211760588,0.9753237661883094,0.9754987749387469,0.975848792439622,0.9765488274413721,0.9767238361918096,0.9772488624431221,0.9779488974448722,0.9830241512075604,0.9840742037101855,0.984249212460623,0.9847742387119356,0.9854742737136857,0.9854742737136857,0.9859992999649982,0.9861743087154358,0.9863493174658733,0.9868743437171859,0.9872243612180609,0.9880994049702485,0.9884494224711236,0.9884494224711236,0.9889744487224361,0.9901995099754988,0.9901995099754988,0.9905495274763738,0.9907245362268113,0.9907245362268113,0.9910745537276864,0.9912495624781239,0.991599579978999,0.9917745887294365,0.9917745887294365,0.9917745887294365,0.992299614980749,0.9924746237311866,0.9926496324816241,0.9929996499824991,0.9929996499824991,0.9931746587329366,0.9936996849842492,0.9936996849842492,0.9938746937346867,0.9938746937346867,0.9942247112355618,0.9945747287364368,0.9945747287364368,0.9947497374868743,0.9947497374868743,0.9950997549877494,0.9954497724886244,0.9954497724886244,0.995974798739937,0.9964998249912496,0.9966748337416871,0.9966748337416871,0.9973748687434372,0.9973748687434372,0.9977248862443122,0.9980749037451873,0.9980749037451873,0.9980749037451873,0.9982499124956248,0.9987749387469373,0.9989499474973749,0.9991249562478124,0.9992999649982499,0.9992999649982499,0.9992999649982499,0.9994749737486874,0.999649982499125,0.999649982499125,0.9998249912495625,0.9998249912495625,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"type\":\"scatter\"}],                        {\"height\":500,\"title\":{\"text\":\"AUC Curve for XGBoost\"},\"width\":800,\"xaxis\":{\"title\":{\"text\":\"False Positive Rate\"}},\"yaxis\":{\"title\":{\"text\":\"True Positive Rate\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('275841ca-2af3-4d69-a93c-d04c1d1e3cd2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.graph_objs as go\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(x_train, y_train)\n",
        "\n",
        "# Calculate ROC curve and AUC for XGBoost\n",
        "xgb_probs = xgb_model.predict_proba(x_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot AUC curve using Plotly\n",
        "auc_trace = go.Scatter(x=fpr, y=tpr, mode='lines', name=f'AUC Curve (AUC = {roc_auc:.2f})')\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='AUC Curve for XGBoost',\n",
        "    xaxis=dict(title='False Positive Rate'),\n",
        "    yaxis=dict(title='True Positive Rate'),\n",
        "    height=500,  # Specify height\n",
        "    width=800    # Specify width\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[auc_trace], layout=layout)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phtenwe9DQxJ",
        "outputId": "01727639-06a4-445b-b3cc-06b88f834c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"25435810-2b12-4863-bfdd-bc9f58875fb8\" class=\"plotly-graph-div\" style=\"height:500px; width:500px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"25435810-2b12-4863-bfdd-bc9f58875fb8\")) {                    Plotly.newPlot(                        \"25435810-2b12-4863-bfdd-bc9f58875fb8\",                        [{\"colorscale\":[[0.0,\"rgb(255,255,217)\"],[0.125,\"rgb(237,248,177)\"],[0.25,\"rgb(199,233,180)\"],[0.375,\"rgb(127,205,187)\"],[0.5,\"rgb(65,182,196)\"],[0.625,\"rgb(29,145,192)\"],[0.75,\"rgb(34,94,168)\"],[0.875,\"rgb(37,52,148)\"],[1.0,\"rgb(8,29,88)\"]],\"reversescale\":false,\"showscale\":true,\"x\":[\"Negative\",\"Positive\"],\"y\":[\"Negative\",\"Positive\"],\"z\":[[4467,335],[47,1962]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"4467\",\"x\":\"Negative\",\"xref\":\"x\",\"y\":\"Negative\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"335\",\"x\":\"Positive\",\"xref\":\"x\",\"y\":\"Negative\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"47\",\"x\":\"Negative\",\"xref\":\"x\",\"y\":\"Positive\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"1962\",\"x\":\"Positive\",\"xref\":\"x\",\"y\":\"Positive\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"4467\",\"x\":\"Negative\",\"y\":\"Negative\"},{\"font\":{\"color\":\"black\"},\"showarrow\":false,\"text\":\"335\",\"x\":\"Positive\",\"y\":\"Negative\"},{\"font\":{\"color\":\"black\"},\"showarrow\":false,\"text\":\"47\",\"x\":\"Negative\",\"y\":\"Positive\"},{\"font\":{\"color\":\"black\"},\"showarrow\":false,\"text\":\"1962\",\"x\":\"Positive\",\"y\":\"Positive\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\"},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Confusion Matrix - Random Forest Model\"},\"height\":500,\"width\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('25435810-2b12-4863-bfdd-bc9f58875fb8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Confusion Matrix for Random Forest Model\n",
        "rf_conf_matrix = [[4467, 335],\n",
        "                  [47, 1962]]\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['Negative', 'Positive']\n",
        "\n",
        "# Plotting Random Forest Confusion Matrix\n",
        "rf_fig = ff.create_annotated_heatmap(z=rf_conf_matrix,\n",
        "                                     x=class_labels,\n",
        "                                     y=class_labels,\n",
        "                                     colorscale='YlGnBu',\n",
        "                                     showscale=True)  # Show color scale\n",
        "\n",
        "# Add values inside cells for Random Forest Confusion Matrix\n",
        "for i in range(len(class_labels)):\n",
        "    for j in range(len(class_labels)):\n",
        "        font_color = 'white' if rf_conf_matrix[i][j] == 4467 else 'black'  # Set font color to white for the value 4467\n",
        "        rf_fig.add_annotation(text=str(rf_conf_matrix[i][j]),\n",
        "                              x=class_labels[j], y=class_labels[i],\n",
        "                              showarrow=False, font=dict(color=font_color))  # Set font color\n",
        "\n",
        "# Update layout\n",
        "rf_fig.update_layout(title_text=\"Confusion Matrix - Random Forest Model\",\n",
        "                     height=500, width=500)\n",
        "\n",
        "rf_fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upzeN6n-DQxJ",
        "outputId": "64ef364b-1d13-46a3-ad5d-14d78a122262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"02a8b447-6ff7-4c0b-bc13-d7507b04e895\" class=\"plotly-graph-div\" style=\"height:500px; width:500px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"02a8b447-6ff7-4c0b-bc13-d7507b04e895\")) {                    Plotly.newPlot(                        \"02a8b447-6ff7-4c0b-bc13-d7507b04e895\",                        [{\"colorscale\":[[0.0,\"rgb(255,255,217)\"],[0.125,\"rgb(237,248,177)\"],[0.25,\"rgb(199,233,180)\"],[0.375,\"rgb(127,205,187)\"],[0.5,\"rgb(65,182,196)\"],[0.625,\"rgb(29,145,192)\"],[0.75,\"rgb(34,94,168)\"],[0.875,\"rgb(37,52,148)\"],[1.0,\"rgb(8,29,88)\"]],\"reversescale\":false,\"showscale\":true,\"x\":[\"Negative\",\"Positive\"],\"y\":[\"Negative\",\"Positive\"],\"z\":[[4463,339],[33,1976]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"4463\",\"x\":\"Negative\",\"xref\":\"x\",\"y\":\"Negative\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"339\",\"x\":\"Positive\",\"xref\":\"x\",\"y\":\"Negative\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"33\",\"x\":\"Negative\",\"xref\":\"x\",\"y\":\"Positive\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"1976\",\"x\":\"Positive\",\"xref\":\"x\",\"y\":\"Positive\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"4463\",\"x\":\"Negative\",\"y\":\"Negative\"},{\"font\":{\"color\":\"black\"},\"showarrow\":false,\"text\":\"339\",\"x\":\"Positive\",\"y\":\"Negative\"},{\"font\":{\"color\":\"black\"},\"showarrow\":false,\"text\":\"33\",\"x\":\"Negative\",\"y\":\"Positive\"},{\"font\":{\"color\":\"black\"},\"showarrow\":false,\"text\":\"1976\",\"x\":\"Positive\",\"y\":\"Positive\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\"},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Confusion Matrix - XGBoost Model\"},\"height\":500,\"width\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('02a8b447-6ff7-4c0b-bc13-d7507b04e895');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Confusion Matrix for XGBoost Model\n",
        "xgb_conf_matrix = [[4463, 339],\n",
        "                   [33, 1976]]\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['Negative', 'Positive']\n",
        "\n",
        "# Plotting XGBoost Confusion Matrix\n",
        "xgb_fig = ff.create_annotated_heatmap(z=xgb_conf_matrix,\n",
        "                                      x=class_labels,\n",
        "                                      y=class_labels,\n",
        "                                      colorscale='YlGnBu',\n",
        "                                      showscale=True)  # Show color scale\n",
        "\n",
        "# Add values inside cells for XGBoost Confusion Matrix\n",
        "for i in range(len(class_labels)):\n",
        "    for j in range(len(class_labels)):\n",
        "        font_color = 'white' if xgb_conf_matrix[i][j] == 4463 else 'black'  # Set font color to white for the value 4463\n",
        "        xgb_fig.add_annotation(text=str(xgb_conf_matrix[i][j]),\n",
        "                               x=class_labels[j], y=class_labels[i],\n",
        "                               showarrow=False, font=dict(color=font_color))  # Set font color\n",
        "\n",
        "# Update layout\n",
        "xgb_fig.update_layout(title_text=\"Confusion Matrix - XGBoost Model\",\n",
        "                      height=500, width=500)\n",
        "\n",
        "xgb_fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvo7LdwYDQxJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0cf20c40-d3d9-4ff3-ca20-bf6d098842dd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-e322c582e4c5>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Save the plot as PDF using plotly.io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confusion_matrix_comparison.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# -------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# Do this first so we don't create a file if image conversion fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     img_data = to_image(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Raise informative error message if Kaleido is not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \"\"\"\n\u001b[1;32m    135\u001b[0m \u001b[0mImage\u001b[0m \u001b[0mexport\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m\"kaleido\"\u001b[0m \u001b[0mengine\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkaleido\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
          ]
        }
      ],
      "source": [
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Confusion Matrix for Random Forest Model\n",
        "rf_conf_matrix = [[4467, 335],\n",
        "                  [47, 1962]]\n",
        "\n",
        "# Confusion Matrix for XGBoost Model\n",
        "xgb_conf_matrix = [[4463, 339],\n",
        "                   [33, 1976]]\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['Negative', 'Positive']\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Random Forest Model\", \"XGBoost Model\"))\n",
        "\n",
        "# Plotting Random Forest Confusion Matrix\n",
        "rf_fig = ff.create_annotated_heatmap(z=rf_conf_matrix,\n",
        "                                     x=class_labels,\n",
        "                                     y=class_labels,\n",
        "                                     colorscale='YlGnBu',\n",
        "                                     showscale=False)  # Hide color scale\n",
        "\n",
        "# Plotting XGBoost Confusion Matrix\n",
        "xgb_fig = ff.create_annotated_heatmap(z=xgb_conf_matrix,\n",
        "                                      x=class_labels,\n",
        "                                      y=class_labels,\n",
        "                                      colorscale='YlGnBu',\n",
        "                                      showscale=False)  # Hide color scale\n",
        "\n",
        "# Add subplots\n",
        "fig.add_trace(rf_fig.data[0], row=1, col=1)\n",
        "\n",
        "# Add values inside cells for Random Forest Confusion Matrix\n",
        "for i in range(len(class_labels)):\n",
        "    for j in range(len(class_labels)):\n",
        "        font_color = 'white' if rf_conf_matrix[i][j] == 4467 else 'black'  # Set font color to white for the value 4467\n",
        "        fig.add_annotation(text=str(rf_conf_matrix[i][j]),\n",
        "                              x=class_labels[j], y=class_labels[i],\n",
        "                              showarrow=False, font=dict(color=font_color), row=1, col=1)  # Set font color\n",
        "\n",
        "fig.add_trace(xgb_fig.data[0], row=1, col=2)\n",
        "\n",
        "# Add values inside cells for XGBoost Confusion Matrix\n",
        "for i in range(len(class_labels)):\n",
        "    for j in range(len(class_labels)):\n",
        "        font_color = 'white' if xgb_conf_matrix[i][j] == 4463 else 'black'  # Set font color to white for the value 4463\n",
        "        fig.add_annotation(text=str(xgb_conf_matrix[i][j]),\n",
        "                               x=class_labels[j], y=class_labels[i],\n",
        "                               showarrow=False, font=dict(color=font_color), row=1, col=2)  # Set font color\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title_text=\"Confusion Matrix Comparison\", title_x=0.5,\n",
        "                  height=500, width=1000,\n",
        "                  margin=dict(l=0, r=5, t=50, b=50),  # Set margins (l=left, r=right, t=top, b=bottom)\n",
        "                  plot_bgcolor='rgba(0,0,0,0)')  # Set plot background color to transparent\n",
        "\n",
        "import plotly.io as pio\n",
        "\n",
        "# Save the plot as PDF using plotly.io\n",
        "pio.write_image(fig, 'confusion_matrix_comparison.pdf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcqrnQQzDQxT"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.io as pio\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "df = df.dropna()  # Drops rows containing any NaN values\n",
        "\n",
        "# Now, you can proceed to split the data and train the model\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(x_train, y_train)\n",
        "\n",
        "# Calculate ROC curve and AUC for XGBoost\n",
        "xgb_probs = xgb_model.predict_proba(x_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot AUC curve using Plotly\n",
        "auc_trace = go.Scatter(x=fpr, y=tpr, mode='lines', name=f'AUC Curve (AUC = {roc_auc:.2f})')\n",
        "\n",
        "layout = go.Layout(\n",
        "    title=dict(\n",
        "        text='AUC Curve for XGBoost',\n",
        "        x=0.5  # Center the title\n",
        "    ),\n",
        "    xaxis=dict(title='False Positive Rate'),\n",
        "    yaxis=dict(title='True Positive Rate'),\n",
        "    height=500,  # Specify height\n",
        "    width=800    # Specify width\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[auc_trace], layout=layout)\n",
        "fig.show()\n",
        "\n",
        "# Save the plot as PDF using plotly.io\n",
        "pio.write_image(fig, 'auc_curve_xgboost.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xp0TfuqDQxT"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1EZopbADQxT"
      },
      "outputs": [],
      "source": [
        "# import plotly.figure_factory as ff\n",
        "\n",
        "# # Subset the DataFrame based on selected features\n",
        "# X_subset = X[selected_features]\n",
        "\n",
        "# # Calculate correlation matrix\n",
        "# corr_matrix = X_subset.corr().round(1)  # Round to 1 decimal place\n",
        "\n",
        "# # Define feature labels\n",
        "# feature_labels = {\n",
        "#     'Age': 'Age',\n",
        "#     'Bachelor': 'Bach.',\n",
        "#     'B_SUBJECT': 'B_Subj.',\n",
        "#     'Bachelor Education Organization (Public / Private)': 'Bach. Edu Org',\n",
        "#     'Technical Skill': 'Tech. Skill',\n",
        "#     'SOFT Skill': 'Soft Skill',\n",
        "#     'Masters': 'Masters',\n",
        "#     'm_SUBJECT': 'm_Subj.',\n",
        "#     'Masters Education Organization (Public / Private)': 'Masters Edu Org'\n",
        "# }\n",
        "\n",
        "# # Create a heatmap using Plotly with customized labels\n",
        "# fig = ff.create_annotated_heatmap(z=corr_matrix.values,\n",
        "#                                   x=[feature_labels[col] for col in corr_matrix.columns],\n",
        "#                                   y=[feature_labels[row] for row in corr_matrix.index],\n",
        "#                                   colorscale='Viridis')\n",
        "\n",
        "# # Update layout with height and width settings\n",
        "# fig.update_layout(title_text='Correlation Heatmap of Selected Features',\n",
        "#                   title_x=0.5,  # Set title position in the middle\n",
        "#                   margin=dict(l=0, r=5, t=120, b=5),  # Remove extra margins\n",
        "#                   height=600,  # Set the height\n",
        "#                   width=800)  # Set the width\n",
        "\n",
        "# # Show the plot\n",
        "# pio.write_image(fig, 'hitmap.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5w8RxtSDQxT"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "\n",
        "\n",
        "# Assuming df is your DataFrame containing the 'output' column\n",
        "# Count the occurrences of each output value\n",
        "output_counts = df['output'].value_counts()\n",
        "\n",
        "# Create a bar chart using Plotly\n",
        "fig = go.Figure(data=[go.Bar(\n",
        "    x=output_counts.index,  # Output values\n",
        "    y=output_counts.values,  # Count of each output value\n",
        "    text=output_counts.values,  # Text labels for each bar\n",
        "    textposition='auto',  # Position of the text labels\n",
        "    marker_color=['orange', 'blue']  # Colors for bars (orange for 1, blue for 0)\n",
        ")])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title_text='Distribution of Output Values',\n",
        "                  title_x=0.5,  # Set title position in the middle\n",
        "                  xaxis_title='Output', yaxis_title='Count',\n",
        "                  height=400, width=600,  # Set height and width\n",
        "                  plot_bgcolor='white',  # Set plot background color to white\n",
        "                  paper_bgcolor='white',  # Set paper background color to white\n",
        "                  xaxis=dict(tickvals=[0, 1], ticktext=['Not Offer (0)', 'Offer (1)']))  # Set tick labels for x-axis\n",
        "\n",
        "# Show the plot\n",
        "pio.write_image(fig, 'output.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8xeSG9aDQxT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Assuming df is your DataFrame with the provided columns\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop(columns=['output'])\n",
        "y = df['output']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing: Standardize numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict_classes(X_test_scaled)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eGz1YYC995r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Assuming df is your DataFrame with the provided columns\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop(columns=['output'])\n",
        "y = df['output']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing: Standardize numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "# Convert probabilities to classes using a threshold (e.g., 0.5)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix ANN:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1NnyL_C995r"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Assuming df is your DataFrame with the provided columns\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop(columns=['output'])\n",
        "y = df['output']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing: Standardize numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the MLP model\n",
        "model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict classes\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix MLP:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4U2LbYX995r"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Define the AdaBoost classifier\n",
        "ada_model = AdaBoostClassifier()\n",
        "\n",
        "# Train the model\n",
        "ada_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob_ada = ada_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Convert probabilities to classes using a threshold (e.g., 0.5)\n",
        "y_pred_ada = (y_pred_prob_ada > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix_ada = confusion_matrix(y_test, y_pred_ada)\n",
        "print(\"Confusion Matrix AdaBoost:\")\n",
        "print(conf_matrix_ada)\n",
        "\n",
        "# Generate classification report\n",
        "class_report_ada = classification_report(y_test, y_pred_ada)\n",
        "print(\"Classification Report AdaBoost:\")\n",
        "print(class_report_ada)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
        "print(\"Accuracy AdaBoost:\", accuracy_ada)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42yZ1NIL995r"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Convert probabilities to classes using a threshold (e.g., 0.5)\n",
        "y_pred_lr = (y_pred_prob_lr > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "print(\"Confusion Matrix Logistic Regression:\")\n",
        "print(conf_matrix_lr)\n",
        "\n",
        "# Generate classification report\n",
        "class_report_lr = classification_report(y_test, y_pred_lr)\n",
        "print(\"Classification Report Logistic Regression:\")\n",
        "print(class_report_lr)\n",
        "# Calculate accuracy\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(\"Accuracy Logistic Regression:\", accuracy_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU5E7M9g995s"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Define the GBM model\n",
        "gbm_model = GradientBoostingClassifier()\n",
        "\n",
        "# Train the model\n",
        "gbm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob_gbm = gbm_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Convert probabilities to classes using a threshold (e.g., 0.5)\n",
        "y_pred_gbm = (y_pred_prob_gbm > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix_gbm = confusion_matrix(y_test, y_pred_gbm)\n",
        "print(\"Confusion Matrix GBM:\")\n",
        "print(conf_matrix_gbm)\n",
        "\n",
        "# Generate classification report\n",
        "class_report_gbm = classification_report(y_test, y_pred_gbm)\n",
        "print(\"Classification Report GBM:\")\n",
        "print(class_report_gbm)\n",
        "# Calculate accuracy\n",
        "accuracy_gbm = accuracy_score(y_test, y_pred_gbm)\n",
        "print(\"Accuracy GBM:\", accuracy_gbm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW03cQs3995s"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Define the Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob_nb = nb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Convert probabilities to classes using a threshold (e.g., 0.5)\n",
        "y_pred_nb = (y_pred_prob_nb > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "print(\"Confusion Matrix Naive Bayes:\")\n",
        "print(conf_matrix_nb)\n",
        "\n",
        "# Generate classification report\n",
        "class_report_nb = classification_report(y_test, y_pred_nb)\n",
        "print(\"Classification Report Naive Bayes:\")\n",
        "print(class_report_nb)\n",
        "# Calculate accuracy\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(\"Accuracy Naive Bayes:\", accuracy_nb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8W5Dbtd995s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Assuming df is your DataFrame with the provided columns\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop(columns=['output'])\n",
        "y = df['output']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing: Standardize numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "\n",
        "# Predict classes\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix ANN:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred_classes)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JigNEBP0995s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}